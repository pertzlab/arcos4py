{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#arcos4py","title":"arcos4py","text":"<p>arcos4py is a Python package designed to detect and analyze collective spatiotemporal phenomena in biological imaging data.</p> <ul> <li>Documentation: https://pertzlab.github.io/arcos4py</li> <li>GitHub Repository: https://github.com/pertzlab/arcos4py</li> <li>PyPI Package: https://pypi.org/project/arcos4py/</li> <li>Free Software License: MIT</li> </ul>"},{"location":"#features","title":"Features","text":"<p>Automated Recognition of Collective Signalling for Python (arcos4py) identifies collective spatial events in time-series data or microscopy images. The software tracks waves of protein activity in 2D and 3D cell cultures and follows them over time.</p> <p>Such collective dynamics have been observed in: - Epithelial homeostasis (Gagliardi et al., 2020; Takeuchi et al., 2020; Aikin et al., 2020) - Acinar morphogenesis (Ender et al., 2020) - Osteoblast regeneration (De Simone et al., 2021) - Coordination of collective cell migration (Aoki et al., 2017; Hino et al., 2020)</p> <p>The R package ARCOS (https://github.com/dmattek/ARCOS) provides a similar R implementation. The <code>arcos4py</code> version includes more recent upgrades and added functionality: - Event tracking directly on image data - Split/merge detection - Motion prediction for robust temporal linking</p> <p>Data format: Long-table format with object coordinates, time, and optionally measurements; or binary image sequences for pixel-level analysis.</p> <p>Modular API: Use the full ARCOS class or individual tools via <code>arcos.tools</code>. Process binary images directly using <code>track_events_images</code> in <code>arcos4py.tools</code>.</p>"},{"location":"#new-in-arcos-with-arcospx-","title":"New in ARCOS with ARCOS.px \ud83c\udf89","text":"<p>We recently released a major update, ARCOS.px, extending <code>arcos4py</code> to track subcellular dynamic structures like actin waves, podosomes, and focal adhesions directly from binarized time-lapse images.</p> <p>Publication: Tracking Coordinated Cellular Dynamics in Time-Lapse Microscopy with ARCOS.px. bioRxiv</p> <p>What\u2019s new: - Pixel-based tracking of discontinuous, irregular structures - Lineage tracking across merges and splits - Optional Motion prediction and frame-to-frame linking with optimal transport - Support for DBSCAN and HDBSCAN clustering and custom clustering methods - Improved memory usage and lazy evaluation for long time series - Integrated into Napari via the <code>arcosPx-napari plugin</code> plugin</p>"},{"location":"#notebooks-and-reproducible-analysis","title":"Notebooks and Reproducible Analysis","text":"<p>To facilitate reproducibility and provide practical examples, we have made available a collection of Jupyter notebooks that demonstrate the use of ARCOS.px in various scenarios. These notebooks cover:</p> <p>Wave Simulation: Scripts to simulate circular &amp; directional waves, and target &amp; chaotic patterns using cellular automaton.</p> <p>Synthetic RhoA Activity Wave: Analysis of optogenetically induced synthetic RhoA activity waves.</p> <p>Podosome Dynamics: Tracking and analysis of podosome-like structures under different conditions.</p> <p>Actin Wave Tracking: Tracking and analysis of actin waves in 2D and extractin temporal order.</p> <p>You can access these notebooks in the ARCOSpx-publication repository under the scripts directory.</p>"},{"location":"#installation","title":"Installation","text":"<p>Install from PyPI: <pre><code>pip install arcos4py\n</code></pre></p>"},{"location":"#napari-plugin","title":"Napari Plugin","text":"<p>Arcos4py is also available as a Napari Plugin arcos-gui. arcos-gui can simplify parameter finding and visualization.</p> <p>or images directly: arcosPx-napari</p> <p></p>"},{"location":"#credits","title":"Credits","text":"<p>Maciej Dobrzynski created the first version of ARCOS.</p> <p>This package was created with Cookiecutter and the waynerv/cookiecutter-pypackage project template.</p>"},{"location":"api/","title":"Modules","text":"<p>Arcos4py top level module.</p> <p>This package is a python package for the detection and tracking of collective events intime-series data and raster images.</p>"},{"location":"api/#arcos4py.ARCOS","title":"<code>ARCOS(data, position_columns=['x'], frame_column='time', obj_id_column='id', measurement_column='meas', clid_column='clTrackID', n_jobs=1, **kwargs)</code>","text":"<p>Detects and tracks collective events in a tracked time-series dataset.</p> <p>Requires binarized measurement column, that can be generated with the bin_measurements method. Tracking makes use of the dbscan algorithm, which is applied to every frame and subsequently connects collective events between frames located within eps distance of each other.</p> <p>Attributes:</p> Name Type Description <code>data</code> <code>DataFrame</code> <p>Data of tracked time-series in \"long format\". Can be used to acess modified dataframe at any point.</p> <code>position_columns</code> <code>list</code> <p>List containing position column names strings inside data e.g. At least one dimension is required.</p> <code>frame_column</code> <code>str</code> <p>Indicating the frame column in input_data.</p> <code>obj_id_column</code> <code>str</code> <p>Indicating the track id/id column in input_data.</p> <code>measurement_column</code> <code>str</code> <p>Indicating the measurement column in input_data.</p> <code>clid_column</code> <code>str</code> <p>Indicating the column name containing the collective event ids.</p> <code>binarized_measurement_column</code> <code>str | None</code> <p>Name of the binary column. This is generated based on the name of the measurement_column after binarization. Optionally can be set in order to provide a already binarized column to skip ARCOS binarization.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>Input Data of tracked time-series in \"long format\" containing position columns, a measurement and an object ID column.</p> required <code>position_columns</code> <code>list</code> <p>List ontaining position column names strings inside data e.g. At least one dimension is required.</p> <code>['x']</code> <code>frame_column</code> <code>str</code> <p>Indicating the frame column in input_data.</p> <code>'time'</code> <code>obj_id_column</code> <code>str</code> <p>Indicating the track id/object id column in input_data. If None, the data is assumed to not have a tracking column. Binarization can only be performed without detrending.</p> <code>'id'</code> <code>measurement_column</code> <code>str</code> <p>Indicating the measurement column in input_data.</p> <code>'meas'</code> <code>clid_column</code> <code>str</code> <p>Indicating the column name containing the collective event ids.</p> <code>'clTrackID'</code> <code>n_jobs</code> <code>str</code> <p>Number of workers to spawn, -1 uses all available cpus.</p> <code>1</code> <code>kwargs</code> <code>Any</code> <p>Additional keyword arguments. Includes old parameter names for backwards compatibility. - posCols: List containing position column names strings inside data e.g.</p> <code>{}</code> Source code in <code>arcos4py/_arcos4py.py</code> <pre><code>def __init__(\n    self,\n    data: pd.DataFrame,\n    position_columns: list = [\"x\"],\n    frame_column: str = 'time',\n    obj_id_column: str | None = 'id',\n    measurement_column: str = 'meas',\n    clid_column: str = 'clTrackID',\n    n_jobs: int = 1,\n    **kwargs,\n) -&gt; None:\n    \"\"\"Constructs class with provided arguments.\n\n    Arguments:\n        data (DataFrame): Input Data of tracked time-series in \"long format\" containing position columns,\n            a measurement and an object ID column.\n        position_columns (list): List ontaining position column names strings inside data e.g.\n            At least one dimension is required.\n        frame_column (str): Indicating the frame column in input_data.\n        obj_id_column (str): Indicating the track id/object id column in input_data. If None, the data is assumed to\n            not have a tracking column. Binarization can only be performed without detrending.\n        measurement_column (str): Indicating the measurement column in input_data.\n        clid_column (str): Indicating the column name containing the collective event ids.\n        n_jobs (str): Number of workers to spawn, -1 uses all available cpus.\n        kwargs (Any): Additional keyword arguments. Includes old parameter names for backwards compatibility.\n            - posCols: List containing position column names strings inside data e.g.\n    \"\"\"\n    # allowed kwargs\n    allowed_kwargs = [\"posCols\", \"id_column\"]\n    for key in kwargs:\n        if key not in allowed_kwargs:\n            raise ValueError(f\"__init__() got an unexpected keyword argument '{key}'\")\n    # Handle deprecated parameters\n    param_mapping = {\n        \"posCols\": \"position_columns\",\n        \"id_column\": \"obj_id_column\",\n    }\n    updated_kwargs = handle_deprecated_params(param_mapping, **kwargs)\n\n    # Assign updated kwargs to class attributes\n    position_columns = updated_kwargs.get(\"position_columns\", position_columns)\n    obj_id_column = updated_kwargs.get(\"obj_id_column\", obj_id_column)\n\n    self.data = data\n    self.position_columns = position_columns\n    self.frame_column = frame_column\n    self.obj_id_column = obj_id_column\n    self.measurement_column = measurement_column\n    self.clid_column = clid_column\n    self.n_jobs = n_jobs\n\n    self.binarized_measurement_column: Union[str, None] = None\n    # to check if no measurement was provided assign None\n    if self.obj_id_column is None:\n        self.data = self.data.sort_values(by=[self.frame_column])\n    else:\n        self.data = self.data.sort_values(by=[self.frame_column, self.obj_id_column])\n    self._check_col()\n    if self.measurement_column is not None:\n        self.resc_col = f\"{self.measurement_column}.resc\"\n        self.binarized_measurement_column = f\"{self.measurement_column}.bin\"\n</code></pre>"},{"location":"api/#arcos4py.ARCOS.bin_col","title":"<code>bin_col: str | None</code>  <code>property</code> <code>writable</code>","text":"<p>Return the name of the binarized measurement column.</p>"},{"location":"api/#arcos4py.ARCOS.id_column","title":"<code>id_column: str | None</code>  <code>property</code> <code>writable</code>","text":"<p>Return the name of the id column.</p>"},{"location":"api/#arcos4py.ARCOS.posCols","title":"<code>posCols: list</code>  <code>property</code> <code>writable</code>","text":"<p>Return the position columns.</p>"},{"location":"api/#arcos4py.ARCOS.bin_measurements","title":"<code>bin_measurements(smooth_k=3, bias_k=51, peak_threshold=0.2, binarization_threshold=0.1, polynomial_degree=1, bias_method='runmed', **kwargs)</code>","text":"<p>Smooth, de-trend, and binarise the input data.</p> <p>First a short-term median filter with size smoothK is applied to remove fast noise from the time series. If the de-trending method is set to \"none\", smoothing is applied on globally rescaled time series. The subsequent de-trending can be performed with a long-term median filter with the size biasK {biasMet = \"runmed\"} or by fitting a polynomial of degree polyDeg {biasMet = \"lm\"}.</p> <p>After de-trending, if the global difference between min/max is greater than the threshold the signal is rescaled to the (0,1) range. The final signal is binarised using the binThr threshold</p> <p>Parameters:</p> Name Type Description Default <code>smooth_k</code> <code>int</code> <p>Size of the short-term median smoothing filter.</p> <code>3</code> <code>bias_k</code> <code>int</code> <p>Size of the long-term de-trending median filter</p> <code>51</code> <code>peak_threshold</code> <code>float</code> <p>Threshold for rescaling of the de-trended signal.</p> <code>0.2</code> <code>binarization_threshold</code> <code>float</code> <p>Threshold for binary classification.</p> <code>0.1</code> <code>polynomial_degree</code> <code>int</code> <p>Sets the degree of the polynomial for lm fitting.</p> <code>1</code> <code>bias_method</code> <code>str</code> <p>De-trending method, one of ['runmed', 'lm', 'none']. If no id_column is provided, only 'none' is allowed.</p> <code>'runmed'</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments. Includes old parameter names for backwards compatibility. - smoothK: Size of the short-term median smoothing filter. - biasK: Size of the long-term de-trending median filter - peakThr: Threshold for rescaling of the de-trended signal. - binThr: Threshold for binary classification. - polyDeg: Sets the degree of the polynomial for lm fitting. - biasMet: De-trending method, one of ['runmed', 'lm', 'none'].</p> <code>{}</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with detrended/smoothed and binarized measurement column.</p> Source code in <code>arcos4py/_arcos4py.py</code> <pre><code>def bin_measurements(\n    self,\n    smooth_k: int = 3,\n    bias_k: int = 51,\n    peak_threshold: float = 0.2,\n    binarization_threshold: float = 0.1,\n    polynomial_degree: int = 1,\n    bias_method: str = \"runmed\",\n    **kwargs,\n) -&gt; pd.DataFrame:\n    r\"\"\"Smooth, de-trend, and binarise the input data.\n\n    First a short-term median filter with size smoothK\n    is applied to remove fast noise from the time series.\n    If the de-trending method is set to \"none\",\n    smoothing is applied on globally rescaled time series.\n    The subsequent de-trending can be performed with a long-term median filter\n    with the size biasK {biasMet = \"runmed\"}\n    or by fitting a polynomial of degree polyDeg {biasMet = \"lm\"}.\n\n    After de-trending,\n    if the global difference between min/max is greater than the threshold\n    the signal is rescaled to the (0,1) range.\n    The final signal is binarised using the binThr threshold\n\n    Arguments:\n        smooth_k (int): Size of the short-term median smoothing filter.\n        bias_k (int): Size of the long-term de-trending median filter\n        peak_threshold (float): Threshold for rescaling of the de-trended signal.\n        binarization_threshold (float): Threshold for binary classification.\n        polynomial_degree (int): Sets the degree of the polynomial for lm fitting.\n        bias_method (str): De-trending method, one of ['runmed', 'lm', 'none'].\n            If no id_column is provided, only 'none' is allowed.\n        **kwargs (Any): Additional keyword arguments. Includes old parameter names for backwards compatibility.\n            - smoothK: Size of the short-term median smoothing filter.\n            - biasK: Size of the long-term de-trending median filter\n            - peakThr: Threshold for rescaling of the de-trended signal.\n            - binThr: Threshold for binary classification.\n            - polyDeg: Sets the degree of the polynomial for lm fitting.\n            - biasMet: De-trending method, one of ['runmed', 'lm', 'none'].\n\n    Returns:\n        DataFrame with detrended/smoothed and binarized measurement column.\n    \"\"\"\n    # allowed kwargs\n    param_mapping = {\n        \"smoothK\": \"smooth_k\",\n        \"biasK\": \"bias_k\",\n        \"peakThr\": \"peak_threshold\",\n        \"binThr\": \"binarization_threshold\",\n        \"polyDeg\": \"polynomial_degree\",\n        \"biasMet\": \"bias_method\",\n    }\n    # allowed kwargs\n    allowed_kwargs = param_mapping.keys()\n    for key in kwargs:\n        if key not in allowed_kwargs:\n            raise ValueError(f\"bin_measurements() got an unexpected keyword argument '{key}'\")\n\n    updated_kwargs = handle_deprecated_params(param_mapping, **kwargs)\n\n    smooth_k = updated_kwargs.get(\"smooth_k\", smooth_k)\n    bias_k = updated_kwargs.get(\"bias_k\", bias_k)\n    peak_threshold = updated_kwargs.get(\"peak_threshold\", peak_threshold)\n    binarization_threshold = updated_kwargs.get(\"binarization_threshold\", binarization_threshold)\n    polynomial_degree = updated_kwargs.get(\"polynomial_degree\", polynomial_degree)\n    bias_method = updated_kwargs.get(\"bias_method\", bias_method)\n\n    self.data = binData(\n        smooth_k,\n        bias_k,\n        peak_threshold,\n        binarization_threshold,\n        polynomial_degree,\n        bias_method,\n        n_jobs=self.n_jobs,\n    ).run(\n        self.data,\n        measurement_column=self.measurement_column,\n        group_column=self.obj_id_column,\n        frame_column=self.frame_column,\n    )\n    return self.data\n</code></pre>"},{"location":"api/#arcos4py.ARCOS.clip_meas","title":"<code>clip_meas(clip_low=0.001, clip_high=0.999)</code>","text":"<p>Clip measurement column to upper and lower quantiles defined in clip_low and clip_high.</p> <p>Parameters:</p> Name Type Description Default <code>clip_low</code> <code>float</code> <p>Lower clipping boundary (quantile).</p> <code>0.001</code> <code>clip_high</code> <code>float</code> <p>Upper clipping boundary (quantile).</p> <code>0.999</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Dataframe with in place clipped measurement column.</p> Source code in <code>arcos4py/_arcos4py.py</code> <pre><code>def clip_meas(self, clip_low: float = 0.001, clip_high: float = 0.999) -&gt; pd.DataFrame:\n    \"\"\"Clip measurement column to upper and lower quantiles defined in clip_low and clip_high.\n\n    Arguments:\n        clip_low (float): Lower clipping boundary (quantile).\n\n        clip_high (float): Upper clipping boundary (quantile).\n\n    Returns:\n        Dataframe with in place clipped measurement column.\n    \"\"\"\n    # Issue a deprecation warning\n    warnings.warn(\n        \"The 'clip_meas' method is deprecated and will be removed in a future version.\\\n        Please use 'clip_measurements' instead.\",\n        DeprecationWarning,\n        stacklevel=2,\n    )\n    return self.clip_measurements(clip_low, clip_high)\n</code></pre>"},{"location":"api/#arcos4py.ARCOS.clip_measurements","title":"<code>clip_measurements(clip_low=0.001, clip_high=0.999)</code>","text":"<p>Clip measurement column to upper and lower quantiles defined in clip_low and clip_high.</p> <p>Parameters:</p> Name Type Description Default <code>clip_low</code> <code>float</code> <p>Lower clipping boundary (quantile).</p> <code>0.001</code> <code>clip_high</code> <code>float</code> <p>Upper clipping boundary (quantile).</p> <code>0.999</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Dataframe with in place clipped measurement column.</p> Source code in <code>arcos4py/_arcos4py.py</code> <pre><code>def clip_measurements(self, clip_low: float = 0.001, clip_high: float = 0.999) -&gt; pd.DataFrame:\n    \"\"\"Clip measurement column to upper and lower quantiles defined in clip_low and clip_high.\n\n    Arguments:\n        clip_low (float): Lower clipping boundary (quantile).\n\n        clip_high (float): Upper clipping boundary (quantile).\n\n    Returns:\n        Dataframe with in place clipped measurement column.\n    \"\"\"\n    meas_column = self.data[self.measurement_column].to_numpy()\n    meas_clipped = clipMeas(meas_column).clip(clip_low, clip_high)\n    self.data[self.measurement_column] = meas_clipped\n    return self.data\n</code></pre>"},{"location":"api/#arcos4py.ARCOS.interpolate_measurements","title":"<code>interpolate_measurements()</code>","text":"<p>Interpolates NaN's in place in measurement column.</p> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Dataframe with interpolated measurement column.</p> Source code in <code>arcos4py/_arcos4py.py</code> <pre><code>def interpolate_measurements(self) -&gt; pd.DataFrame:\n    \"\"\"Interpolates NaN's in place in measurement column.\n\n    Returns:\n        Dataframe with interpolated measurement column.\n    \"\"\"\n    meas_interp = interpolation(self.data).interpolate()\n    self.data = meas_interp\n    return self.data\n</code></pre>"},{"location":"api/#arcos4py.ARCOS.trackCollev","title":"<code>trackCollev(eps=1, eps_prev=None, min_clustersize=1, n_prev=1, clustering_method='dbscan', linking_method='nearest', min_samples=None, **kwargs)</code>","text":"<p>Detects and tracks collective events in a tracked time-series dataset.</p> <p>Makes use of the dbscan algorithm, applies this to every timeframe and subsequently connects collective events between frames located within eps distance of each other.</p> <p>Parameters:</p> Name Type Description Default <code>eps</code> <code>float</code> <p>The maximum distance between two samples for one to be considered as in the neighbourhood of the other. This is not a maximum bound on the distances of points within a cluster.</p> <code>1</code> <code>eps_prev</code> <code>float | None</code> <p>Frame to frame distance, value is used to connect collective events across multiple frames.If \"None\", same value as eps is used.</p> <code>None</code> <code>min_clustersize</code> <code>int</code> <p>The minimum size for a cluster to be identified as a collective event</p> <code>1</code> <code>n_prev</code> <code>int</code> <p>Number of previous frames the tracking algorithm looks back to connect collective events</p> <code>1</code> <code>clustering_method</code> <code>str</code> <p>Clustering method, one of ['dbscan', 'hdbscan'].</p> <code>'dbscan'</code> <code>min_samples</code> <code>int | None</code> <p>The number of samples (or total weight) in a neighbourhood for a point to be considered as a core point. This includes the point itself. Only used if clustering_method is 'hdbscan'. If None, min_samples =  min_clustersize.</p> <code>None</code> <code>linking_method</code> <code>str</code> <p>Linking method, one of ['nearest', 'transportation'].</p> <code>'nearest'</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments. Includes old parameter names for backwards compatibility. - epsPrev: Frame to frame distance, value is used to connect     collective events across multiple frames. - minClsz: The minimum size for a cluster to be identified as a collective event - nPrev: Number of previous frames the tracking     algorithm looks back to connect collective events - clusteringMethod: Clustering method, one of ['dbscan', 'hdbscan']. - minSamples: The number of samples (or total weight) in a neighbourhood for a     point to be considered as a core point. This includes the point itself.     Only used if clustering_method is 'hdbscan'. If None, min_samples =  min_clustersize. - linkingMethod: Linking method, one of ['nearest', 'transportation'].</p> <code>{}</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with detected collective events across time.</p> Source code in <code>arcos4py/_arcos4py.py</code> <pre><code>def trackCollev(\n    self,\n    eps: float = 1,\n    eps_prev: Union[float, None] = None,\n    min_clustersize: int = 1,\n    n_prev: int = 1,\n    clustering_method: str = \"dbscan\",\n    linking_method: str = \"nearest\",\n    min_samples: int | None = None,\n    **kwargs,\n) -&gt; pd.DataFrame:\n    \"\"\"Detects and tracks collective events in a tracked time-series dataset.\n\n    Makes use of the dbscan algorithm,\n    applies this to every timeframe and subsequently connects\n    collective events between frames located within eps distance of each other.\n\n    Arguments:\n        eps (float): The maximum distance between two samples for one to be considered as in\n            the neighbourhood of the other.\n            This is not a maximum bound on the distances of points within a cluster.\n        eps_prev (float | None): Frame to frame distance, value is used to connect\n            collective events across multiple frames.If \"None\", same value as eps is used.\n        min_clustersize (int): The minimum size for a cluster to be identified as a collective event\n        n_prev (int): Number of previous frames the tracking\n            algorithm looks back to connect collective events\n        clustering_method (str): Clustering method, one of ['dbscan', 'hdbscan'].\n        min_samples (int | None): The number of samples (or total weight) in a neighbourhood for a\n            point to be considered as a core point. This includes the point itself.\n            Only used if clustering_method is 'hdbscan'. If None, min_samples =  min_clustersize.\n        linking_method (str): Linking method, one of ['nearest', 'transportation'].\n        **kwargs (Any): Additional keyword arguments. Includes old parameter names for backwards compatibility.\n            - epsPrev: Frame to frame distance, value is used to connect\n                collective events across multiple frames.\n            - minClsz: The minimum size for a cluster to be identified as a collective event\n            - nPrev: Number of previous frames the tracking\n                algorithm looks back to connect collective events\n            - clusteringMethod: Clustering method, one of ['dbscan', 'hdbscan'].\n            - minSamples: The number of samples (or total weight) in a neighbourhood for a\n                point to be considered as a core point. This includes the point itself.\n                Only used if clustering_method is 'hdbscan'. If None, min_samples =  min_clustersize.\n            - linkingMethod: Linking method, one of ['nearest', 'transportation'].\n\n    Returns:\n        DataFrame with detected collective events across time.\n    \"\"\"\n    warnings.warn(\n        \"The 'trackCollev' method is deprecated and will be removed in a future version.\\\n            Please use 'track_collective_events' instead.\",\n        DeprecationWarning,\n        stacklevel=2,\n    )\n    return self.track_collective_events(\n        eps, eps_prev, min_clustersize, n_prev, clustering_method, linking_method, min_samples, **kwargs\n    )\n</code></pre>"},{"location":"api/#arcos4py.ARCOS.track_collective_events","title":"<code>track_collective_events(eps=1, eps_prev=None, min_clustersize=1, n_prev=1, clustering_method='dbscan', linking_method='nearest', min_samples=None, **kwargs)</code>","text":"<p>Detects and tracks collective events in a tracked time-series dataset.</p> <p>Makes use of the dbscan algorithm, applies this to every timeframe and subsequently connects collective events between frames located within eps distance of each other.</p> <p>Parameters:</p> Name Type Description Default <code>eps</code> <code>float</code> <p>The maximum distance between two samples for one to be considered as in the neighbourhood of the other. This is not a maximum bound on the distances of points within a cluster.</p> <code>1</code> <code>eps_prev</code> <code>float | None</code> <p>Frame to frame distance, value is used to connect collective events across multiple frames.If \"None\", same value as eps is used.</p> <code>None</code> <code>min_clustersize</code> <code>int</code> <p>The minimum size for a cluster to be identified as a collective event</p> <code>1</code> <code>n_prev</code> <code>int</code> <p>Number of previous frames the tracking algorithm looks back to connect collective events</p> <code>1</code> <code>clustering_method</code> <code>str</code> <p>Clustering method, one of ['dbscan', 'hdbscan'].</p> <code>'dbscan'</code> <code>min_samples</code> <code>int | None</code> <p>The number of samples (or total weight) in a neighbourhood for a point to be considered as a core point. This includes the point itself. Only used if clustering_method is 'hdbscan'. If None, min_samples =  min_clustersize.</p> <code>None</code> <code>linking_method</code> <code>str</code> <p>Linking method, one of ['nearest', 'transportation'].</p> <code>'nearest'</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments. Includes old parameter names for backwards compatibility. - epsPrev: Frame to frame distance, value is used to connect     collective events across multiple frames. - minClsz: The minimum size for a cluster to be identified as a collective event - nPrev: Number of previous frames the tracking     algorithm looks back to connect collective events - clusteringMethod: Clustering method, one of ['dbscan', 'hdbscan']. - minSamples: The number of samples (or total weight) in a neighbourhood for a     point to be considered as a core point. This includes the point itself.     Only used if clustering_method is 'hdbscan'. If None, min_samples =  min_clustersize. - linkingMethod: Linking method, one of ['nearest', 'transportation'].</p> <code>{}</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame with detected collective events across time.</p> Source code in <code>arcos4py/_arcos4py.py</code> <pre><code>def track_collective_events(\n    self,\n    eps: float = 1,\n    eps_prev: Union[float, None] = None,\n    min_clustersize: int = 1,\n    n_prev: int = 1,\n    clustering_method: str = \"dbscan\",\n    linking_method: str = \"nearest\",\n    min_samples: int | None = None,\n    **kwargs,\n) -&gt; pd.DataFrame:\n    \"\"\"Detects and tracks collective events in a tracked time-series dataset.\n\n    Makes use of the dbscan algorithm,\n    applies this to every timeframe and subsequently connects\n    collective events between frames located within eps distance of each other.\n\n    Arguments:\n        eps (float): The maximum distance between two samples for one to be considered as in\n            the neighbourhood of the other.\n            This is not a maximum bound on the distances of points within a cluster.\n        eps_prev (float | None): Frame to frame distance, value is used to connect\n            collective events across multiple frames.If \"None\", same value as eps is used.\n        min_clustersize (int): The minimum size for a cluster to be identified as a collective event\n        n_prev (int): Number of previous frames the tracking\n            algorithm looks back to connect collective events\n        clustering_method (str): Clustering method, one of ['dbscan', 'hdbscan'].\n        min_samples (int | None): The number of samples (or total weight) in a neighbourhood for a\n            point to be considered as a core point. This includes the point itself.\n            Only used if clustering_method is 'hdbscan'. If None, min_samples =  min_clustersize.\n        linking_method (str): Linking method, one of ['nearest', 'transportation'].\n        **kwargs (Any): Additional keyword arguments. Includes old parameter names for backwards compatibility.\n            - epsPrev: Frame to frame distance, value is used to connect\n                collective events across multiple frames.\n            - minClsz: The minimum size for a cluster to be identified as a collective event\n            - nPrev: Number of previous frames the tracking\n                algorithm looks back to connect collective events\n            - clusteringMethod: Clustering method, one of ['dbscan', 'hdbscan'].\n            - minSamples: The number of samples (or total weight) in a neighbourhood for a\n                point to be considered as a core point. This includes the point itself.\n                Only used if clustering_method is 'hdbscan'. If None, min_samples =  min_clustersize.\n            - linkingMethod: Linking method, one of ['nearest', 'transportation'].\n\n    Returns:\n        DataFrame with detected collective events across time.\n    \"\"\"\n    param_mapping = {\n        \"epsPrev\": \"eps_prev\",\n        \"minClsz\": \"min_clustersize\",\n        \"nPrev\": \"n_prev\",\n        \"clusteringMethod\": \"clustering_method\",\n        \"minSamples\": \"min_samples\",\n        \"linkingMethod\": \"linking_method\",\n    }\n    # allowed kwargs\n    allowed_kwargs = param_mapping.keys()\n    for key in kwargs:\n        if key not in allowed_kwargs:\n            raise ValueError(f\"track_collective_events() got an unexpected keyword argument '{key}'\")\n    updated_kwargs = handle_deprecated_params(param_mapping, **kwargs)\n\n    eps_prev = updated_kwargs.get(\"eps_prev\", eps_prev)\n    min_clustersize = updated_kwargs.get(\"min_clustersize\", min_clustersize)\n    n_prev = updated_kwargs.get(\"n_prev\", n_prev)\n    clustering_method = updated_kwargs.get(\"clustering_method\", clustering_method)\n    min_samples = updated_kwargs.get(\"min_samples\", min_samples)\n    linking_method = updated_kwargs.get(\"linking_method\", linking_method)\n\n    data_events = track_events_dataframe(\n        X=self.data,\n        position_columns=self.position_columns,\n        frame_column=self.frame_column,\n        id_column=self.obj_id_column,\n        binarized_measurement_column=self.binarized_measurement_column,\n        eps=eps,\n        eps_prev=eps_prev,\n        min_clustersize=min_clustersize,\n        n_prev=n_prev,\n        clid_column=self.clid_column,\n        linking_method=linking_method,\n        clustering_method=clustering_method,\n        min_samples=min_samples,\n        n_jobs=self.n_jobs,\n    )\n\n    return data_events\n</code></pre>"},{"location":"api/#arcos4py.track_events_dataframe","title":"<code>track_events_dataframe(X, position_columns, frame_column, id_column=None, binarized_measurement_column=None, clid_column='collid', eps=1.0, eps_prev=None, min_clustersize=3, min_samples=None, clustering_method='dbscan', linking_method='nearest', allow_merges=False, allow_splits=False, stability_threshold=10, remove_small_clusters=False, min_size_for_split=1, reg=1, reg_m=10, cost_threshold=0, n_prev=1, predictor=False, n_jobs=1, show_progress=True, **kwargs)</code>","text":"<p>Function to track collective events in a dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>The input dataframe containing the data to track.</p> required <code>position_columns</code> <code>List[str]</code> <p>The names of the columns representing coordinates.</p> required <code>frame_column</code> <code>str</code> <p>The name of the column containing frame ids.</p> required <code>id_column</code> <code>str | None</code> <p>The name of the column representing IDs. None if no such column.</p> <code>None</code> <code>binarized_measurement_column</code> <code>str | None</code> <p>The name of the column representing binarized measurements, if None all measurements are used.</p> <code>None</code> <code>clid_column</code> <code>str</code> <p>The name of the output column representing collective events, will be generated.</p> <code>'collid'</code> <code>eps</code> <code>float</code> <p>Maximum distance for clustering, default is 1.</p> <code>1.0</code> <code>eps_prev</code> <code>float | None</code> <p>Maximum distance for linking previous clusters, if None, eps is used. Default is None.</p> <code>None</code> <code>min_clustersize</code> <code>int</code> <p>Minimum cluster size. Default is 3.</p> <code>3</code> <code>min_samples</code> <code>int</code> <p>The number of samples (or total weight) in a neighbourhood for a point to be considered as a core point. This includes the point itself. Only used if clusteringMethod is 'hdbscan'. If None, minSamples =  minClsz.</p> <code>None</code> <code>clustering_method</code> <code>str</code> <p>The method used for clustering, one of [dbscan, hdbscan]. Default is \"dbscan\".</p> <code>'dbscan'</code> <code>linking_method</code> <code>str</code> <p>The method used for linking, one of ['nearest', 'transportsolver']. Default is 'nearest'.</p> <code>'nearest'</code> <code>allow_merges</code> <code>bool</code> <p>Whether or not to allow merges. Default is False.</p> <code>False</code> <code>allow_splits</code> <code>bool</code> <p>Whether or not to allow splits. Default is False.</p> <code>False</code> <code>stability_threshold</code> <code>int</code> <p>Number of frames to consider for stability. Default is 10.</p> <code>10</code> <code>remove_small_clusters</code> <code>bool</code> <p>Whether or not to remove small clusters. Default is False.</p> <code>False</code> <code>min_size_for_split</code> <code>int</code> <p>Minimum size for a split. Default is 1.</p> <code>1</code> <code>reg</code> <code>float</code> <p>Regularization parameter for transportation solver. Default is 1.</p> <code>1</code> <code>reg_m</code> <code>float</code> <p>Regularization parameter for transportation solver. Default is 10.</p> <code>10</code> <code>cost_threshold</code> <code>float</code> <p>Cost threshold for transportation solver. Default is 0.</p> <code>0</code> <code>n_prev</code> <code>int</code> <p>Number of previous frames to consider. Default is 1.</p> <code>1</code> <code>predictor</code> <code>bool | Callable</code> <p>Whether or not to use a predictor. Default is False. True uses the default predictor. A callable can be passed to use a custom predictor. See default predictor method for details.</p> <code>False</code> <code>n_jobs</code> <code>int</code> <p>Number of jobs to run in parallel. Default is 1.</p> <code>1</code> <code>show_progress</code> <code>bool</code> <p>Whether or not to show progress bar. Default is True.</p> <code>True</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments. Includes deprecated parameters for backwards compatibility. - epsPrev: Deprecated parameter for eps_prev. Use eps_prev instead. - minClSz: Deprecated parameter for min_clustersize. Use min_clustersize instead. - minSamples: Deprecated parameter for min_samples. Use min_samples instead. - clusteringMethod: Deprecated parameter for clustering_method. Use clustering_method instead. - linkingMethod: Deprecated parameter for linking_method. Use linking_method instead. - nPrev: Deprecated parameter for n_prev. Use n_prev instead. - nJobs: Deprecated parameter for n_jobs. Use n_jobs instead. - showProgress: Deprecated parameter for show_progress. Use show_progress instead.</p> <code>{}</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Dataframe with tracked events.</p> Source code in <code>arcos4py/tools/_detect_events.py</code> <pre><code>def track_events_dataframe(\n    X: pd.DataFrame,\n    position_columns: List[str],\n    frame_column: str,\n    id_column: str | None = None,\n    binarized_measurement_column: str | None = None,\n    clid_column: str = \"collid\",\n    eps: float = 1.0,\n    eps_prev: float | None = None,\n    min_clustersize: int = 3,\n    min_samples: int | None = None,\n    clustering_method: str = \"dbscan\",\n    linking_method: str = 'nearest',\n    allow_merges: bool = False,\n    allow_splits: bool = False,\n    stability_threshold: int = 10,\n    remove_small_clusters: bool = False,\n    min_size_for_split: int = 1,\n    reg: float = 1,\n    reg_m: float = 10,\n    cost_threshold: float = 0,\n    n_prev: int = 1,\n    predictor: bool | Callable = False,\n    n_jobs: int = 1,\n    show_progress: bool = True,\n    **kwargs,\n) -&gt; pd.DataFrame:\n    \"\"\"Function to track collective events in a dataframe.\n\n    Arguments:\n        X (pd.DataFrame): The input dataframe containing the data to track.\n        position_columns (List[str]): The names of the columns representing coordinates.\n        frame_column (str): The name of the column containing frame ids.\n        id_column (str | None): The name of the column representing IDs. None if no such column.\n        binarized_measurement_column (str | None): The name of the column representing binarized measurements,\n            if None all measurements are used.\n        clid_column (str): The name of the output column representing collective events, will be generated.\n        eps (float): Maximum distance for clustering, default is 1.\n        eps_prev (float | None): Maximum distance for linking previous clusters, if None, eps is used. Default is None.\n        min_clustersize (int): Minimum cluster size. Default is 3.\n        min_samples (int): The number of samples (or total weight) in a neighbourhood for a\n            point to be considered as a core point. This includes the point itself.\n            Only used if clusteringMethod is 'hdbscan'. If None, minSamples =  minClsz.\n        clustering_method (str): The method used for clustering, one of [dbscan, hdbscan]. Default is \"dbscan\".\n        linking_method (str): The method used for linking, one of ['nearest', 'transportsolver']. Default is 'nearest'.\n        allow_merges (bool): Whether or not to allow merges. Default is False.\n        allow_splits (bool): Whether or not to allow splits. Default is False.\n        stability_threshold (int): Number of frames to consider for stability. Default is 10.\n        remove_small_clusters (bool): Whether or not to remove small clusters. Default is False.\n        min_size_for_split (int): Minimum size for a split. Default is 1.\n        reg (float): Regularization parameter for transportation solver. Default is 1.\n        reg_m (float): Regularization parameter for transportation solver. Default is 10.\n        cost_threshold (float): Cost threshold for transportation solver. Default is 0.\n        n_prev (int): Number of previous frames to consider. Default is 1.\n        predictor (bool | Callable): Whether or not to use a predictor. Default is False.\n            True uses the default predictor. A callable can be passed to use a custom predictor.\n            See default predictor method for details.\n        n_jobs (int): Number of jobs to run in parallel. Default is 1.\n        show_progress (bool): Whether or not to show progress bar. Default is True.\n        **kwargs (Any): Additional keyword arguments. Includes deprecated parameters for backwards compatibility.\n            - epsPrev: Deprecated parameter for eps_prev. Use eps_prev instead.\n            - minClSz: Deprecated parameter for min_clustersize. Use min_clustersize instead.\n            - minSamples: Deprecated parameter for min_samples. Use min_samples instead.\n            - clusteringMethod: Deprecated parameter for clustering_method. Use clustering_method instead.\n            - linkingMethod: Deprecated parameter for linking_method. Use linking_method instead.\n            - nPrev: Deprecated parameter for n_prev. Use n_prev instead.\n            - nJobs: Deprecated parameter for n_jobs. Use n_jobs instead.\n            - showProgress: Deprecated parameter for show_progress. Use show_progress instead.\n\n    Returns:\n        pd.DataFrame: Dataframe with tracked events.\n    \"\"\"\n    map_params = {\n        \"coordinates_column\": \"position_columns\",\n        \"bin_meas_column\": \"binarized_measurement_column\",\n        \"collid_column\": \"clid_column\",\n        'epsPrev': 'eps_prev',\n        'minClSz': 'min_clustersize',\n        'minSamples': 'min_samples',\n        'clusteringMethod': 'clustering_method',\n        'linkingMethod': 'linking_method',\n        'nPrev': 'n_prev',\n        'nJobs': 'n_jobs',\n        'showProgress': 'show_progress',\n    }\n\n    # check for allowed kwargs\n    for key in kwargs:\n        if key not in map_params.keys():\n            raise ValueError(f'Invalid keyword argument {key}')\n\n    # Handle deprecated parameters\n    kwargs = handle_deprecated_params(map_params, **kwargs)\n\n    # Assign parameters\n    eps_prev = kwargs.get('eps_prev', eps_prev)\n    min_clustersize = kwargs.get('min_clustersize', min_clustersize)\n    min_samples = kwargs.get('min_samples', min_samples)\n    clustering_method = kwargs.get('clustering_method', clustering_method)\n    linking_method = kwargs.get('linking_method', linking_method)\n    n_prev = kwargs.get('n_prev', n_prev)\n    n_jobs = kwargs.get('n_jobs', n_jobs)\n\n    linker = Linker(\n        eps=eps,\n        eps_prev=eps_prev,\n        min_clustersize=min_clustersize,\n        min_samples=min_samples,\n        clustering_method=clustering_method,\n        linking_method=linking_method,\n        n_prev=n_prev,\n        predictor=predictor,\n        n_jobs=n_jobs,\n        allow_merges=allow_merges,\n        allow_splits=allow_splits,\n        stability_threshold=stability_threshold,\n        remove_small_clusters=remove_small_clusters,\n        min_size_for_split=min_size_for_split,\n        reg=reg,\n        reg_m=reg_m,\n        cost_threshold=cost_threshold,\n    )\n\n    tracker = DataFrameTracker(\n        linker=linker,\n        position_columns=position_columns,\n        frame_column=frame_column,\n        obj_id_column=id_column,\n        binarized_measurement_column=binarized_measurement_column,\n        clid_column=clid_column,\n    )\n    df_out = pd.concat(\n        [timepoint for timepoint in tqdm(tracker.track(X), total=X[frame_column].nunique(), disable=not show_progress)]\n    ).reset_index(drop=True)\n\n    if any([allow_merges, allow_splits]):\n        return df_out.query(f\"{clid_column} != -1\").reset_index(drop=True), linker.lineage_tracker\n    return df_out.query(f\"{clid_column} != -1\").reset_index(drop=True)\n</code></pre>"},{"location":"api/#arcos4py.track_events_image","title":"<code>track_events_image(X, eps=1, eps_prev=None, min_clustersize=1, min_samples=None, clustering_method='dbscan', n_prev=1, predictor=False, linking_method='nearest', allow_merges=False, allow_splits=False, stability_threshold=10, remove_small_clusters=False, min_size_for_split=1, reg=1, reg_m=10, cost_threshold=0, dims='TXY', downsample=1, n_jobs=1, show_progress=True, **kwargs)</code>","text":"<p>Function to track events in an image using specified linking and clustering methods.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>The input array containing the images to track.</p> required <code>eps</code> <code>float</code> <p>Distance for clustering. Default is 1.</p> <code>1</code> <code>eps_prev</code> <code>float | None</code> <p>Maximum distance for linking previous clusters, if None, eps is used. Default is None.</p> <code>None</code> <code>min_clustersize</code> <code>int</code> <p>Minimum cluster size. Default is 1.</p> <code>1</code> <code>min_samples</code> <code>int | None</code> <p>The number of samples (or total weight) in a neighbourhood for a point to be considered as a core point. This includes the point itself. Only used if clusteringMethod is 'hdbscan'. If None, minSamples =  minClsz.</p> <code>None</code> <code>clustering_method</code> <code>str</code> <p>The method used for clustering, one of [dbscan, hdbscan]. Default is \"dbscan\".</p> <code>'dbscan'</code> <code>n_prev</code> <code>int</code> <p>Number of previous frames to consider. Default is 1.</p> <code>1</code> <code>predictor</code> <code>bool | Callable</code> <p>Whether or not to use a predictor. Default is False. True uses the default predictor. A callable can be passed to use a custom predictor. See default predictor method for details.</p> <code>False</code> <code>linking_method</code> <code>str</code> <p>The method used for linking. Default is 'nearest'.</p> <code>'nearest'</code> <code>allow_merges</code> <code>bool</code> <p>Whether or not to allow merges. Default is False.</p> <code>False</code> <code>allow_splits</code> <code>bool</code> <p>Whether or not to allow splits. Default is False.</p> <code>False</code> <code>stability_threshold</code> <code>int</code> <p>The number of frames required for a stable merge or split. Default is 10.</p> <code>10</code> <code>remove_small_clusters</code> <code>bool</code> <p>Whether or not to remove small clusters. Default is False.</p> <code>False</code> <code>min_size_for_split</code> <code>int</code> <p>Minimum size for a split. Default is 1.</p> <code>1</code> <code>reg</code> <code>float</code> <p>Entropy regularization parameter for unbalanced OT algorithm (only for transportation linking).</p> <code>1</code> <code>reg_m</code> <code>float</code> <p>Marginal relaxation parameter for unbalanced OT (only for transportation linking).</p> <code>10</code> <code>cost_threshold</code> <code>float</code> <p>Threshold for filtering low-probability matches (only for transportation linking).</p> <code>0</code> <code>dims</code> <code>str</code> <p>String of dimensions in order, such as. Default is \"TXY\". Possible values are \"T\", \"X\", \"Y\", \"Z\".</p> <code>'TXY'</code> <code>downsample</code> <code>int</code> <p>Factor by which to downsample the image. Default is 1.</p> <code>1</code> <code>n_jobs</code> <code>int</code> <p>Number of jobs to run in parallel. Default is 1.</p> <code>1</code> <code>show_progress</code> <code>bool</code> <p>Whether or not to show progress bar. Default is True.</p> <code>True</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments. Includes deprecated parameters for backwards compatibility. - epsPrev: Deprecated parameter for eps_prev. Use eps_prev instead. - minClSz: Deprecated parameter for min_clustersize. Use min_clustersize instead. - minSamples: Deprecated parameter for min_samples. Use min_samples instead. - clusteringMethod: Deprecated parameter for clustering_method. Use clustering_method instead. - linkingMethod: Deprecated parameter for linking_method. Use linking_method instead. - nPrev: Deprecated parameter for n_prev. Use n_prev instead. - nJobs: Deprecated parameter for n_jobs. Use n_jobs instead. - showProgress: Deprecated parameter for show_progress. Use show_progress instead.</p> <code>{}</code> <p>Returns:</p> Type Description <code>ndarray | tuple[ndarray, LineageTracker]</code> <p>np.ndarray: Array of images with tracked events.</p> Source code in <code>arcos4py/tools/_detect_events.py</code> <pre><code>def track_events_image(\n    X: np.ndarray,\n    eps: float = 1,\n    eps_prev: float | None = None,\n    min_clustersize: int = 1,\n    min_samples: int | None = None,\n    clustering_method: str = \"dbscan\",\n    n_prev: int = 1,\n    predictor: bool | Callable = False,\n    linking_method: str = 'nearest',\n    allow_merges: bool = False,\n    allow_splits: bool = False,\n    stability_threshold: int = 10,\n    remove_small_clusters: bool = False,\n    min_size_for_split: int = 1,\n    reg: float = 1,\n    reg_m: float = 10,\n    cost_threshold: float = 0,\n    dims: str = \"TXY\",\n    downsample: int = 1,\n    n_jobs: int = 1,\n    show_progress: bool = True,\n    **kwargs,\n) -&gt; np.ndarray | tuple[np.ndarray, LineageTracker]:\n    \"\"\"Function to track events in an image using specified linking and clustering methods.\n\n    Arguments:\n        X (np.ndarray): The input array containing the images to track.\n        eps (float): Distance for clustering. Default is 1.\n        eps_prev (float | None): Maximum distance for linking previous clusters, if None, eps is used. Default is None.\n        min_clustersize (int): Minimum cluster size. Default is 1.\n        min_samples (int | None): The number of samples (or total weight) in a neighbourhood for a\n            point to be considered as a core point. This includes the point itself.\n            Only used if clusteringMethod is 'hdbscan'. If None, minSamples =  minClsz.\n        clustering_method (str): The method used for clustering, one of [dbscan, hdbscan]. Default is \"dbscan\".\n        n_prev (int): Number of previous frames to consider. Default is 1.\n        predictor (bool | Callable): Whether or not to use a predictor. Default is False.\n            True uses the default predictor. A callable can be passed to use a custom predictor.\n            See default predictor method for details.\n        linking_method (str): The method used for linking. Default is 'nearest'.\n        allow_merges (bool): Whether or not to allow merges. Default is False.\n        allow_splits (bool): Whether or not to allow splits. Default is False.\n        stability_threshold (int): The number of frames required for a stable merge or split. Default is 10.\n        remove_small_clusters (bool): Whether or not to remove small clusters. Default is False.\n        min_size_for_split (int): Minimum size for a split. Default is 1.\n        reg (float): Entropy regularization parameter for unbalanced OT algorithm (only for transportation linking).\n        reg_m (float): Marginal relaxation parameter for unbalanced OT (only for transportation linking).\n        cost_threshold (float): Threshold for filtering low-probability matches (only for transportation linking).\n        dims (str): String of dimensions in order, such as. Default is \"TXY\". Possible values are \"T\", \"X\", \"Y\", \"Z\".\n        downsample (int): Factor by which to downsample the image. Default is 1.\n        n_jobs (int): Number of jobs to run in parallel. Default is 1.\n        show_progress (bool): Whether or not to show progress bar. Default is True.\n        **kwargs (Any): Additional keyword arguments. Includes deprecated parameters for backwards compatibility.\n            - epsPrev: Deprecated parameter for eps_prev. Use eps_prev instead.\n            - minClSz: Deprecated parameter for min_clustersize. Use min_clustersize instead.\n            - minSamples: Deprecated parameter for min_samples. Use min_samples instead.\n            - clusteringMethod: Deprecated parameter for clustering_method. Use clustering_method instead.\n            - linkingMethod: Deprecated parameter for linking_method. Use linking_method instead.\n            - nPrev: Deprecated parameter for n_prev. Use n_prev instead.\n            - nJobs: Deprecated parameter for n_jobs. Use n_jobs instead.\n            - showProgress: Deprecated parameter for show_progress. Use show_progress instead.\n\n    Returns:\n        np.ndarray: Array of images with tracked events.\n    \"\"\"\n    map_params = {\n        'epsPrev': 'eps_prev',\n        'minClSz': 'min_clustersize',\n        'minSamples': 'min_samples',\n        'clusteringMethod': 'clustering_method',\n        'linkingMethod': 'linking_method',\n        'nPrev': 'n_prev',\n        'nJobs': 'n_jobs',\n        'showProgress': 'show_progress',\n    }\n\n    # check for allowed kwargs\n    for key in kwargs:\n        if key not in map_params.keys():\n            raise ValueError(f'Invalid keyword argument {key}')\n\n    # Handle deprecated parameters\n    kwargs = handle_deprecated_params(map_params, **kwargs)\n\n    # Assign parameters\n    eps_prev = kwargs.get('eps_prev', eps_prev)\n    min_clustersize = kwargs.get('min_clustersize', min_clustersize)\n    min_samples = kwargs.get('min_samples', min_samples)\n    clustering_method = kwargs.get('clustering_method', clustering_method)\n    linking_method = kwargs.get('linking_method', linking_method)\n    n_prev = kwargs.get('n_prev', n_prev)\n    n_jobs = kwargs.get('n_jobs', n_jobs)\n\n    # Determine the dimensionality\n    spatial_dims = set(\"XYZ\")\n    D = len([d for d in dims if d in spatial_dims])\n\n    # Adjust parameters based on dimensionality\n    adjusted_epsPrev = eps_prev / downsample if eps_prev is not None else None\n    adjusted_minClSz = int(min_clustersize / (downsample**D))\n    adjusted_minSamples = int(min_samples / (downsample**D)) if min_samples is not None else None\n\n    linker = Linker(\n        eps=eps / downsample,\n        eps_prev=adjusted_epsPrev,\n        min_clustersize=adjusted_minClSz,\n        min_samples=adjusted_minSamples,\n        clustering_method=clustering_method,\n        linking_method=linking_method,\n        n_prev=n_prev,\n        predictor=predictor,\n        reg=reg,\n        reg_m=reg_m,\n        cost_threshold=cost_threshold,\n        n_jobs=n_jobs,\n        allow_merges=allow_merges,\n        allow_splits=allow_splits,\n        stability_threshold=stability_threshold,\n        remove_small_clusters=remove_small_clusters,\n        min_size_for_split=min_size_for_split,\n    )\n    tracker = ImageTracker(linker, downsample=downsample)\n    # find indices of T in dims\n    T_index = dims.upper().index(\"T\")\n    out = np.zeros_like(X, dtype=np.uint16)\n\n    for i in tqdm(range(X.shape[T_index]), disable=not show_progress):\n        out[i] = tracker.track_iteration(X[i])\n\n    if any([allow_merges, allow_splits]):\n        return out, linker.lineage_tracker\n\n    return out\n</code></pre>"},{"location":"api/#arcos4py.plotting","title":"<code>plotting</code>","text":"<p>Tools for plotting collective events.</p>"},{"location":"api/#arcos4py.plotting.LineagePlot","title":"<code>LineagePlot(figsize=(18, 18), node_size=50, edge_width=2, edge_alpha=0.8, color_seed=42, title='Cluster Lineage Tree', xlabel='Frame', ylabel='Lineage', font_size=16, curve_factor=0.9, orphan_color=(0.7, 0.7, 0.7, 1.0), color_by='lineage_id', show_node_labels=False, main_lineage_id=None)</code>","text":"<p>Class to draw a lineage tree of clusters over time.</p> <p>Attributes:</p> Name Type Description <code>figsize</code> <code>tuple</code> <p>Size of the figure.</p> <code>node_size</code> <code>int</code> <p>Size of the nodes.</p> <code>edge_width</code> <code>int</code> <p>Width of the edges.</p> <code>edge_alpha</code> <code>float</code> <p>Alpha value of the edges.</p> <code>color_seed</code> <code>int</code> <p>Seed for the color generation.</p> <code>title</code> <code>str</code> <p>Title of the plot.</p> <code>xlabel</code> <code>str</code> <p>Label of the x-axis.</p> <code>ylabel</code> <code>str</code> <p>Label of the y-axis.</p> <code>font_size</code> <code>int</code> <p>Font size of the labels.</p> <code>curve_factor</code> <code>float</code> <p>Factor to curve the edges.</p> <code>orphan_color</code> <code>tuple</code> <p>Color of the orphan nodes.</p> <code>color_by</code> <code>str</code> <p>Attribute to color the plot by ('lineage_id' or 'cluster_id').</p> <code>show_node_labels</code> <code>bool</code> <p>If True, display node labels on the plot.</p> <code>main_lineage_id</code> <code>int</code> <p>The lineage ID of the main lineage to be plotted on the same row.</p> Source code in <code>arcos4py/plotting/_plotting.py</code> <pre><code>def __init__(\n    self,\n    figsize=(18, 18),\n    node_size=50,\n    edge_width=2,\n    edge_alpha=0.8,\n    color_seed=42,\n    title=\"Cluster Lineage Tree\",\n    xlabel=\"Frame\",\n    ylabel=\"Lineage\",\n    font_size=16,\n    curve_factor=0.9,\n    orphan_color=(0.7, 0.7, 0.7, 1.0),\n    color_by='lineage_id',  # 'lineage_id' or 'cluster_id'\n    show_node_labels=False,  # Whether to display node labels\n    main_lineage_id=None,  # The main lineage to keep on the same row\n):\n    \"\"\"Constructs class with given parameters.\"\"\"\n    self.fig, self.ax = plt.subplots(figsize=figsize)\n    self.node_size = node_size\n    self.edge_width = edge_width\n    self.edge_alpha = edge_alpha\n    self.title = title\n    self.xlabel = xlabel\n    self.ylabel = ylabel\n    self.font_size = font_size\n    self.curve_factor = curve_factor\n    self.orphan_color = orphan_color\n    self.color_seed = color_seed\n    self.color_by = color_by\n    self.show_node_labels = show_node_labels\n    self.main_lineage_id = main_lineage_id  # Store the main lineage ID\n\n    self.colors: Dict[int, Tuple[float, float, float, float]] = {}\n    self.node_positions: Dict[Tuple[int, int], Tuple[float, float]] = {}\n    self.lineage_edges: List[Tuple[Tuple[int, int], Tuple[int, int]]] = []\n    self.node_color: Dict[Tuple[int, int], Tuple[float, float, float, float]] = {}\n    self.child_to_parent: Dict[Tuple[int, int], Set[Tuple[int, int]]] = {}\n    self.parent_to_child: Dict[Tuple[int, int], Set[Tuple[int, int]]] = {}\n    self.lineage_order: List[int] = []\n    self.all_nodes: Set[Tuple[int, int]] = set()\n    self.frame_to_nodes: Dict[int, List[Tuple[int, int]]] = defaultdict(list)\n    self.node_to_lineage_id: Dict[Tuple[int, int], int] = {}\n    self.node_to_cluster_id: Dict[Tuple[int, int], int] = {}\n    self.node_to_plot_lineage_id: Dict[Tuple[int, int], int] = {}\n    self.plot_lineage_id_to_lineage_id: Dict[int, int] = {}  # New mapping\n</code></pre>"},{"location":"api/#arcos4py.plotting.LineagePlot.draw_tree","title":"<code>draw_tree(tracker)</code>","text":"<p>Draw the lineage tree based on the processed data.</p> Source code in <code>arcos4py/plotting/_plotting.py</code> <pre><code>def draw_tree(self, tracker):\n    \"\"\"Draw the lineage tree based on the processed data.\"\"\"\n    self._process_data(tracker)\n\n    # Draw edges with assigned coloring\n    for source, target in self.lineage_edges:\n        source_pos = self.node_positions[source]\n        target_pos = self.node_positions[target]\n        color = self.node_color.get(source, self.orphan_color)\n        self._draw_curved_edge(source_pos, target_pos, color)\n\n    # Draw nodes\n    for node in self.minframe_nodes:\n        pos = self.node_positions[node]\n        color = self.node_color.get(node, self.orphan_color)\n        self.ax.scatter(pos[0], pos[1], s=self.node_size, c=[color], zorder=2)\n        if self.show_node_labels:\n            label = f\"{node[1]}\"  # Use cluster_id as label\n            self.ax.text(pos[0], pos[1], label, fontsize=self.font_size - 4, ha='right', va='bottom')\n\n    # Set labels and title\n    self.ax.set_title(self.title, fontsize=self.font_size)\n    self.ax.set_xlabel(self.xlabel, fontsize=self.font_size - 2)\n    self.ax.set_ylabel(self.ylabel, fontsize=self.font_size - 2)\n\n    # Set y-ticks to plotting lineage IDs with true lineage IDs as labels\n    lineage_ids = self.lineage_order\n    plot_lineage_id_to_y = {lineage_id: idx for idx, lineage_id in enumerate(lineage_ids)}\n    max_idx = len(lineage_ids) - 1 if len(lineage_ids) &gt; 0 else 1\n    y_ticks = [plot_lineage_id_to_y[lineage_id] / max_idx if max_idx &gt; 0 else 0.5 for lineage_id in lineage_ids]\n    # Get corresponding true lineage IDs for labels\n    y_tick_labels = [self.plot_lineage_id_to_lineage_id[lineage_id] for lineage_id in lineage_ids]\n    self.ax.set_yticks(y_ticks)\n    self.ax.set_yticklabels(y_tick_labels, fontsize=self.font_size - 4)\n    plt.tight_layout()\n</code></pre>"},{"location":"api/#arcos4py.plotting.LineagePlot.show","title":"<code>show()</code>","text":"<p>Display the plot.</p> Source code in <code>arcos4py/plotting/_plotting.py</code> <pre><code>def show(self):\n    \"\"\"Display the plot.\"\"\"\n    plt.show()\n</code></pre>"},{"location":"api/#arcos4py.plotting.NoodlePlot","title":"<code>NoodlePlot(df, clid_column='collid', obj_id_column='obj_id', frame_column='frame', posx='x', posy='y', posz=None, **kwargs)</code>","text":"<p>Create Noodle Plot of cell tracks, colored by collective event id.</p> <p>Attributes:</p> Name Type Description <code>df</code> <code>DataFrame</code> <p>DataFrame containing collective events from arcos.</p> <code>colev</code> <code>str</code> <p>Name of the collective event column in df.</p> <code>trackid</code> <code>str</code> <p>Name of the track column in df.</p> <code>frame</code> <code>str</code> <p>Name of the frame column in df.</p> <code>posx</code> <code>str</code> <p>Name of the X coordinate column in df.</p> <code>posy</code> <code>str</code> <p>Name of the Y coordinate column in df.</p> <code>posz</code> <code>str</code> <p>Name of the Z coordinate column in df, or None if no z column.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame containing collective events from arcos.</p> required <code>clid_column</code> <code>str</code> <p>Name of the collective event column in df.</p> <code>'collid'</code> <code>obj_id_column</code> <code>str</code> <p>Name of the track column in df.</p> <code>'obj_id'</code> <code>frame_column</code> <code>str</code> <p>Name of the frame column in df.</p> <code>'frame'</code> <code>posx</code> <code>str</code> <p>Name of the X coordinate column in df.</p> <code>'x'</code> <code>posy</code> <code>str</code> <p>Name of the Y coordinate column in df.</p> <code>'y'</code> <code>posz</code> <code>str | None</code> <p>Name of the Z coordinate column in df, or None if no z column.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments for plot. Includes deprecated parameters. - colev (str): Deprecated. Use clid_column instead. - trackid (str): Deprecated. Use obj_id_column instead. - frame (str): Deprecated. Use frame_column instead.</p> <code>{}</code> Source code in <code>arcos4py/plotting/_plotting.py</code> <pre><code>def __init__(\n    self,\n    df: pd.DataFrame,\n    clid_column: str = \"collid\",\n    obj_id_column: str = \"obj_id\",\n    frame_column: str = \"frame\",\n    posx: str = \"x\",\n    posy: str = \"y\",\n    posz: Union[str, None] = None,\n    **kwargs,\n):\n    \"\"\"Constructs class with given parameters.\n\n    Arguments:\n        df (pd.DataFrame): DataFrame containing collective events from arcos.\n        clid_column (str): Name of the collective event column in df.\n        obj_id_column (str): Name of the track column in df.\n        frame_column (str): Name of the frame column in df.\n        posx (str): Name of the X coordinate column in df.\n        posy (str): Name of the Y coordinate column in df.\n        posz (str | None): Name of the Z coordinate column in df,\n            or None if no z column.\n        **kwargs (Any): Additional keyword arguments for plot. Includes deprecated parameters.\n            - colev (str): Deprecated. Use clid_column instead.\n            - trackid (str): Deprecated. Use obj_id_column instead.\n            - frame (str): Deprecated. Use frame_column instead.\n    \"\"\"\n    map_deprecated_params = {\n        \"colev\": \"clid_column\",\n        \"trackid\": \"obj_id_column\",\n        \"frame\": \"frame_column\",\n    }\n\n    # allowed matplotlib kwargs\n    allowed_kwargs = [\n        \"alpha\",\n        \"animated\",\n        \"c\",\n        \"label\",\n        \"linewidth\",\n        \"linestyle\",\n        \"marker\",\n        \"markersize\",\n        \"markeredgecolor\",\n        \"markerfacecolor\",\n        \"markerfacecoloralt\",\n        \"markeredgewidth\",\n        \"path_effects\",\n        \"picker\",\n        \"pickradius\",\n        \"solid_capstyle\",\n        \"solid_joinstyle\",\n        \"transform\",\n        \"visible\",\n        \"zorder\",\n    ]\n\n    # check allowed kwargs\n    allowed_kwargs_2 = map_deprecated_params.keys()\n    for key in kwargs:\n        if key not in allowed_kwargs and key not in allowed_kwargs_2:\n            raise ValueError(f\"Got an unexpected keyword argument '{key}'\")\n\n    updated_kwargs = handle_deprecated_params(map_deprecated_params, **kwargs)\n\n    # Assigning the parameters\n    clid_column = updated_kwargs.pop(\"clid_column\", clid_column)\n    obj_id_column = updated_kwargs.pop(\"obj_id_column\", obj_id_column)\n    frame_column = updated_kwargs.pop(\"frame_column\", frame_column)\n\n    self.df = df\n    self.clid_column = clid_column\n    self.obj_id_column = obj_id_column\n    self.frame_column = frame_column\n    self.posx = posx\n    self.posy = posy\n    self.posz = posz\n    self.plot_kwargs = updated_kwargs\n</code></pre>"},{"location":"api/#arcos4py.plotting.NoodlePlot.plot","title":"<code>plot(projection_axis, color_cylce=TAB20)</code>","text":"<p>Create Noodle Plot of cell tracks, colored by collective event id.</p> <p>Parameters:</p> Name Type Description Default <code>projection_axis</code> <code>str</code> <p>Specify with witch coordinate the noodle plot should be drawn. Has to be one of the posx, posy or posz arguments passed in during the class instantiation process.</p> required <code>color_cylce</code> <code>list[str]</code> <p>List of hex color values or string names (i.e. ['red', 'yellow']) used to color collecitve events. Cycles through list.</p> <code>TAB20</code> <p>Returns:</p> Name Type Description <code>fig</code> <code>Figure</code> <p>Matplotlib figure object for the noodle plot.</p> <code>axes</code> <code>Axes</code> <p>Matplotlib axes for the nooble plot.</p> Source code in <code>arcos4py/plotting/_plotting.py</code> <pre><code>def plot(self, projection_axis: str, color_cylce: list[str] = TAB20):\n    \"\"\"Create Noodle Plot of cell tracks, colored by collective event id.\n\n    Arguments:\n        projection_axis (str): Specify with witch coordinate the noodle\n            plot should be drawn. Has to be one of the posx, posy or posz arguments\n            passed in during the class instantiation process.\n        color_cylce (list[str]): List of hex color values or string names\n            (i.e. ['red', 'yellow']) used to color collecitve events. Cycles through list.\n\n    Returns:\n        fig (matplotlib.figure.Figure): Matplotlib figure object for the noodle plot.\n        axes (matplotlib.axes.Axes): Matplotlib axes for the nooble plot.\n    \"\"\"\n    if projection_axis not in [self.posx, self.posy, self.posz]:\n        raise ValueError(f\"projection_axis has to be one of {[self.posx, self.posy, self.posz]}\")\n    if projection_axis == self.posx:\n        self.projection_index = 3\n    elif projection_axis == self.posy:\n        self.projection_index = 4\n    elif projection_axis == self.posz:\n        self.projection_index = 5\n    if self.df.empty:\n        grpd_data: list[np.ndarray] = []\n        colors: np.ndarray = np.array([])\n    else:\n        grpd_data, colors = self._prepare_data_noodleplot(\n            self.df,\n            color_cylce,\n            self.clid_column,\n            self.obj_id_column,\n            self.frame_column,\n            self.posx,\n            self.posy,\n            self.posz,\n        )\n    fig, axes = self._create_noodle_plot(grpd_data, colors)\n    return fig, axes\n</code></pre>"},{"location":"api/#arcos4py.plotting.dataPlots","title":"<code>dataPlots(data, frame_column='frame', measurement_column='m', obj_id_column='obj_id', **kwargs)</code>","text":"<p>Plot different metrics of input data.</p> <p>Attributes:</p> Name Type Description <code>data</code> <code>Dataframe</code> <p>containing ARCOS data.</p> <code>frame_column</code> <code>str</code> <p>name of frame column in data.</p> <code>measurement_column</code> <code>str</code> <p>name of measurement column in data.</p> <code>obj_id_column</code> <code>str</code> <p>name of track id column.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Dataframe</code> <p>containing ARCOS data.</p> required <code>frame_column</code> <code>str</code> <p>name of frame column in data.</p> <code>'frame'</code> <code>measurement_column</code> <code>str</code> <p>name of measurement column in data.</p> <code>'m'</code> <code>obj_id_column</code> <code>str</code> <p>name of track id column.</p> <code>'obj_id'</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments. Includes deprecated parameters. - id (str): Deprecated. Use obj_id_column instead. - frame (str): Deprecated. Use frame_column instead. - measurement (str): Deprecated. Use measurement_column instead.</p> <code>{}</code> Source code in <code>arcos4py/plotting/_plotting.py</code> <pre><code>def __init__(\n    self,\n    data: pd.DataFrame,\n    frame_column: str = 'frame',\n    measurement_column: str = 'm',\n    obj_id_column: str = 'obj_id',\n    **kwargs,\n):\n    \"\"\"Plot different metrics such as histogram, position-t and density.\n\n    Arguments:\n        data (Dataframe): containing ARCOS data.\n        frame_column (str): name of frame column in data.\n        measurement_column (str): name of measurement column in data.\n        obj_id_column (str): name of track id column.\n        **kwargs (Any): Additional keyword arguments. Includes deprecated parameters.\n            - id (str): Deprecated. Use obj_id_column instead.\n            - frame (str): Deprecated. Use frame_column instead.\n            - measurement (str): Deprecated. Use measurement_column instead.\n    \"\"\"\n    map_deprecated_params = {\n        \"id\": \"obj_id_column\",\n        \"frame\": \"frame_column\",\n        \"measurement\": \"measurement_column\",\n    }\n\n    # check allowed kwargs\n    allowed_kwargs = map_deprecated_params.keys()\n    for key in kwargs:\n        if key not in allowed_kwargs:\n            raise ValueError(f\"Got an unexpected keyword argument '{key}'\")\n\n    updated_kwargs = handle_deprecated_params(map_deprecated_params, **kwargs)\n\n    # Assigning the parameters\n    obj_id_column = updated_kwargs.get(\"obj_id_column\", obj_id_column)\n    frame_column = updated_kwargs.get(\"frame_column\", frame_column)\n    measurement_column = updated_kwargs.get(\"measurement_column\", measurement_column)\n\n    self.data = data\n    self.obj_id = obj_id_column\n    self.frame_column = frame_column\n    self.measurement_column = measurement_column\n</code></pre>"},{"location":"api/#arcos4py.plotting.dataPlots.density_plot","title":"<code>density_plot(*args, **kwargs)</code>","text":"<p>Density plot of measurement.</p> <p>Uses Seaborn distplot to plot measurement density.</p> <p>Parameters:</p> Name Type Description Default <code>*args</code> <code>Any</code> <p>arguments passed on to seaborn histplot function.</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>keyword arguments passed on to seaborn histplot function.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>FacetGrid</code> <code>FacetGrid</code> <p>Seaborn FacetGrid of density density plot.</p> Source code in <code>arcos4py/plotting/_plotting.py</code> <pre><code>def density_plot(self, *args, **kwargs):\n    \"\"\"Density plot of measurement.\n\n    Uses Seaborn distplot to plot measurement density.\n\n    Arguments:\n        *args (Any): arguments passed on to seaborn histplot function.\n        **kwargs (Any): keyword arguments passed on to seaborn histplot function.\n\n    Returns:\n        FacetGrid (seaborn.FacetGrid): Seaborn FacetGrid of density density plot.\n    \"\"\"\n    plot = sns.displot(\n        self.data[self.measurement_column],\n        kind=\"kde\",\n        palette=\"pastel\",\n        label=self.measurement_column,\n        *args,\n        **kwargs,\n    )\n    # Plot formatting\n    plt.legend(prop={'size': 10})\n    plt.title('Density Plot of Measurement')\n    plt.xlabel('Measurement')\n    plt.ylabel('Density')\n    return plot\n</code></pre>"},{"location":"api/#arcos4py.plotting.dataPlots.histogram","title":"<code>histogram(bins='auto', *args, **kwargs)</code>","text":"<p>Histogram of tracklenght.</p> <p>Uses seaborn histplot function to plot tracklenght histogram.</p> <p>Parameters:</p> Name Type Description Default <code>bins</code> <code>str</code> <p>number or width of bins in histogram</p> <code>'auto'</code> <code>*args</code> <code>Any</code> <p>arguments passed on to seaborn histplot function.</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>keyword arguments passed on to seaborn histplot function.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>AxesSubplot</code> <code>Axes</code> <p>Matplotlib AxesSubplot of histogram.</p> Source code in <code>arcos4py/plotting/_plotting.py</code> <pre><code>def histogram(self, bins: str = 'auto', *args, **kwargs) -&gt; plt.Axes:\n    \"\"\"Histogram of tracklenght.\n\n    Uses seaborn histplot function to plot tracklenght histogram.\n\n    Arguments:\n        bins (str): number or width of bins in histogram\n        *args (Any): arguments passed on to seaborn histplot function.\n        **kwargs (Any): keyword arguments passed on to seaborn histplot function.\n\n    Returns:\n        AxesSubplot: Matplotlib AxesSubplot of histogram.\n    \"\"\"\n    # Draw histogram\n    track_length = self.data.groupby(self.obj_id).size()\n    axes = sns.histplot(track_length, label=\"Track Length\", bins=bins, *args, **kwargs)\n    # Plot formatting\n    plt.title('Track length Histogram')\n    axes.set_xlabel('Track Length')\n    axes.set_ylabel('Count')\n    return axes\n</code></pre>"},{"location":"api/#arcos4py.plotting.dataPlots.position_t_plot","title":"<code>position_t_plot(position_columns={'x'}, n=20, **kwargs)</code>","text":"<p>Plots X and Y over T to visualize tracklength.</p> <p>Parameters:</p> Name Type Description Default <code>position_columns</code> <code>set</code> <p>containing names of position columns in data.</p> <code>{'x'}</code> <code>n</code> <code>int</code> <p>number of samples to plot.</p> <code>20</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments. Includes deprecated parameters. - posCol (set): Deprecated. Use position_columns instead.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>fig</code> <code>Figure</code> <p>Matplotlib figure object of density plot.</p> <code>axes</code> <code>Axes</code> <p>Matplotlib axes of density plot.</p> Source code in <code>arcos4py/plotting/_plotting.py</code> <pre><code>def position_t_plot(self, position_columns: set[str] = {'x'}, n: int = 20, **kwargs) -&gt; Union[plt.Figure, Any]:\n    \"\"\"Plots X and Y over T to visualize tracklength.\n\n    Arguments:\n        position_columns (set): containing names of position columns in data.\n        n (int): number of samples to plot.\n        **kwargs (Any): Additional keyword arguments. Includes deprecated parameters.\n            - posCol (set): Deprecated. Use position_columns instead.\n\n    Returns:\n        fig (matplotlib.figure.Figure): Matplotlib figure object of density plot.\n        axes (matplotlib.axes.Axes): Matplotlib axes of density plot.\n    \"\"\"\n    map_deprecated_params = {\n        \"posCol\": \"position_columns\",\n    }\n\n    # check allowed kwargs\n    allowed_kwargs = map_deprecated_params.keys()\n    for key in kwargs:\n        if key not in allowed_kwargs:\n            raise ValueError(f\"Got an unexpected keyword argument '{key}'\")\n\n    updated_kwargs = handle_deprecated_params(map_deprecated_params, **kwargs)\n\n    # Assigning the parameters\n    position_columns = updated_kwargs.get(\"position_columns\", position_columns)\n\n    sample = pd.Series(self.data[self.obj_id].unique()).sample(n)\n    pd_from_r_df = self.data.loc[self.data[self.obj_id].isin(sample)]\n    fig, axes = plt.subplots(1, len(position_columns), figsize=(6, 3))\n    for _, df in pd_from_r_df.groupby(self.obj_id):\n        for index, value in enumerate(position_columns):\n            if len(position_columns) &gt; 1:\n                df.plot(x=self.frame_column, y=value, ax=axes[index], legend=None)\n            else:\n                df.plot(x=self.frame_column, y=value, ax=axes, legend=None)\n    if len(position_columns) &gt; 1:\n        for index, value in enumerate(position_columns):\n            axes[index].set_title(value)\n    else:\n        axes.set_title(value)\n    return fig, axes\n</code></pre>"},{"location":"api/#arcos4py.plotting.plotOriginalDetrended","title":"<code>plotOriginalDetrended(data, frame_column='frame', measurement_column='m', detrended_column='m_detrended', obj_id_column='obj_id', seed=42, **kwargs)</code>","text":"<p>Plot original and detrended data.</p> <p>Attributes:</p> Name Type Description <code>data</code> <code>DataFrame</code> <p>containing ARCOS data.</p> <code>frame_column</code> <code>str</code> <p>name of frame column in data.</p> <code>measurement_column</code> <code>str</code> <p>name of measurement column in data.</p> <code>detrended_column</code> <code>str</code> <p>name of detrended column in data.</p> <code>obj_id_column</code> <code>str</code> <p>name of track id column.</p> <code>seed</code> <code>int</code> <p>seed for random number generator.</p> <p>Methods:</p> Name Description <code>plot_detrended</code> <p>plot detrended data.</p> <code>plot_original</code> <p>plot original data.</p> <code>plot_original_and_detrended</code> <p>plot original and detrended data.</p> Source code in <code>arcos4py/plotting/_plotting.py</code> <pre><code>def __init__(\n    self,\n    data: pd.DataFrame,\n    frame_column: str = \"frame\",\n    measurement_column: str = \"m\",\n    detrended_column: str = \"m_detrended\",\n    obj_id_column: str = \"obj_id\",\n    seed: int = 42,\n    **kwargs,\n):\n    \"\"\"Constructs class with given parameters.\"\"\"\n    map_deprecated_params = {\n        \"id\": \"obj_id_column\",\n        \"frame\": \"frame_column\",\n        \"detrended\": \"detrended_column\",\n        \"measurement\": \"measurement_column\",\n    }\n\n    # check allowed kwargs\n    allowed_kwargs = map_deprecated_params.keys()\n    for key in kwargs:\n        if key not in allowed_kwargs:\n            raise ValueError(f\"Got an unexpected keyword argument '{key}'\")\n\n    updated_kwargs = handle_deprecated_params(map_deprecated_params, **kwargs)\n\n    # Assigning the parameters\n    obj_id_column = updated_kwargs.get(\"obj_id_column\", obj_id_column)\n    frame_column = updated_kwargs.get(\"frame_column\", frame_column)\n    measurement_column = updated_kwargs.get(\"measurement_column\", measurement_column)\n\n    self.data = data\n    self.frame_column = frame_column\n    self.measurement_column = measurement_column\n    self.detrended_column = detrended_column\n    self.obj_id_column = obj_id_column\n    self.seed = seed\n</code></pre>"},{"location":"api/#arcos4py.plotting.plotOriginalDetrended.plot_detrended","title":"<code>plot_detrended(n_samples=25, subplots=(5, 5), plotsize=(20, 10), add_binary_segments=False)</code>","text":"<p>Plots detrended data.</p> <p>Parameters:</p> Name Type Description Default <code>n_samples</code> <code>int</code> <p>number of samples to plot.</p> <code>25</code> <code>subplots</code> <code>tuple</code> <p>number of subplots in x and y direction.</p> <code>(5, 5)</code> <code>plotsize</code> <code>tuple</code> <p>size of the plot.</p> <code>(20, 10)</code> <code>add_binary_segments</code> <code>bool</code> <p>if True, binary segments are added to the plot.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>fig</code> <code>Figure</code> <p>Matplotlib figure object of plot.</p> <code>axes</code> <code>Axes</code> <p>Matplotlib axes of plot.</p> Source code in <code>arcos4py/plotting/_plotting.py</code> <pre><code>def plot_detrended(\n    self,\n    n_samples: int = 25,\n    subplots: tuple = (5, 5),\n    plotsize: tuple = (20, 10),\n    add_binary_segments: bool = False,\n) -&gt; tuple[plt.Figure, Any]:\n    \"\"\"Plots detrended data.\n\n    Arguments:\n        n_samples (int): number of samples to plot.\n        subplots (tuple): number of subplots in x and y direction.\n        plotsize (tuple): size of the plot.\n        add_binary_segments (bool): if True, binary segments are added to the plot.\n\n    Returns:\n        fig (matplotlib.figure.Figure): Matplotlib figure object of plot.\n        axes (matplotlib.axes.Axes): Matplotlib axes of plot.\n    \"\"\"\n    grouped = self._prepare_data(n_samples)\n    return self._plot_data(\n        grouped, subplots[0], subplots[1], plotsize, [self.detrended_column], [\"detrended\"], add_binary_segments\n    )\n</code></pre>"},{"location":"api/#arcos4py.plotting.plotOriginalDetrended.plot_original","title":"<code>plot_original(n_samples=25, subplots=(5, 5), plotsize=(20, 10), add_binary_segments=False)</code>","text":"<p>Plots original data.</p> <p>Parameters:</p> Name Type Description Default <code>n_samples</code> <code>int</code> <p>number of samples to plot.</p> <code>25</code> <code>subplots</code> <code>tuple</code> <p>number of subplots in x and y direction.</p> <code>(5, 5)</code> <code>plotsize</code> <code>tuple</code> <p>size of the plot.</p> <code>(20, 10)</code> <code>add_binary_segments</code> <code>bool</code> <p>if True, binary segments are added to the plot.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>fig</code> <code>Figure</code> <p>Matplotlib figure object of plot.</p> <code>axes</code> <code>Axes</code> <p>Matplotlib axes of plot.</p> Source code in <code>arcos4py/plotting/_plotting.py</code> <pre><code>def plot_original(\n    self,\n    n_samples: int = 25,\n    subplots: tuple = (5, 5),\n    plotsize: tuple = (20, 10),\n    add_binary_segments: bool = False,\n) -&gt; tuple[plt.Figure, Any]:\n    \"\"\"Plots original data.\n\n    Arguments:\n        n_samples (int): number of samples to plot.\n        subplots (tuple): number of subplots in x and y direction.\n        plotsize (tuple): size of the plot.\n        add_binary_segments (bool): if True, binary segments are added to the plot.\n\n    Returns:\n        fig (matplotlib.figure.Figure): Matplotlib figure object of plot.\n        axes (matplotlib.axes.Axes): Matplotlib axes of plot.\n    \"\"\"\n    grouped = self._prepare_data(n_samples)\n    return self._plot_data(\n        grouped,\n        subplots[0],\n        subplots[1],\n        plotsize,\n        [self.measurement_column],\n        [\"original\"],\n        add_binary_segments,\n    )\n</code></pre>"},{"location":"api/#arcos4py.plotting.plotOriginalDetrended.plot_original_and_detrended","title":"<code>plot_original_and_detrended(n_samples=25, subplots=(5, 5), plotsize=(20, 10), add_binary_segments=False)</code>","text":"<p>Plots original and detrended data.</p> <p>Parameters:</p> Name Type Description Default <code>n_samples</code> <code>int</code> <p>number of samples to plot.</p> <code>25</code> <code>subplots</code> <code>tuple</code> <p>number of subplots in x and y direction.</p> <code>(5, 5)</code> <code>plotsize</code> <code>tuple</code> <p>size of the plot.</p> <code>(20, 10)</code> <code>add_binary_segments</code> <code>bool</code> <p>if True, binary segments are added to the plot.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>fig</code> <code>Figure</code> <p>Matplotlib figure object of plot.</p> <code>axes</code> <code>Axes</code> <p>Matplotlib axes of plot.</p> Source code in <code>arcos4py/plotting/_plotting.py</code> <pre><code>def plot_original_and_detrended(\n    self,\n    n_samples: int = 25,\n    subplots: tuple = (5, 5),\n    plotsize: tuple = (20, 10),\n    add_binary_segments: bool = False,\n) -&gt; tuple[plt.Figure, Any]:\n    \"\"\"Plots original and detrended data.\n\n    Arguments:\n        n_samples (int): number of samples to plot.\n        subplots (tuple): number of subplots in x and y direction.\n        plotsize (tuple): size of the plot.\n        add_binary_segments (bool): if True, binary segments are added to the plot.\n\n    Returns:\n        fig (matplotlib.figure.Figure): Matplotlib figure object of plot.\n        axes (matplotlib.axes.Axes): Matplotlib axes of plot.\n    \"\"\"\n    grouped = self._prepare_data(n_samples)\n    return self._plot_data(\n        grouped,\n        subplots[0],\n        subplots[1],\n        plotsize,\n        [self.measurement_column, self.detrended_column],\n        [\"original\", \"detrended\"],\n        add_binary_segments,\n    )\n</code></pre>"},{"location":"api/#arcos4py.plotting.statsPlots","title":"<code>statsPlots(data)</code>","text":"<p>Plot data generated by the stats module.</p> <p>Attributes:</p> Name Type Description <code>data</code> <code>DataFrame</code> <p>containing ARCOS stats data.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>containing ARCOS stats data.</p> required Source code in <code>arcos4py/plotting/_plotting.py</code> <pre><code>def __init__(self, data: pd.DataFrame):\n    \"\"\"Plot detrended vs original data.\n\n    Arguments:\n        data (DataFrame): containing ARCOS stats data.\n    \"\"\"\n    self.data = data\n</code></pre>"},{"location":"api/#arcos4py.plotting.statsPlots.plot_events_duration","title":"<code>plot_events_duration(total_size, duration, point_size=40, *args, **kwargs)</code>","text":"<p>Scatterplot of collective event duration.</p> <p>Parameters:</p> Name Type Description Default <code>total_size</code> <code>str</code> <p>name of total size column.</p> required <code>duration</code> <code>str</code> <p>, name of column with collective event duration.</p> required <code>point_size</code> <code>int</code> <p>scatterplot point size.</p> <code>40</code> <code>*args</code> <code>Any</code> <p>Arguments passed on to seaborn scatterplot function.</p> <code>()</code> <code>**kwargs</code> <code>Any</code> <p>Keyword arguments passed on to seaborn scatterplot function.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Axes</code> <code>Axes</code> <p>Matplotlib Axes object of scatterplot</p> Source code in <code>arcos4py/plotting/_plotting.py</code> <pre><code>def plot_events_duration(self, total_size: str, duration: str, point_size: int = 40, *args, **kwargs) -&gt; plt.Axes:\n    \"\"\"Scatterplot of collective event duration.\n\n    Arguments:\n        total_size (str): name of total size column.\n        duration (str):, name of column with collective event duration.\n        point_size (int): scatterplot point size.\n        *args (Any): Arguments passed on to seaborn scatterplot function.\n        **kwargs (Any): Keyword arguments passed on to seaborn scatterplot function.\n\n    Returns:\n        Axes (matplotlib.axes.Axes): Matplotlib Axes object of scatterplot\n    \"\"\"\n    plot = sns.scatterplot(x=self.data[total_size], y=self.data[duration], s=point_size, *args, **kwargs)\n    return plot\n</code></pre>"},{"location":"api/#arcos4py.plotting.save_animation_frames","title":"<code>save_animation_frames(arcos_data, all_cells_data, output_dir, frame_col, collid_col, pos_cols, measurement_col=None, bin_col=None, plot_all_cells=True, color_all_cells_by_measurement=True, plot_bin_cells=True, plot_events=True, plot_convex_hulls=True, point_size=10.0, event_cmap=DEFAULT_EVENT_CMAP, event_alpha=0.9, hull_alpha=0.8, hull_linewidth_size_factor=1.0, bin_cell_color=DEFAULT_BIN_COLOR, bin_cell_alpha=0.7, bin_cell_marker_factor=0.8, all_cells_cmap=DEFAULT_ALL_CELLS_CMAP, all_cells_fixed_color=DEFAULT_ALL_CELLS_FIXED_COLOR, all_cells_alpha=0.5, all_cells_marker_size_factor=0.2, measurement_min_max=None, add_measurement_colorbar=True, filename_prefix='frame', dpi=150)</code>","text":"<p>Generates and saves individual frames of a cell activity visualization as PNG images.</p> <p>This function acts as a caller for the <code>yield_animation_frames</code> generator. It handles the iteration over frames, saving each frame to a file with appropriate naming and padding, and ensures figures are closed to free memory.</p>"},{"location":"api/#arcos4py.plotting.save_animation_frames--parameters","title":"Parameters","text":"<p>arcos_data : pd.DataFrame     DataFrame containing cell activity data, potentially including collective     event IDs (<code>collid_col</code>) and binarization status (<code>bin_col</code>). all_cells_data : pd.DataFrame     DataFrame containing all cells (or a representative background set)     for background plotting. Must include <code>frame_col</code>, <code>pos_cols</code>, and     <code>measurement_col</code> if <code>color_all_cells_by_measurement</code> is True. output_dir : str     Directory where the output frames will be saved. frame_col : str     Name of the column indicating the time frame. collid_col : str     Name of the column indicating the collective event ID.     Values &gt; 0 are treated as events. pos_cols : List[str]     List of column names for spatial coordinates (e.g., ['x', 'y'] or ['x', 'y', 'z']). measurement_col : Optional[str], optional     Name of the column containing the measurement value. REQUIRED if     <code>color_all_cells_by_measurement</code> is True. Used for coloring background cells.     Default None. bin_col : Optional[str], optional     Name of the column indicating binarized activity (e.g., values &gt; 0 mean     binarized). Used for <code>plot_bin_cells</code>. Default None. plot_all_cells : bool, optional     Whether to plot the background cells from <code>all_cells_data</code>. Default True. color_all_cells_by_measurement : bool, optional     If True and <code>plot_all_cells</code> is True, color background cells using     <code>measurement_col</code> and <code>all_cells_cmap</code>. Requires <code>measurement_col</code> to     be valid in <code>all_cells_data</code>. If False or requirements not met, uses     <code>all_cells_fixed_color</code>. Default True. plot_bin_cells : bool, optional     Whether to plot cells marked active by <code>bin_col</code> but not part of a     collective event (<code>collid_col</code> &lt;= 0). Requires <code>bin_col</code> to be set.     Default True. plot_events : bool, optional     Whether to plot cells identified as part of collective events     (<code>collid_col</code> &gt; 0). Default True. plot_convex_hulls : bool, optional     Whether to draw convex hulls around collective events (2D only).     Default True. point_size : float, optional     Base size for plotted points (event cells). Default 10.0. event_cmap : str, optional     Name of the Matplotlib colormap used to assign unique colors to     different collective event IDs. Default 'tab20'. event_alpha : float, optional     Alpha transparency for event cells. Default 0.9. hull_alpha : float, optional     Alpha transparency for convex hull lines. Default 0.8. hull_linewidth_size_factor : float, optional     Size factor for convex hull line width. Default 1.0. bin_cell_color : str, optional     Color for binarized (non-event) cells. Default 'black'. bin_cell_alpha : float, optional     Alpha transparency for binarized (non-event) cells. Default 0.7. bin_cell_marker_factor : float, optional     Size multiplier for binarized (non-event) cells relative to <code>point_size</code>.     Default 0.8. all_cells_cmap : str, optional     Name of the Matplotlib colormap used for background cells when     <code>color_all_cells_by_measurement</code> is True. Default 'viridis'. all_cells_fixed_color : str, optional     Color for background cells if <code>color_all_cells_by_measurement</code> is False     or requirements are not met. Default 'gray'. all_cells_alpha : float, optional     Alpha transparency for background cells. Default 0.5. all_cells_marker_size_factor : float, optional     Size multiplier for background cells relative to <code>point_size</code>. Default 0.2. measurement_min_max : Optional[Tuple[float, float]], optional     Manual min/max values for the measurement colormap normalization. If None,     the range is determined from <code>all_cells_data[measurement_col]</code>. Default None. add_measurement_colorbar : bool, optional     If True and coloring all cells by measurement, add a static colorbar     to the figure. Default True. filename_prefix : str, optional     Prefix for the output filenames. Default 'frame'. dpi : int, optional     DPI for the saved images. Default 150.</p> Source code in <code>arcos4py/plotting/_plotting.py</code> <pre><code>def save_animation_frames(\n    arcos_data: pd.DataFrame,\n    all_cells_data: pd.DataFrame,\n    output_dir: str,\n    frame_col: str,\n    collid_col: str,\n    pos_cols: List[str],\n    measurement_col: Optional[str] = None,\n    bin_col: Optional[str] = None,\n    plot_all_cells: bool = True,\n    color_all_cells_by_measurement: bool = True,\n    plot_bin_cells: bool = True,\n    plot_events: bool = True,\n    plot_convex_hulls: bool = True,\n    point_size: float = 10.0,\n    event_cmap: str = DEFAULT_EVENT_CMAP,\n    event_alpha: float = 0.9,\n    hull_alpha: float = 0.8,\n    hull_linewidth_size_factor: float = 1.0,\n    bin_cell_color: str = DEFAULT_BIN_COLOR,\n    bin_cell_alpha: float = 0.7,\n    bin_cell_marker_factor: float = 0.8,\n    all_cells_cmap: str = DEFAULT_ALL_CELLS_CMAP,\n    all_cells_fixed_color: str = DEFAULT_ALL_CELLS_FIXED_COLOR,\n    all_cells_alpha: float = 0.5,\n    all_cells_marker_size_factor: float = 0.2,\n    measurement_min_max: Optional[Tuple[float, float]] = None,\n    add_measurement_colorbar: bool = True,\n    filename_prefix: str = \"frame\",\n    dpi: int = 150,\n) -&gt; None:\n    \"\"\"Generates and saves individual frames of a cell activity visualization as PNG images.\n\n    This function acts as a caller for the `yield_animation_frames` generator.\n    It handles the iteration over frames, saving each frame to a file with\n    appropriate naming and padding, and ensures figures are closed to free memory.\n\n    Parameters\n    ----------\n    arcos_data : pd.DataFrame\n        DataFrame containing cell activity data, potentially including collective\n        event IDs (`collid_col`) and binarization status (`bin_col`).\n    all_cells_data : pd.DataFrame\n        DataFrame containing all cells (or a representative background set)\n        for background plotting. Must include `frame_col`, `pos_cols`, and\n        `measurement_col` if `color_all_cells_by_measurement` is True.\n    output_dir : str\n        Directory where the output frames will be saved.\n    frame_col : str\n        Name of the column indicating the time frame.\n    collid_col : str\n        Name of the column indicating the collective event ID.\n        Values &gt; 0 are treated as events.\n    pos_cols : List[str]\n        List of column names for spatial coordinates (e.g., ['x', 'y'] or ['x', 'y', 'z']).\n    measurement_col : Optional[str], optional\n        Name of the column containing the measurement value. REQUIRED if\n        `color_all_cells_by_measurement` is True. Used for coloring background cells.\n        Default None.\n    bin_col : Optional[str], optional\n        Name of the column indicating binarized activity (e.g., values &gt; 0 mean\n        binarized). Used for `plot_bin_cells`. Default None.\n    plot_all_cells : bool, optional\n        Whether to plot the background cells from `all_cells_data`. Default True.\n    color_all_cells_by_measurement : bool, optional\n        If True and `plot_all_cells` is True, color background cells using\n        `measurement_col` and `all_cells_cmap`. Requires `measurement_col` to\n        be valid in `all_cells_data`. If False or requirements not met, uses\n        `all_cells_fixed_color`. Default True.\n    plot_bin_cells : bool, optional\n        Whether to plot cells marked active by `bin_col` but not part of a\n        collective event (`collid_col` &lt;= 0). Requires `bin_col` to be set.\n        Default True.\n    plot_events : bool, optional\n        Whether to plot cells identified as part of collective events\n        (`collid_col` &gt; 0). Default True.\n    plot_convex_hulls : bool, optional\n        Whether to draw convex hulls around collective events (2D only).\n        Default True.\n    point_size : float, optional\n        Base size for plotted points (event cells). Default 10.0.\n    event_cmap : str, optional\n        Name of the Matplotlib colormap used to assign unique colors to\n        different collective event IDs. Default 'tab20'.\n    event_alpha : float, optional\n        Alpha transparency for event cells. Default 0.9.\n    hull_alpha : float, optional\n        Alpha transparency for convex hull lines. Default 0.8.\n    hull_linewidth_size_factor : float, optional\n        Size factor for convex hull line width. Default 1.0.\n    bin_cell_color : str, optional\n        Color for binarized (non-event) cells. Default 'black'.\n    bin_cell_alpha : float, optional\n        Alpha transparency for binarized (non-event) cells. Default 0.7.\n    bin_cell_marker_factor : float, optional\n        Size multiplier for binarized (non-event) cells relative to `point_size`.\n        Default 0.8.\n    all_cells_cmap : str, optional\n        Name of the Matplotlib colormap used for background cells when\n        `color_all_cells_by_measurement` is True. Default 'viridis'.\n    all_cells_fixed_color : str, optional\n        Color for background cells if `color_all_cells_by_measurement` is False\n        or requirements are not met. Default 'gray'.\n    all_cells_alpha : float, optional\n        Alpha transparency for background cells. Default 0.5.\n    all_cells_marker_size_factor : float, optional\n        Size multiplier for background cells relative to `point_size`. Default 0.2.\n    measurement_min_max : Optional[Tuple[float, float]], optional\n        Manual min/max values for the measurement colormap normalization. If None,\n        the range is determined from `all_cells_data[measurement_col]`. Default None.\n    add_measurement_colorbar : bool, optional\n        If True and coloring all cells by measurement, add a static colorbar\n        to the figure. Default True.\n    filename_prefix : str, optional\n        Prefix for the output filenames. Default 'frame'.\n    dpi : int, optional\n        DPI for the saved images. Default 150.\n    \"\"\"\n    # --- Setup Output Directory ---\n    try:\n        os.makedirs(output_dir, exist_ok=True)\n        print(f\"Saving animation frames to directory: {output_dir}\")\n    except OSError as e:\n        print(f\"Error creating output directory '{output_dir}': {e}\")\n        return  # Cannot proceed without output directory\n\n    # --- Determine Frame Range and Padding (needed for filenames) ---\n    # This duplicates a small part of the generator's logic, but is necessary\n    # to format filenames correctly *before* the loop starts.\n    min_frame_val = float(\"inf\")\n    max_frame_val = float(\"-inf\")\n    if not arcos_data.empty and frame_col in arcos_data:\n        min_frame_val = 0\n        max_frame_val = max(max_frame_val, arcos_data[frame_col].max())\n    if not all_cells_data.empty and frame_col in all_cells_data:\n        min_frame_val = 0\n        max_frame_val = max(max_frame_val, all_cells_data[frame_col].max())\n\n    if min_frame_val == float(\"inf\") or max_frame_val == float(\"-inf\"):\n        # Generator will also warn, but we add a message here too.\n        print(\"Could not determine frame range from input data. No frames will be saved.\")\n        return\n\n    num_total_frames = int(max_frame_val) - int(min_frame_val) + 1\n    padding_digits = (\n        math.ceil(math.log10(max(1, int(max_frame_val)) + 1)) if max_frame_val &gt;= 0 else 1\n    )  # Calculate padding based on max frame number\n\n    # --- Instantiate the Generator ---\n    frame_generator = _yield_animation_frames(\n        arcos_data=arcos_data,\n        all_cells_data=all_cells_data,\n        frame_col=frame_col,\n        collid_col=collid_col,\n        pos_cols=pos_cols,\n        measurement_col=measurement_col,\n        bin_col=bin_col,\n        plot_all_cells=plot_all_cells,\n        color_all_cells_by_measurement=color_all_cells_by_measurement,\n        plot_bin_cells=plot_bin_cells,\n        plot_events=plot_events,\n        plot_convex_hulls=plot_convex_hulls,\n        point_size=point_size,\n        event_cmap=event_cmap,\n        event_alpha=event_alpha,\n        hull_alpha=hull_alpha,\n        hull_linewidth_size_factor=hull_linewidth_size_factor,\n        bin_cell_color=bin_cell_color,\n        bin_cell_alpha=bin_cell_alpha,\n        bin_cell_marker_factor=bin_cell_marker_factor,\n        all_cells_cmap=all_cells_cmap,\n        all_cells_fixed_color=all_cells_fixed_color,\n        all_cells_alpha=all_cells_alpha,\n        all_cells_marker_size_factor=all_cells_marker_size_factor,\n        measurement_min_max=measurement_min_max,\n        add_measurement_colorbar=add_measurement_colorbar,\n    )\n\n    # --- Iterate, Save, and Close ---\n    saved_frame_count = 0\n    print(f\"Starting frame generation and saving (estimated {num_total_frames} frames)...\")\n\n    for fig in tqdm(frame_generator, desc=\"Saving frames\", total=num_total_frames, unit=\"frame\"):\n        # Get frame number from the figure title (set by the generator)\n        try:\n            title = fig.axes[0].get_title()\n            # Handle potential variations in title format slightly more robustly\n            frame_num_str = title.split(':')[-1].strip()\n            frame_num = int(frame_num_str)\n        except (IndexError, ValueError, AttributeError) as e:\n            warnings.warn(\n                f\"Could not reliably determine frame number from figure title ('{title}'). Using counter. Error: {e}\"\n            )\n            # Fallback to a simple counter if title parsing fails\n            frame_num = saved_frame_count + int(min_frame_val)  # Estimate frame num\n\n        # Construct filename with padding\n        frame_filename = f\"{filename_prefix}_{frame_num:0{padding_digits}d}.png\"\n        output_path = os.path.join(output_dir, frame_filename)\n\n        # Save the figure\n        try:\n            fig.savefig(output_path, dpi=dpi, bbox_inches='tight')\n            saved_frame_count += 1\n        except Exception as e:\n            print(f\"\\nError saving frame {output_path}: {e}\")\n        finally:\n            # CRITICAL: Close the figure to free memory, regardless of save success\n            plt.close(fig)\n\n    print(f\"\\nFinished saving {saved_frame_count} frames to {output_dir}.\")\n    if saved_frame_count == 0:\n        print(\"Note: No frames were generated or saved. Check input data and parameters.\")\n</code></pre>"},{"location":"api/#arcos4py.tools","title":"<code>tools</code>","text":"<p>Tools for detecting collective events.</p>"},{"location":"api/#arcos4py.tools.DataFrameTracker","title":"<code>DataFrameTracker(linker, position_columns=['x'], frame_column='frame', obj_id_column=None, binarized_measurement_column=None, clid_column='clTrackID', **kwargs)</code>","text":"<p>               Bases: <code>BaseTracker</code></p> <p>Tracker class for data frames that works in conjunction with the Linker class.</p> <p>Methods:</p> Name Description <code>track_iteration</code> <p>pd.DataFrame): Tracks events in a single frame.</p> <code>track</code> <p>pd.DataFrame) -&gt; Generator: Main method for tracking events through the dataframe. Yields the tracked data frame for each iteration.</p> <p>Parameters:</p> Name Type Description Default <code>linker</code> <code>Linker</code> <p>The Linker object used for linking events.</p> required <code>position_columns</code> <code>list[str]</code> <p>List of strings representing the coordinate columns.</p> <code>['x']</code> <code>frame_column</code> <code>str</code> <p>String representing the frame/timepoint column in the dataframe.</p> <code>'frame'</code> <code>obj_id_column</code> <code>str | None</code> <p>String representing the ID column, or None if not present. Defaults to None.</p> <code>None</code> <code>binarized_measurement_column</code> <code>str | None</code> <p>String representing the binary measurement column, or None if not present. Defaults to None.</p> <code>None</code> <code>clid_column</code> <code>str</code> <p>String representing the collision track ID column. Defaults to 'clTrackID'.</p> <code>'clTrackID'</code> <code>kwargs</code> <code>Any</code> <p>Additional keyword arguments. Includes deprecated parameters for backwards compatibility. - coordinates_column: Deprecated parameter for position_columns. Use position_columns instead. - collid_column: Deprecated parameter, use clid_column instead. - id_column: Deprecated parameter, use obj_id_column instead. - bin_meas_column: Deprecated parameter, use binarized_measurement_column instead.</p> <code>{}</code> Source code in <code>arcos4py/tools/_detect_events.py</code> <pre><code>def __init__(\n    self,\n    linker: Linker,\n    position_columns: list[str] = ['x'],\n    frame_column: str = 'frame',\n    obj_id_column: str | None = None,\n    binarized_measurement_column: str | None = None,\n    clid_column: str = 'clTrackID',\n    **kwargs,\n):\n    \"\"\"Initializes the DataFrameTracker object.\n\n    Arguments:\n        linker (Linker): The Linker object used for linking events.\n        position_columns (list[str]): List of strings representing the coordinate columns.\n        frame_column (str): String representing the frame/timepoint column in the dataframe.\n        obj_id_column (str | None): String representing the ID column, or None if not present. Defaults to None.\n        binarized_measurement_column (str | None): String representing the binary measurement column, or None if not present.\n            Defaults to None.\n        clid_column (str): String representing the collision track ID column. Defaults to 'clTrackID'.\n        kwargs (Any): Additional keyword arguments. Includes deprecated parameters for backwards compatibility.\n            - coordinates_column: Deprecated parameter for position_columns. Use position_columns instead.\n            - collid_column: Deprecated parameter, use clid_column instead.\n            - id_column: Deprecated parameter, use obj_id_column instead.\n            - bin_meas_column: Deprecated parameter, use binarized_measurement_column instead.\n    \"\"\"\n    map_deprecated_params = {\n        'coordinates_column': 'position_columns',\n        'collid_column': 'clid_column',\n        'id_column': 'obj_id_column',\n        'bin_meas_column': 'binarized_measurement_column',\n    }\n\n    # check for allowed kwargs\n    for key in kwargs:\n        if key not in map_deprecated_params.keys():\n            raise ValueError(f'Invalid keyword argument {key}')\n\n    corrected_kwargs = handle_deprecated_params(map_deprecated_params, **kwargs)\n\n    # Assign parameters\n    position_columns = corrected_kwargs.get('position_columns', position_columns)\n    obj_id_column = corrected_kwargs.get('obj_id_column', obj_id_column)\n    binarized_measurement_column = corrected_kwargs.get(\n        'binarized_measurement_column', binarized_measurement_column\n    )\n    clid_column = corrected_kwargs.get('clid_column', clid_column)\n\n    super().__init__(linker)\n    self._coordinates_column = position_columns\n    self._frame_column = frame_column\n    self._id_column = obj_id_column\n    self._binarized_measurement_column = binarized_measurement_column\n    self._collid_column = clid_column\n    self._validate_input(position_columns, frame_column, obj_id_column, binarized_measurement_column, clid_column)\n</code></pre>"},{"location":"api/#arcos4py.tools.DataFrameTracker.track","title":"<code>track(x)</code>","text":"<p>Main method for tracking events through the dataframe. Yields the tracked dataframe for each iteration.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>DataFrame</code> <p>Dataframe to track.</p> required <p>Yields:</p> Name Type Description <code>Generator</code> <code>Generator</code> <p>Tracked dataframe.</p> Source code in <code>arcos4py/tools/_detect_events.py</code> <pre><code>def track(self, x: pd.DataFrame) -&gt; Generator:\n    \"\"\"Main method for tracking events through the dataframe. Yields the tracked dataframe for each iteration.\n\n    Arguments:\n        x (pd.DataFrame): Dataframe to track.\n\n    Yields:\n        Generator: Tracked dataframe.\n    \"\"\"\n    if x.empty:\n        raise ValueError('Input is empty')\n    x_sorted = self._sort_input(x, frame_column=self._frame_column, object_id_column=self._id_column)\n\n    for t in range(x_sorted[self._frame_column].min(), x_sorted[self._frame_column].max() + 1):\n        x_frame = x_sorted.query(f'{self._frame_column} == {t}')\n        x_tracked = self.track_iteration(x_frame)\n        yield x_tracked\n</code></pre>"},{"location":"api/#arcos4py.tools.DataFrameTracker.track_iteration","title":"<code>track_iteration(x)</code>","text":"<p>Tracks events in a single frame. Returns dataframe with event ids.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>DataFrame</code> <p>Dataframe to track.</p> required <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Dataframe with event ids.</p> Source code in <code>arcos4py/tools/_detect_events.py</code> <pre><code>def track_iteration(self, x: pd.DataFrame) -&gt; pd.DataFrame:\n    \"\"\"Tracks events in a single frame. Returns dataframe with event ids.\n\n    Arguments:\n        x (pd.DataFrame): Dataframe to track.\n\n    Returns:\n        pd.DataFrame: Dataframe with event ids.\n    \"\"\"\n    x_filtered = self._filter_active(x, self._binarized_measurement_column)\n\n    coordinates_data = self._select_necessary_columns(\n        x_filtered,\n        self._coordinates_column,\n    )\n    self.linker.link(coordinates_data)\n\n    if self._collid_column in x.columns:\n        df_out = x_filtered.drop(columns=[self._collid_column]).copy()\n    else:\n        df_out = x_filtered.copy()\n    event_ids = self.linker.event_ids\n\n    if not event_ids.size:\n        df_out[self._collid_column] = 0\n        return df_out\n\n    df_out[self._collid_column] = self.linker.event_ids\n    if any([self.linker._allow_merges, self.linker._allow_splits]):\n        df_out = self.linker.lineage_tracker._add_parents_and_lineage_to_df(\n            df_out,\n            self._collid_column,\n        )\n    return df_out\n</code></pre>"},{"location":"api/#arcos4py.tools.ImageTracker","title":"<code>ImageTracker(linker, downsample=1)</code>","text":"<p>               Bases: <code>BaseTracker</code></p> <p>Tracker class for image data that works in conjunction with the Linker class.</p> <p>Methods:</p> Name Description <code>track_iteration</code> <p>np.ndarray): Tracks events in a single frame. Returns the tracked labels.</p> <code>track</code> <p>np.ndarray, dims: str = \"TXY\") -&gt; Generator: Main method for tracking events through the image series. Yields the tracked image for each iteration.</p> <p>Parameters:</p> Name Type Description Default <code>linker</code> <code>Linker</code> <p>The Linker object used for linking events.</p> required <code>downsample</code> <code>int</code> <p>Downsampling factor for the images. Defaults to 1, meaning no downsampling.</p> <code>1</code> Source code in <code>arcos4py/tools/_detect_events.py</code> <pre><code>def __init__(self, linker: Linker, downsample: int = 1):\n    \"\"\"Initializes the ImageTracker object.\n\n    Arguments:\n        linker (Linker): The Linker object used for linking events.\n        downsample (int): Downsampling factor for the images. Defaults to 1, meaning no downsampling.\n    \"\"\"\n    super().__init__(linker)\n    self._downsample = downsample\n</code></pre>"},{"location":"api/#arcos4py.tools.ImageTracker.track","title":"<code>track(x, dims='TXY')</code>","text":"<p>Method for tracking events through the image series. Yields the tracked image for each iteration.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>Image to track.</p> required <code>dims</code> <code>str</code> <p>String of dimensions in order. Default is \"TXY\". Possible values are \"T\", \"X\", \"Y\", and \"Z\".</p> <code>'TXY'</code> <p>Returns:</p> Name Type Description <code>Generator</code> <code>Generator</code> <p>Generator that yields the tracked image for each iteration.</p> Source code in <code>arcos4py/tools/_detect_events.py</code> <pre><code>def track(self, x: np.ndarray, dims: str = \"TXY\") -&gt; Generator:\n    \"\"\"Method for tracking events through the image series. Yields the tracked image for each iteration.\n\n    Arguments:\n        x (np.ndarray): Image to track.\n        dims (str): String of dimensions in order. Default is \"TXY\". Possible values are \"T\", \"X\", \"Y\", and \"Z\".\n\n    Returns:\n        Generator: Generator that yields the tracked image for each iteration.\n    \"\"\"\n    available_dims = [\"T\", \"X\", \"Y\", \"Z\"]\n    dims_list = list(dims.upper())\n\n    # check input\n    for i in dims_list:\n        if i not in dims_list:\n            raise ValueError(f\"Invalid dimension {i}. Must be 'T', 'X', 'Y', or 'Z'.\")\n\n    if len(dims_list) &gt; len(set(dims_list)):\n        raise ValueError(\"Duplicate dimensions in dims.\")\n\n    if len(dims_list) != x.ndim:\n        raise ValueError(\n            f\"Length of dims must be equal to number of dimensions in image. Image has {x.ndim} dimensions.\"\n        )\n\n    dims_dict = {i: dims_list.index(i) for i in available_dims if i in dims_list}\n\n    # reorder image so T is first dimension\n    image_reshaped = np.moveaxis(x, dims_dict[\"T\"], 0)\n\n    for x_frame in image_reshaped:\n        x_tracked = self.track_iteration(x_frame)\n        yield x_tracked\n</code></pre>"},{"location":"api/#arcos4py.tools.ImageTracker.track_iteration","title":"<code>track_iteration(x)</code>","text":"<p>Tracks events in a single frame. Returns the tracked labels.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>ndarray</code> <p>Image to track.</p> required <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray: Tracked labels.</p> Source code in <code>arcos4py/tools/_detect_events.py</code> <pre><code>def track_iteration(self, x: np.ndarray) -&gt; np.ndarray:\n    \"\"\"Tracks events in a single frame. Returns the tracked labels.\n\n    Arguments:\n        x (np.ndarray): Image to track.\n\n    Returns:\n        np.ndarray: Tracked labels.\n    \"\"\"\n    x = downscale_image(x, self._downsample)\n    coordinates_data, meas_data = self._image_to_coordinates(x)\n    coordinates_data_filtered = self._filter_active(coordinates_data, meas_data)\n\n    self.linker.link(coordinates_data_filtered)\n\n    tracked_events = self.linker.event_ids\n    out_img = self._coordinates_to_image(x, coordinates_data_filtered, tracked_events)\n\n    if self._downsample &gt; 1:\n        out_img = upscale_image(out_img, self._downsample)\n\n    return out_img\n</code></pre>"},{"location":"api/#arcos4py.tools.Linker","title":"<code>Linker(eps=1, eps_prev=None, min_clustersize=1, min_samples=None, clustering_method='dbscan', linking_method='nearest', predictor=True, n_prev=1, cost_threshold=0, reg=1, reg_m=10, n_jobs=1, allow_merges=False, allow_splits=False, stability_threshold=10, remove_small_clusters=False, min_size_for_split=1, **kwargs)</code>","text":"<p>Linker class to link clusters across frames and detect collective events.</p> <p>Attributes:</p> Name Type Description <code>event_ids</code> <code>ndarray</code> <p>The event IDs.</p> <code>frame_counter</code> <code>int</code> <p>The current frame counter.</p> <code>LineageTracker</code> <code>LineageTracker</code> <p>The LineageTracker object.</p> <p>Methods:</p> Name Description <code>link</code> <p>Links clusters across frames and detects collective events.</p> <code>get_event_ids</code> <p>Returns the event IDs.</p> <p>Parameters:</p> Name Type Description Default <code>eps</code> <code>float</code> <p>The maximum distance between two samples for one to be considered as in the neighbourhood of the other.</p> <code>1</code> <code>eps_prev</code> <code>float | None</code> <p>Frame to frame distance, value is used to connect collective events across multiple frames. If \"None\", same value as eps is used.</p> <code>None</code> <code>min_clustersize</code> <code>int</code> <p>The minimum size for a cluster to be identified as a collective event.</p> <code>1</code> <code>min_samples</code> <code>int | None</code> <p>The number of samples (or total weight) in a neighbourhood for a point to be considered as a core point. This includes the point itself. Only used if clusteringMethod is 'hdbscan'. If None, minSamples =  minClsz.</p> <code>None</code> <code>clustering_method</code> <code>str | Callable</code> <p>The clustering method to be used. One of ['dbscan', 'hdbscan'] or a callable that takes a 2d array of coordinates and returns a list of cluster labels. Arguments <code>eps</code>, <code>minClSz</code> and <code>minSamples</code> are ignored if a callable is passed.</p> <code>'dbscan'</code> <code>linking_method</code> <code>str</code> <p>The linking method to be used.</p> <code>'nearest'</code> <code>predictor</code> <code>bool | Callable</code> <p>The predictor method to be used.</p> <code>True</code> <code>n_prev</code> <code>int</code> <p>Number of previous frames the tracking algorithm looks back to connect collective events.</p> <code>1</code> <code>n_jobs</code> <code>int</code> <p>Number of jobs to run in parallel (only for clustering algorithm).</p> <code>1</code> <code>cost_threshold</code> <code>int</code> <p>Threshold for filtering low-probability matches (only for transportation linking).</p> <code>0</code> <code>reg</code> <code>float</code> <p>Entropy regularization parameter for unbalanced OT algorithm (only for transportation linking).</p> <code>1</code> <code>reg_m</code> <code>float</code> <p>Marginal relaxation parameter for unbalanced OT (only for transportation linking).</p> <code>10</code> <code>stability_threshold</code> <code>int</code> <p>Number of consecutive frames a merge/split must persist to be considered stable.</p> <code>10</code> <code>allow_merges</code> <code>bool</code> <p>Whether to allow merges.</p> <code>False</code> <code>allow_splits</code> <code>bool</code> <p>Whether to allow splits.</p> <code>False</code> <code>remove_small_clusters</code> <code>bool</code> <p>Whether to remove clusters smaller than min_clustersize.</p> <code>False</code> <code>min_size_for_split</code> <code>int</code> <p>The minimum size for a cluster to be considered for splitting. Multiple of min_clustersize.</p> <code>1</code> <code>kwargs</code> <code>Any</code> <p>Additional keyword arguments. Includes deprecated parameters for backwards compatibility. - epsPrev: Deprecated parameter for eps_prev. Use eps_prev instead. - minClSz: Deprecated parameter for min_clustersize. Use min_clustersize instead. - minSamples: Deprecated parameter for min_samples. Use min_samples instead. - clusteringMethod: Deprecated parameter for clustering_method. Use clustering_method instead. - nPrev: Deprecated parameter for n_prev. Use n_prev instead. - nJobs: Deprecated parameter for n_jobs. Use n_jobs instead.</p> <code>{}</code> Source code in <code>arcos4py/tools/_detect_events.py</code> <pre><code>def __init__(\n    self,\n    eps: float = 1,\n    eps_prev: float | None = None,\n    min_clustersize: int = 1,\n    min_samples: int | None = None,\n    clustering_method: str | Callable = \"dbscan\",\n    linking_method: str = \"nearest\",\n    predictor: bool | Callable = True,\n    n_prev: int = 1,\n    cost_threshold: float = 0,\n    reg: float = 1,\n    reg_m: float = 10,\n    n_jobs: int = 1,\n    allow_merges: bool = False,\n    allow_splits: bool = False,\n    stability_threshold: int = 10,\n    remove_small_clusters: bool = False,\n    min_size_for_split: int = 1,\n    **kwargs,\n):\n    \"\"\"Initializes the Linker object.\n\n    Arguments:\n        eps (float): The maximum distance between two samples for one to be considered as in\n            the neighbourhood of the other.\n        eps_prev (float | None): Frame to frame distance, value is used to connect\n            collective events across multiple frames. If \"None\", same value as eps is used.\n        min_clustersize (int): The minimum size for a cluster to be identified as a collective event.\n        min_samples (int | None): The number of samples (or total weight) in a neighbourhood for a\n            point to be considered as a core point. This includes the point itself.\n            Only used if clusteringMethod is 'hdbscan'. If None, minSamples =  minClsz.\n        clustering_method (str | Callable): The clustering method to be used. One of ['dbscan', 'hdbscan']\n            or a callable that takes a 2d array of coordinates and returns a list of cluster labels.\n            Arguments `eps`, `minClSz` and `minSamples` are ignored if a callable is passed.\n        linking_method (str): The linking method to be used.\n        predictor (bool | Callable): The predictor method to be used.\n        n_prev (int): Number of previous frames the tracking\n            algorithm looks back to connect collective events.\n        n_jobs (int): Number of jobs to run in parallel (only for clustering algorithm).\n        cost_threshold (int): Threshold for filtering low-probability matches (only for transportation linking).\n        reg (float): Entropy regularization parameter for unbalanced OT algorithm (only for transportation linking).\n        reg_m (float): Marginal relaxation parameter for unbalanced OT (only for transportation linking).\n        stability_threshold (int): Number of consecutive frames a merge/split must persist to be considered stable.\n        allow_merges (bool): Whether to allow merges.\n        allow_splits (bool): Whether to allow splits.\n        remove_small_clusters (bool): Whether to remove clusters smaller than min_clustersize.\n        min_size_for_split (int): The minimum size for a cluster to be considered for splitting. Multiple of min_clustersize.\n        kwargs (Any): Additional keyword arguments. Includes deprecated parameters for backwards compatibility.\n            - epsPrev: Deprecated parameter for eps_prev. Use eps_prev instead.\n            - minClSz: Deprecated parameter for min_clustersize. Use min_clustersize instead.\n            - minSamples: Deprecated parameter for min_samples. Use min_samples instead.\n            - clusteringMethod: Deprecated parameter for clustering_method. Use clustering_method instead.\n            - nPrev: Deprecated parameter for n_prev. Use n_prev instead.\n            - nJobs: Deprecated parameter for n_jobs. Use n_jobs instead.\n    \"\"\"\n    map_params = {\n        'epsPrev': 'eps_prev',\n        'minClSz': 'min_clustersize',\n        'minSamples': 'min_samples',\n        'clusteringMethod': 'clustering_method',\n        'linkingMethod': 'linking_method',\n        'nPrev': 'n_prev',\n        'nJobs': 'n_jobs',\n    }\n\n    # check for allowed kwargs\n    for key in kwargs:\n        if key not in map_params.keys():\n            raise ValueError(f'Invalid keyword argument {key}')\n\n    # Handle deprecated parameters\n    kwargs = handle_deprecated_params(map_params, **kwargs)\n\n    # Assign parameters\n    eps_prev = kwargs.get('eps_prev', eps_prev)\n    min_clustersize = kwargs.get('min_clustersize', min_clustersize)\n    min_samples = kwargs.get('min_samples', min_samples)\n    clustering_method = kwargs.get('clustering_method', clustering_method)\n    n_prev = kwargs.get('n_prev', n_prev)\n    n_jobs = kwargs.get('n_jobs', n_jobs)\n\n    self._predictor: Predictor | None  # for mypy\n    self._memory = Memory(n_timepoints=n_prev)\n\n    if callable(predictor):\n        self._predictor = Predictor(predictor)\n    elif predictor:\n        self._predictor = Predictor.with_default_predictor()\n    else:\n        self._predictor = None\n\n    self._nn_tree: KDTree | None = None\n    if eps_prev is None:\n        self._eps_prev = eps\n    else:\n        self._eps_prev = eps_prev\n\n    self._reg = reg\n    self._reg_m = reg_m\n    self._cost_threshold = cost_threshold\n\n    self._n_jobs = n_jobs\n    self._validate_input(eps, eps_prev, min_clustersize, min_samples, clustering_method, n_prev, n_jobs)\n\n    self.event_ids = np.empty((0, 0), dtype=np.int64)\n\n    if hasattr(clustering_method, '__call__'):  # check if it's callable\n        self.clustering_function = clustering_method\n    else:\n        if clustering_method == \"dbscan\":\n            self.clustering_function = functools.partial(_dbscan, eps=eps, minClSz=min_clustersize)\n        elif clustering_method == \"hdbscan\":\n            self.clustering_function = functools.partial(\n                _hdbscan, eps=eps, minClSz=min_clustersize, min_samples=min_samples, cluster_selection_method='eom'\n            )\n        else:\n            raise ValueError(\n                f'Clustering method must be either in {AVAILABLE_CLUSTERING_METHODS} or a callable with data as the only argument an argument'  # noqa E501\n            )\n\n    if hasattr(linking_method, '__call__'):  # check if it's callable\n        self.linking_function = linking_method\n    else:\n        if linking_method == \"nearest\":\n            self.linking_function = 'brute_force_linking'\n        elif linking_method == \"transportation\":\n            self.linking_function = 'transportation_linking'\n        else:\n            raise ValueError(\n                f'Linking method must be either in {AVAILABLE_LINKING_METHODS} or a callable'  # noqa E501\n            )\n\n    self._stability_threshold = stability_threshold\n    self._allow_merges = allow_merges\n    self._allow_splits = allow_splits\n    self._merge_candidate_history: Dict[int, List[Tuple[List[int], int]]] = {}\n    self._split_candidate_history: Dict[int, List[Tuple[int, List[int]]]] = {}\n    self.lineage_tracker = LineageTracker()\n    self.frame_counter = -1  # Start at -1 to get the first frame to be 0\n    self._remove_small_clusters = remove_small_clusters\n    self._min_clustersize = min_clustersize\n    self._min_size_for_split = min_size_for_split\n</code></pre>"},{"location":"api/#arcos4py.tools.Linker.link","title":"<code>link(input_coordinates)</code>","text":"<p>Links clusters across frames and detects collective events.</p> <p>Parameters:</p> Name Type Description Default <code>input_coordinates</code> <code>ndarray</code> <p>The input coordinates.</p> required Source code in <code>arcos4py/tools/_detect_events.py</code> <pre><code>def link(self, input_coordinates: np.ndarray) -&gt; None:\n    \"\"\"Links clusters across frames and detects collective events.\n\n    Arguments:\n        input_coordinates (np.ndarray): The input coordinates.\n    \"\"\"\n    self.frame_counter += 1\n    original_cluster_ids, coordinates, nanrows = self._clustering(input_coordinates)\n\n    if not len(self._memory.prev_cluster_ids):\n        linked_cluster_ids = self._update_id_empty(original_cluster_ids)\n    elif original_cluster_ids.size == 0 or self._memory.all_cluster_ids.size == 0:\n        linked_cluster_ids = self._update_id_empty(original_cluster_ids)\n    else:\n        linked_cluster_ids = self._update_id(original_cluster_ids, coordinates)\n\n    if self._remove_small_clusters:\n        final_cluster_ids = self._apply_remove_small_clusters(linked_cluster_ids, original_cluster_ids)\n\n    # Apply stable merges and splits\n    final_cluster_ids = self._apply_stable_merges_splits(linked_cluster_ids, original_cluster_ids)\n\n    # Update lineage graph\n    self.lineage_tracker._add_frame(linked_cluster_ids, final_cluster_ids, self.frame_counter)\n\n    # Update memory and fit predictor\n    self._memory.add_frame(new_coordinates=coordinates, new_cluster_ids=final_cluster_ids)\n    if self._predictor is not None and len(self._memory.coordinates) &gt; 1:\n        self._predictor.fit(coordinates=self._memory.coordinates, cluster_ids=self._memory.prev_cluster_ids)\n    self._memory.remove_timepoint()\n\n    event_ids = np.full_like(nanrows, -1, dtype=np.int64)\n    event_ids[~nanrows] = final_cluster_ids\n\n    self.event_ids = event_ids\n</code></pre>"},{"location":"api/#arcos4py.tools.binData","title":"<code>binData(smooth_k=3, bias_k=51, peak_threshold=0.2, binarization_threshold=0.1, polynomial_degree=1, bias_method='runmed', n_jobs=1, **kwargs)</code>","text":"<p>               Bases: <code>detrender</code></p> <p>Smooth, de-trend, and binarise the input data.</p> <p>First a short-term median filter with size smoothK is applied to remove fast noise from the time series. If the de-trending method is set to \"none\", smoothing is applied on globally rescaled time series. The subsequent de-trending can be performed with a long-term median filter with the size biasK {biasMet = \"runmed\"} or by fitting a polynomial of degree polyDeg {biasMet = \"lm\"}.</p> <p>After de-trending, if the global difference between min/max is greater than the threshold the signal is rescaled to the (0,1) range. The final signal is binarised using the binThr threshold.</p> <p>Attributes:</p> Name Type Description <code>smoothK</code> <code>int</code> <p>Size of the short-term median smoothing filter.</p> <code>biasK</code> <code>int</code> <p>Size of the long-term de-trending median filter.</p> <code>peakThr</code> <code>float</code> <p>Threshold for rescaling of the de-trended signal.</p> <code>binThr</code> <code>float</code> <p>Threshold for binarizing the de-trended signal.</p> <code>polyDeg</code> <code>int</code> <p>Sets the degree of the polynomial for lm fitting.</p> <code>biasMet</code> <code>str</code> <p>De-trending method, one of ['runmed', 'lm', 'none'].</p> <p>Parameters:</p> Name Type Description Default <code>smooth_k</code> <code>int</code> <p>Size of the short-term median smoothing filter.</p> <code>3</code> <code>bias_k</code> <code>int</code> <p>Size of the long-term de-trending median filter.</p> <code>51</code> <code>peak_threshold</code> <code>float</code> <p>Threshold for rescaling of the de-trended signal.</p> <code>0.2</code> <code>binarization_threshold</code> <code>float</code> <p>Threshold for binarizing the de-trended signal.</p> <code>0.1</code> <code>polynomial_degree</code> <code>int</code> <p>Sets the degree of the polynomial for lm fitting.</p> <code>1</code> <code>bias_method</code> <code>str</code> <p>De-trending method, one of ['runmed', 'lm', 'none'].</p> <code>'runmed'</code> <code>n_jobs</code> <code>int</code> <p>Number of jobs to run in parallel.</p> <code>1</code> Source code in <code>arcos4py/tools/_binarize_detrend.py</code> <pre><code>def __init__(\n    self,\n    smooth_k: int = 3,\n    bias_k: int = 51,\n    peak_threshold: float = 0.2,\n    binarization_threshold: float = 0.1,\n    polynomial_degree: int = 1,\n    bias_method: str = \"runmed\",\n    n_jobs: int = 1,\n    **kwargs,\n) -&gt; None:\n    \"\"\"Smooth, de-trend, and binarise the input data.\n\n    Arguments:\n        smooth_k (int): Size of the short-term median smoothing filter.\n        bias_k (int): Size of the long-term de-trending median filter.\n        peak_threshold (float): Threshold for rescaling of the de-trended signal.\n        binarization_threshold (float): Threshold for binarizing the de-trended signal.\n        polynomial_degree (int): Sets the degree of the polynomial for lm fitting.\n        bias_method (str): De-trending method, one of ['runmed', 'lm', 'none'].\n        n_jobs (int): Number of jobs to run in parallel.\n    \"\"\"\n    super().__init__(smooth_k, bias_k, peak_threshold, polynomial_degree, bias_method, n_jobs, **kwargs)\n    self.binarization_threshold = binarization_threshold\n</code></pre>"},{"location":"api/#arcos4py.tools.binData.run","title":"<code>run(x, group_column, measurement_column, frame_column, **kwargs)</code>","text":"<p>Runs binarization and detrending.</p> <p>If the bias_method is 'none', first it rescales the data to between [0,1], then local smoothing is applied to the measurement by groups, followed by binarization.</p> <p>If bias_method is one of ['lm', 'runmed'], first the data is detrended locally with a median filter and then detrended globally, for 'lm' with a linear model and for 'runmed' with a median filter. Followed by binarization of the data.</p> <p>Parameters:</p> Name Type Description Default <code>x</code> <code>DataFrame</code> <p>The time-series data for smoothing, detrending and binarization.</p> required <code>group_column</code> <code>str | None</code> <p>Object id column in x. Detrending and rescaling is performed on a per-object basis. If None, no detrending is performed, only rescaling and bias method is ignored.</p> required <code>measurement_column</code> <code>str</code> <p>Measurement column in x on which detrending and rescaling is performed.</p> required <code>frame_column</code> <code>str</code> <p>Frame column in Time-series data. Used for sorting.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments. Includes old parameters for backwards compatibility. - GroupCol (str): Object id column in x. Detrending and rescaling is performed on a per-object basis. - colMeas (str): Measurement column in x on which detrending and rescaling is performed. - colFrame (str): Frame column in Time-series data. Used for sorting.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Dataframe containing binarized data, rescaled data and the original columns.</p> Source code in <code>arcos4py/tools/_binarize_detrend.py</code> <pre><code>def run(\n    self, x: pd.DataFrame, group_column: str | None, measurement_column: str, frame_column: str, **kwargs\n) -&gt; pd.DataFrame:\n    \"\"\"Runs binarization and detrending.\n\n    If the bias_method is 'none', first it rescales the data to between [0,1], then\n    local smoothing is applied to the measurement by groups, followed by\n    binarization.\n\n    If bias_method is one of ['lm', 'runmed'], first the data is detrended locally with a\n    median filter and then detrended globally, for 'lm' with a linear model and for 'runmed' with a\n    median filter.\n    Followed by binarization of the data.\n\n    Arguments:\n        x (DataFrame): The time-series data for smoothing, detrending and binarization.\n        group_column (str | None): Object id column in x. Detrending and rescaling is performed on a per-object basis.\n            If None, no detrending is performed, only rescaling and bias method is ignored.\n        measurement_column (str): Measurement column in x on which detrending and rescaling is performed.\n        frame_column (str): Frame column in Time-series data. Used for sorting.\n        **kwargs (Any): Additional keyword arguments. Includes old parameters for backwards compatibility.\n            - GroupCol (str): Object id column in x. Detrending and rescaling is performed on a per-object basis.\n            - colMeas (str): Measurement column in x on which detrending and rescaling is performed.\n            - colFrame (str): Frame column in Time-series data. Used for sorting.\n\n    Returns:\n        DataFrame: Dataframe containing binarized data, rescaled data and the original columns.\n    \"\"\"\n    # handle deprecated parameters\n    param_mapping = {\n        \"GroupCol\": \"group_column\",\n        \"colMeas\": \"measurement_column\",\n        \"colFrame\": \"frame_column\",\n    }\n    # allowed_kwargs\n    allowed_kwargs = param_mapping.keys()\n    for key in kwargs:\n        if key not in allowed_kwargs:\n            raise ValueError(f\"Invalid keyword argument: {key}\")\n    updated_kwargs = handle_deprecated_params(param_mapping, **kwargs)\n\n    # update the parameters\n    group_column = updated_kwargs.get(\"group_column\", group_column)\n    measurement_column = updated_kwargs.get(\"measurement_column\", measurement_column)\n    frame_column = updated_kwargs.get(\"frame_column\", frame_column)\n\n    if group_column is None:\n        return self._run_without_groupcol(x, measurement_column, frame_column)\n    else:\n        return self._run_with_groupcol(x, group_column, measurement_column, frame_column)\n</code></pre>"},{"location":"api/#arcos4py.tools.calcCollevStats","title":"<code>calcCollevStats()</code>","text":"<p>Class to calculate statistics of collective events.</p> Source code in <code>arcos4py/tools/_stats.py</code> <pre><code>def __init__(self) -&gt; None:\n    \"\"\"Initialize the class.\"\"\"\n    warnings.warn(\n        \"The 'calcCollevStats' class is deprecated and will be removed in a future version. \"\n        \"Please use the standalone functions instead (calculate_statistics).\",\n        DeprecationWarning,\n    )\n</code></pre>"},{"location":"api/#arcos4py.tools.calcCollevStats.calculate","title":"<code>calculate(data, frame_column, collid_column, obj_id_column, posCol=None)</code>","text":"<p>Calculate summary statistics for collective events based on the entire duration of each event.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>Input data containing information on the collective events.</p> required <code>frame_column</code> <code>str</code> <p>The column name representing the frame numbers.</p> required <code>collid_column</code> <code>str</code> <p>The column name representing the collective event IDs.</p> required <code>obj_id_column</code> <code>str</code> <p>The column name representing the object IDs. Defaults to None.</p> required <code>posCol</code> <code>list</code> <p>List of column names representing the position coordinates. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A DataFrame containing the summary statistics of the collective events.</p> Deprecated Source code in <code>arcos4py/tools/_stats.py</code> <pre><code>def calculate(\n    self,\n    data: pd.DataFrame,\n    frame_column: str,\n    collid_column: str,\n    obj_id_column: Union[str, None],\n    posCol: Union[list, None] = None,\n) -&gt; pd.DataFrame:\n    \"\"\"Calculate summary statistics for collective events based on the entire duration of each event.\n\n    Arguments:\n        data (pd.DataFrame): Input data containing information on the collective events.\n        frame_column (str): The column name representing the frame numbers.\n        collid_column (str): The column name representing the collective event IDs.\n        obj_id_column (str, optional): The column name representing the object IDs. Defaults to None.\n        posCol (list, optional): List of column names representing the position coordinates. Defaults to None.\n\n    Returns:\n        pd.DataFrame: A DataFrame containing the summary statistics of the collective events.\n\n    Deprecated:\n        calculate: Use calculate_statistics instead.\n    \"\"\"\n    warnings.warn(\n        \"The 'calculate' method is deprecated and will be removed in a future version. \"\n        \"Please use the 'calculate_statistics' function instead.\",\n        DeprecationWarning,\n    )\n    return calculate_statistics(data, frame_column, collid_column, obj_id_column, posCol)\n</code></pre>"},{"location":"api/#arcos4py.tools.clipMeas","title":"<code>clipMeas(data)</code>","text":"<p>Clip input array.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>ndarray</code> <p>To be clipped.</p> required Source code in <code>arcos4py/tools/_cleandata.py</code> <pre><code>def __init__(self, data: np.ndarray) -&gt; None:\n    \"\"\"Clips array to quantilles.\n\n    Arguments:\n        data (ndarray): To be clipped.\n    \"\"\"\n    self.data = data\n</code></pre>"},{"location":"api/#arcos4py.tools.clipMeas.clip","title":"<code>clip(clip_low=0.001, clip_high=0.999)</code>","text":"<p>Clip input array to upper and lower quantiles defined in clip_low and clip_high.</p> <p>Parameters:</p> Name Type Description Default <code>clip_low</code> <code>float</code> <p>Lower clipping boundary (quantile).</p> <code>0.001</code> <code>clip_high</code> <code>float</code> <p>Upper clipping boundry (quantille).</p> <code>0.999</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray (np.ndarray): A clipped array of the input data.</p> Source code in <code>arcos4py/tools/_cleandata.py</code> <pre><code>def clip(self, clip_low: float = 0.001, clip_high: float = 0.999) -&gt; np.ndarray:\n    \"\"\"Clip input array to upper and lower quantiles defined in clip_low and clip_high.\n\n    Arguments:\n        clip_low (float): Lower clipping boundary (quantile).\n        clip_high (float): Upper clipping boundry (quantille).\n\n    Returns:\n        np.ndarray (np.ndarray): A clipped array of the input data.\n    \"\"\"\n    low, high = self._calculate_percentile(self.data, clip_low, clip_high)\n    out = self.data.clip(low, high)\n    return out\n</code></pre>"},{"location":"api/#arcos4py.tools.detectCollev","title":"<code>detectCollev(input_data, eps=1, epsPrev=None, minClSz=1, nPrev=1, posCols=['x'], frame_column='time', id_column=None, bin_meas_column='meas', clid_column='clTrackID', dims='TXY', method='dbscan', min_samples=None, linkingMethod='nearest', n_jobs=1, predictor=False, show_progress=True)</code>","text":"<p>Class to detect collective events.</p> <p>Attributes:</p> Name Type Description <code>input_data</code> <code>Union[DataFrame, ndarray]</code> <p>The input data to track.</p> <code>eps</code> <code>float</code> <p>Maximum distance for clustering, default is 1.</p> <code>epsPrev</code> <code>Union[float, None]</code> <p>Maximum distance for linking previous clusters, if None, eps is used. Default is None.</p> <code>minClSz</code> <code>int</code> <p>Minimum cluster size. Default is 3.</p> <code>nPrev</code> <code>int</code> <p>Number of previous frames to consider. Default is 1.</p> <code>posCols</code> <code>list</code> <p>List of column names for the position columns. Default is [\"x\"].</p> <code>frame_column</code> <code>str</code> <p>Name of the column containing the frame number. Default is 'time'.</p> <code>id_column</code> <code>Union[str, None]</code> <p>Name of the column containing the id. Default is None.</p> <code>bin_meas_column</code> <code>Union[str, None]</code> <p>Name of the column containing the binary measurement. Default is 'meas'.</p> <code>clid_column</code> <code>str</code> <p>Name of the column containing the cluster id. Default is 'clTrackID'.</p> <code>dims</code> <code>str</code> <p>String of dimensions in order, such as. Default is \"TXY\". Possible values are \"T\", \"X\", \"Y\", \"Z\".</p> <code>method</code> <code>str</code> <p>The method used for clustering, one of [dbscan, hdbscan]. Default is \"dbscan\".</p> <code>min_samples</code> <code>int | None</code> <p>The number of samples (or total weight) in a neighbourhood for a     point to be considered as a core point. This includes the point itself.     Only used if clusteringMethod is 'hdbscan'. If None, minSamples =  minClsz.</p> <code>linkingMethod</code> <code>str</code> <p>The method used for linking. Default is 'nearest'.</p> <code>n_jobs</code> <code>int</code> <p>Number of jobs to run in parallel. Default is 1.</p> <code>predictor</code> <code>bool | Callable</code> <p>Whether or not to use a predictor. Default is False. True uses the default predictor. A callable can be passed to use a custom predictor. See default predictor method for details.</p> <code>show_progress</code> <code>bool</code> <p>Whether or not to show progress bar. Default is True.</p> <p>Parameters:</p> Name Type Description Default <code>input_data</code> <code>DataFrame</code> <p>Input data to be processed. Must contain a binarized measurement column.</p> required <code>eps</code> <code>float</code> <p>The maximum distance between two samples for one to be considered as in the neighbourhood of the other. This is not a maximum bound on the distances of points within a cluster.</p> <code>1</code> <code>epsPrev</code> <code>float | None</code> <p>Frame to frame distance, value is used to connect collective events across multiple frames.If \"None\", same value as eps is used.</p> <code>None</code> <code>minClSz</code> <code>int</code> <p>Minimum size for a cluster to be identified as a collective event.</p> <code>1</code> <code>nPrev</code> <code>int</code> <p>Number of previous frames the tracking algorithm looks back to connect collective events.</p> <code>1</code> <code>posCols</code> <code>list</code> <p>List of position columns contained in the data. Must at least contain one.</p> <code>['x']</code> <code>frame_column</code> <code>str</code> <p>Indicating the frame column in input_data.</p> <code>'time'</code> <code>id_column</code> <code>str | None</code> <p>Indicating the track id/id column in input_data, optional.</p> <code>None</code> <code>bin_meas_column</code> <code>str</code> <p>Indicating the bin_meas_column in input_data or None.</p> <code>'meas'</code> <code>clid_column</code> <code>str</code> <p>Indicating the column name containing the ids of collective events.</p> <code>'clTrackID'</code> <code>dims</code> <code>str</code> <p>String of dimensions in order, used if input_data is a numpy array. Default is \"TXY\". Possible values are \"T\", \"X\", \"Y\", \"Z\".</p> <code>'TXY'</code> <code>method</code> <code>str</code> <p>The method used for clustering, one of [dbscan, hdbscan]. Default is \"dbscan\".</p> <code>'dbscan'</code> <code>min_samples</code> <code>int | None</code> <p>The number of samples (or total weight) in a neighbourhood for a point to be considered as a core point. This includes the point itself. Only used if clusteringMethod is 'hdbscan'. If None, minSamples =  minClsz.</p> <code>None</code> <code>linkingMethod</code> <code>str</code> <p>The method used for linking. Default is 'nearest'.</p> <code>'nearest'</code> <code>n_jobs</code> <code>int</code> <p>Number of paralell workers to spawn, -1 uses all available cpus.</p> <code>1</code> <code>predictor</code> <code>bool | Callable</code> <p>Whether or not to use a predictor. Default is False. True uses the default predictor. A callable can be passed to use a custom predictor. See default predictor method for details.</p> <code>False</code> <code>show_progress</code> <code>bool</code> <p>Whether or not to show progress bar. Default is True.</p> <code>True</code> Source code in <code>arcos4py/tools/_detect_events.py</code> <pre><code>def __init__(\n    self,\n    input_data: Union[pd.DataFrame, np.ndarray],\n    eps: float = 1,\n    epsPrev: Union[float, None] = None,\n    minClSz: int = 1,\n    nPrev: int = 1,\n    posCols: list = [\"x\"],\n    frame_column: str = 'time',\n    id_column: Union[str, None] = None,\n    bin_meas_column: Union[str, None] = 'meas',\n    clid_column: str = 'clTrackID',\n    dims: str = \"TXY\",\n    method: str = \"dbscan\",\n    min_samples: int | None = None,\n    linkingMethod='nearest',\n    n_jobs: int = 1,\n    predictor: bool | Callable = False,\n    show_progress: bool = True,\n) -&gt; None:\n    \"\"\"Constructs class with input parameters.\n\n    Arguments:\n        input_data (DataFrame): Input data to be processed. Must contain a binarized measurement column.\n        eps (float): The maximum distance between two samples for one to be considered as in\n            the neighbourhood of the other.\n            This is not a maximum bound on the distances of points within a cluster.\n        epsPrev (float | None): Frame to frame distance, value is used to connect\n            collective events across multiple frames.If \"None\", same value as eps is used.\n        minClSz (int): Minimum size for a cluster to be identified as a collective event.\n        nPrev (int): Number of previous frames the tracking\n            algorithm looks back to connect collective events.\n        posCols (list): List of position columns contained in the data.\n            Must at least contain one.\n        frame_column (str): Indicating the frame column in input_data.\n        id_column (str | None): Indicating the track id/id column in input_data, optional.\n        bin_meas_column (str): Indicating the bin_meas_column in input_data or None.\n        clid_column (str): Indicating the column name containing the ids of collective events.\n        dims (str): String of dimensions in order, used if input_data is a numpy array. Default is \"TXY\".\n            Possible values are \"T\", \"X\", \"Y\", \"Z\".\n        method (str): The method used for clustering, one of [dbscan, hdbscan]. Default is \"dbscan\".\n        min_samples (int | None): The number of samples (or total weight) in a neighbourhood for a\n            point to be considered as a core point. This includes the point itself.\n            Only used if clusteringMethod is 'hdbscan'. If None, minSamples =  minClsz.\n        linkingMethod (str): The method used for linking. Default is 'nearest'.\n        n_jobs (int): Number of paralell workers to spawn, -1 uses all available cpus.\n        predictor (bool | Callable): Whether or not to use a predictor. Default is False.\n            True uses the default predictor. A callable can be passed to use a custom predictor.\n            See default predictor method for details.\n        show_progress (bool): Whether or not to show progress bar. Default is True.\n    \"\"\"\n    self.input_data = input_data\n    self.eps = eps\n    self.epsPrev = epsPrev\n    self.minClSz = minClSz\n    self.nPrev = nPrev\n    self.posCols = posCols\n    self.frame_column = frame_column\n    self.id_column = id_column\n    self.bin_meas_column = bin_meas_column\n    self.clid_column = clid_column\n    self.dims = dims\n    self.method = method\n    self.linkingMethod = linkingMethod\n    self.min_samples = min_samples\n    self.predictor = predictor\n    self.n_jobs = n_jobs\n    self.show_progress = show_progress\n    warnings.warn(\n        \"This class is deprecated and will be removed a future release, use the track_events_dataframe or track_events_image functions directly.\",  # noqa: E501\n        DeprecationWarning,\n    )\n</code></pre>"},{"location":"api/#arcos4py.tools.detectCollev.run","title":"<code>run(copy=True)</code>","text":"<p>Runs the collective event detection algorithm.</p> <p>Parameters:</p> Name Type Description Default <code>copy</code> <code>bool</code> <p>Whether or not to copy the input data. Default is True.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Input data with added collective event ids.</p> Source code in <code>arcos4py/tools/_detect_events.py</code> <pre><code>def run(self, copy: bool = True) -&gt; pd.DataFrame:\n    \"\"\"Runs the collective event detection algorithm.\n\n    Arguments:\n        copy (bool): Whether or not to copy the input data. Default is True.\n\n    Returns:\n        DataFrame: Input data with added collective event ids.\n    \"\"\"\n    if isinstance(self.input_data, pd.DataFrame):\n        if copy:\n            self.input_data = self.input_data.copy()\n        return track_events_dataframe(\n            X=self.input_data,\n            position_columns=self.posCols,\n            frame_column=self.frame_column,\n            id_column=self.id_column,\n            binarized_measurement_column=self.bin_meas_column,\n            clid_column=self.clid_column,\n            eps=self.eps,\n            eps_prev=self.epsPrev,\n            min_clustersize=self.minClSz,\n            min_samples=self.min_samples,\n            clustering_method=self.method,\n            linking_method=self.linkingMethod,\n            n_prev=self.nPrev,\n            predictor=self.predictor,\n            n_jobs=self.n_jobs,\n            show_progress=self.show_progress,\n        )\n    elif isinstance(self.input_data, np.ndarray):\n        if copy:\n            self.input_data = np.copy(self.input_data)\n        return track_events_image(\n            X=self.input_data,\n            eps=self.eps,\n            eps_prev=self.epsPrev,\n            min_clustersize=self.minClSz,\n            min_samples=self.min_samples,\n            clustering_method=self.method,\n            n_prev=self.nPrev,\n            predictor=self.predictor,\n            linking_method=self.linkingMethod,\n            dims=self.dims,\n            n_jobs=self.n_jobs,\n            show_progress=self.show_progress,\n        )\n</code></pre>"},{"location":"api/#arcos4py.tools.filterCollev","title":"<code>filterCollev(data, frame_column='time', clid_column='collid', obj_id_column='trackID', **kwargs)</code>","text":"<p>Select Collective events that last longer than coll_duration    and have a larger total size than coll_total_size.</p> <p>Attributes:</p> Name Type Description <code>data</code> <code>Dataframe</code> <p>With detected collective events.</p> <code>frame_column</code> <code>str</code> <p>Indicating the frame column in data.</p> <code>collid_column</code> <code>str</code> <p>Indicating the collective event id column in data.</p> <code>obj_id_column</code> <code>str</code> <p>Inidicating the object identifier column such as cell track id.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Dataframe</code> <p>With detected collective events.</p> required <code>frame_column</code> <code>str</code> <p>Indicating the frame column in data.</p> <code>'time'</code> <code>clid_column</code> <code>str</code> <p>Indicating the collective event id column in data.</p> <code>'collid'</code> <code>obj_id_column</code> <code>str</code> <p>Inidicating the object identifier column such as cell track id.</p> <code>'trackID'</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments. Includes deprecated parameters. - collid_column (str): Deprecated. Use clid_column instead.</p> <code>{}</code> Source code in <code>arcos4py/tools/_filter_events.py</code> <pre><code>def __init__(\n    self,\n    data: pd.DataFrame,\n    frame_column: str = \"time\",\n    clid_column: str = \"collid\",\n    obj_id_column: str = \"trackID\",\n    **kwargs,\n):\n    \"\"\"Constructs filterCollev class with Parameters.\n\n    Arguments:\n        data (Dataframe): With detected collective events.\n        frame_column (str): Indicating the frame column in data.\n        clid_column (str): Indicating the collective event id column in data.\n        obj_id_column (str): Inidicating the object identifier column such as cell track id.\n        **kwargs (Any): Additional keyword arguments. Includes deprecated parameters.\n            - collid_column (str): Deprecated. Use clid_column instead.\n    \"\"\"\n    map_deprecated_params = {\n        \"collid_column\": \"clid_column\",\n    }\n\n    # check allowed kwargs\n    allowed_kwargs = map_deprecated_params.keys()\n    for key in kwargs:\n        if key not in allowed_kwargs:\n            raise ValueError(f\"Got an unexpected keyword argument '{key}'\")\n\n    updated_kwargs = handle_deprecated_params(map_deprecated_params, **kwargs)\n\n    # Assigning the parameters\n    clid_column = updated_kwargs.get(\"clid_column\", clid_column)\n\n    self.data = data\n    self.frame_column = frame_column\n    self.clid_column = clid_column\n    self.obj_id_column = obj_id_column\n</code></pre>"},{"location":"api/#arcos4py.tools.filterCollev.filter","title":"<code>filter(min_duration=9, min_total_size=10, **kwargs)</code>","text":"<p>Filter collective events.</p> <p>Method to filter collective events according to the parameters specified in the object instance.</p> <p>Parameters:</p> Name Type Description Default <code>min_duration</code> <code>int</code> <p>Minimal duration of collective events to be selected.</p> <code>9</code> <code>min_total_size</code> <code>int</code> <p>Minimal total size of collective events to be selected.</p> <code>10</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments. Includes deprecated parameters. - coll_duration (int): Deprecated. Use min_duration instead. - coll_total_size (int): Deprecated. Use min_total_size instead.</p> <code>{}</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>Returns pandas dataframe containing filtered collective events</p> Source code in <code>arcos4py/tools/_filter_events.py</code> <pre><code>def filter(self, min_duration: int = 9, min_total_size: int = 10, **kwargs) -&gt; pd.DataFrame:\n    \"\"\"Filter collective events.\n\n    Method to filter collective events according to the\n    parameters specified in the object instance.\n\n    Arguments:\n        min_duration (int): Minimal duration of collective events to be selected.\n        min_total_size (int): Minimal total size of collective events to be selected.\n        **kwargs (Any): Additional keyword arguments. Includes deprecated parameters.\n            - coll_duration (int): Deprecated. Use min_duration instead.\n            - coll_total_size (int): Deprecated. Use min_total_size instead.\n\n    Returns:\n         Returns pandas dataframe containing filtered collective events\n    \"\"\"\n    map_deprecated_params = {\n        \"coll_duration\": \"min_duration\",\n        \"coll_total_size\": \"min_total_size\",\n    }\n\n    # check allowed kwargs\n    allowed_kwargs = map_deprecated_params.keys()\n    for key in kwargs:\n        if key not in allowed_kwargs:\n            raise ValueError(f\"Got an unexpected keyword argument '{key}'\")\n\n    updated_kwargs = handle_deprecated_params(map_deprecated_params, **kwargs)\n\n    # Assigning the parameters\n    min_duration = updated_kwargs.get(\"min_duration\", min_duration)\n    min_total_size = updated_kwargs.get(\"min_total_size\", min_total_size)\n\n    if self.data.empty:\n        return self.data\n    stats = calcCollevStats()\n    stats_df = stats.calculate(self.data, self.frame_column, self.clid_column, self.obj_id_column)\n\n    filtered_df = self._filter_collev(\n        data=self.data,\n        clid_stats=stats_df,\n        clid_column=self.clid_column,\n        min_duration=min_duration,\n        min_total_size=min_total_size,\n    )\n    return filtered_df\n</code></pre>"},{"location":"api/#arcos4py.tools.interpolation","title":"<code>interpolation(data)</code>","text":"<p>Interpolate nan values in a numpy array.</p> <p>Attributes:</p> Name Type Description <code>data</code> <code>DataFrame</code> <p>Where NaN should be replaced with interpolated values.</p> <p>Uses pandas.interpolate with liner interpolation.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>Where NaN should be replaced with interpolated values.</p> required Source code in <code>arcos4py/tools/_cleandata.py</code> <pre><code>def __init__(self, data: pd.DataFrame):\n    \"\"\"Interpolate nan values in a pandas dataframe.\n\n    Uses pandas.interpolate with liner interpolation.\n\n    Arguments:\n        data (DataFrame): Where NaN should be replaced with interpolated values.\n    \"\"\"\n    self.data = data\n</code></pre>"},{"location":"api/#arcos4py.tools.interpolation.interpolate","title":"<code>interpolate()</code>","text":"<p>Interpolate nan and missing values.</p> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Interpolated input data.</p> Source code in <code>arcos4py/tools/_cleandata.py</code> <pre><code>def interpolate(self) -&gt; pd.DataFrame:\n    \"\"\"Interpolate nan and missing values.\n\n    Returns:\n        DataFrame: Interpolated input data.\n    \"\"\"\n    self.data = self.data.interpolate(axis=0)\n\n    return self.data\n</code></pre>"},{"location":"api/#arcos4py.tools.calculate_statistics","title":"<code>calculate_statistics(data, frame_column='frame', clid_column='collid', obj_id_column=None, position_columns=None, **kwargs)</code>","text":"<p>Calculate summary statistics for collective events based on the entire duration of each event.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>Input data containing information on the collective events.</p> required <code>frame_column</code> <code>str</code> <p>The column name representing the frame numbers.</p> <code>'frame'</code> <code>clid_column</code> <code>str</code> <p>The column name representing the collective event IDs.</p> <code>'collid'</code> <code>obj_id_column</code> <code>str</code> <p>The column name representing the object IDs. Defaults to None.</p> <code>None</code> <code>position_columns</code> <code>List[str]</code> <p>List of column names representing the position coordinates. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments. Includes deprecated parameters. - collid_column (str): Deprecated. Use clid_column instead. - pos_columns (List[str], optional): Deprecated. Use position_columns instead.</p> <code>{}</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A DataFrame containing the summary statistics of the collective events.</p> Statistics Calculated <ul> <li>collid: The unique ID representing each collective event.</li> <li>duration: The duration of each event, calculated as the difference between the maximum     and minimum frame values plus one.</li> <li>first_timepoint, last_timepoint: The first and last frames in which each event occurs.</li> <li>total_size: The total number of unique objects involved in each event     (calculated if obj_id_column is provided).</li> <li>min_size, max_size: The minimum and maximum size of each event,     defined as the number of objects in the event's smallest and largest frames, respectively.</li> <li>first_frame_centroid_x, first_frame_centroid_y, last_frame_centroid_x, last_frame_centroid_y:     The x and y coordinates of the centroid of all objects in the first and last frames of each event     (calculated if posCol is provided).</li> <li>centroid_speed: The speed of the centroid, calculated as the distance between     the first and last frame centroids divided by the duration (calculated if posCol is provided).</li> <li>direction: The direction of motion of the centroid, calculated as the arctangent of the change in y divided     the change in x (calculated if posCol is provided).</li> <li>first_frame_spatial_extent, last_frame_spatial_extent: The maximum distance between any pair of objects in the first and last frames (calculated if posCol is provided).</li> <li>first_frame_convex_hull_area, last_frame_convex_hull_area: The areas of the convex hulls enclosing all objects     in the first and last frames (calculated if posCol is provided).</li> <li>size_variability: The standard deviation of the event size over all frames, providing a measure of the     variability in the size of the event over time (calculated if obj_id_column is provided).</li> </ul> Source code in <code>arcos4py/tools/_stats.py</code> <pre><code>def calculate_statistics(\n    data: pd.DataFrame,\n    frame_column: str = \"frame\",\n    clid_column: str = \"collid\",\n    obj_id_column: Union[str, None] = None,\n    position_columns: Union[List[str], None] = None,\n    **kwargs,\n) -&gt; pd.DataFrame:\n    \"\"\"Calculate summary statistics for collective events based on the entire duration of each event.\n\n    Arguments:\n        data (pd.DataFrame): Input data containing information on the collective events.\n        frame_column (str): The column name representing the frame numbers.\n        clid_column (str): The column name representing the collective event IDs.\n        obj_id_column (str, optional): The column name representing the object IDs. Defaults to None.\n        position_columns (List[str], optional): List of column names representing the position coordinates. Defaults to None.\n        **kwargs (Any): Additional keyword arguments. Includes deprecated parameters.\n            - collid_column (str): Deprecated. Use clid_column instead.\n            - pos_columns (List[str], optional): Deprecated. Use position_columns instead.\n\n    Returns:\n        pd.DataFrame: A DataFrame containing the summary statistics of the collective events.\n\n    Statistics Calculated:\n        - collid: The unique ID representing each collective event.\n        - duration: The duration of each event, calculated as the difference between the maximum\n            and minimum frame values plus one.\n        - first_timepoint, last_timepoint: The first and last frames in which each event occurs.\n        - total_size: The total number of unique objects involved in each event\n            (calculated if obj_id_column is provided).\n        - min_size, max_size: The minimum and maximum size of each event,\n            defined as the number of objects in the event's smallest and largest frames, respectively.\n        - first_frame_centroid_x, first_frame_centroid_y, last_frame_centroid_x, last_frame_centroid_y:\n            The x and y coordinates of the centroid of all objects in the first and last frames of each event\n            (calculated if posCol is provided).\n        - centroid_speed: The speed of the centroid, calculated as the distance between\n            the first and last frame centroids divided by the duration (calculated if posCol is provided).\n        - direction: The direction of motion of the centroid, calculated as the arctangent of the change in y divided\n            the change in x (calculated if posCol is provided).\n        - first_frame_spatial_extent, last_frame_spatial_extent: The maximum distance between any pair of objects in the\n        first and last frames (calculated if posCol is provided).\n        - first_frame_convex_hull_area, last_frame_convex_hull_area: The areas of the convex hulls enclosing all objects\n            in the first and last frames (calculated if posCol is provided).\n        - size_variability: The standard deviation of the event size over all frames, providing a measure of the\n            variability in the size of the event over time (calculated if obj_id_column is provided).\n    \"\"\"\n    map_deprecated_params = {\n        \"collid_column\": \"clid_column\",\n        \"pos_columns\": \"position_columns\",\n    }\n\n    # check allowed kwargs\n    allowed_kwargs = map_deprecated_params.keys()\n    for key in kwargs:\n        if key not in allowed_kwargs:\n            raise ValueError(f\"Got an unexpected keyword argument '{key}'\")\n\n    updated_kwargs = handle_deprecated_params(map_deprecated_params, **kwargs)\n\n    # Assigning the parameters\n    clid_column = updated_kwargs.get(\"clid_column\", clid_column)\n    position_columns = updated_kwargs.get(\"position_columns\", position_columns)\n\n    # Error handling: Check if necessary columns are present in the input data\n    if data.empty:\n        return pd.DataFrame(\n            columns=[clid_column, \"duration\", \"first_timepoint\", \"last_timepoint\"]\n            + ([\"total_size\", \"min_size\", \"max_size\", \"size_variability\"] if obj_id_column else [])\n            + (\n                [f'first_frame_centroid_{col}' for col in position_columns]\n                + [f'last_frame_centroid_{col}' for col in position_columns]\n                + [\"centroid_speed\", \"direction\"]\n                + [f'{t}_spatial_extent' for t in [\"first_frame\", \"last_frame\"]]\n                + [f'{t}_convex_hull_area' for t in [\"first_frame\", \"last_frame\"]]\n                if position_columns\n                else []\n            )\n        )\n\n    necessary_columns = [frame_column, clid_column]\n    if obj_id_column:\n        necessary_columns.append(obj_id_column)\n    if position_columns:\n        necessary_columns.extend(position_columns)\n\n    for col in necessary_columns:\n        if col not in data.columns and col is not None:\n            raise ValueError(f\"The column '{col}' is not present in the input data.\")\n\n    collid_groups = data.groupby(clid_column)\n\n    # Initialize an empty list to store the statistics\n    stats_list = []\n\n    for collid, group_data in collid_groups:\n\n        collid_stats = {clid_column: collid}\n\n        # Grouping by collid_column to get initial statistics\n        duration = group_data[frame_column].max() - group_data[frame_column].min() + 1\n        collid_stats['duration'] = duration\n        collid_stats['first_timepoint'] = group_data[frame_column].min()\n        collid_stats['last_timepoint'] = group_data[frame_column].max()\n\n        # If obj_id_column is provided, calculate size related stats\n        if obj_id_column:\n            total_size = group_data[obj_id_column].nunique()\n\n            collid_stats['total_size'] = total_size\n\n        # calculate min and max size based on the number of objects in each frame\n        frame_size_stats = group_data.groupby(frame_column).size()\n        collid_stats['min_size'] = frame_size_stats.min()\n        collid_stats['max_size'] = frame_size_stats.max()\n\n        # If posCol is provided, calculate centroid coordinates for the\n        if position_columns:\n            tp_1 = collid_stats['first_timepoint']\n            tp_2 = collid_stats['last_timepoint']\n\n            centroid_data = group_data.groupby(frame_column)[position_columns].mean().reset_index()\n\n            for col in position_columns:\n                collid_stats[f'first_frame_centroid_{col}'] = centroid_data.query(f'{frame_column} == {tp_1}')[\n                    col\n                ].to_numpy()[0]\n                collid_stats[f'last_frame_centroid_{col}'] = centroid_data.query(f'{frame_column} == {tp_2}')[\n                    col\n                ].to_numpy()[0]\n\n            # Calculate speed and direction\n            speed = np.linalg.norm(\n                np.column_stack([collid_stats[f'first_frame_centroid_{col}'] for col in position_columns])\n                - np.column_stack([collid_stats[f'last_frame_centroid_{col}'] for col in position_columns]),\n                axis=1,\n            ) / (collid_stats['duration'] - 1)\n\n            collid_stats['centroid_speed'] = speed[0]\n\n            # Direction For 2D data\n            if len(position_columns) == 2:\n                collid_stats['direction'] = np.arctan2(\n                    collid_stats[f'last_frame_centroid_{position_columns[1]}']\n                    - collid_stats[f'first_frame_centroid_{position_columns[1]}'],\n                    collid_stats[f'last_frame_centroid_{position_columns[0]}']\n                    - collid_stats[f'first_frame_centroid_{position_columns[0]}'],\n                )\n            # Direction For 3D data\n            elif len(position_columns) == 3:\n                dx = (\n                    collid_stats[f'last_frame_centroid_{position_columns[0]}']\n                    - collid_stats[f'first_frame_centroid_{position_columns[0]}']\n                )\n                dy = (\n                    collid_stats[f'last_frame_centroid_{position_columns[1]}']\n                    - collid_stats[f'first_frame_centroid_{position_columns[1]}']\n                )\n                dz = (\n                    collid_stats[f'last_frame_centroid_{position_columns[2]}']\n                    - collid_stats[f'first_frame_centroid_{position_columns[2]}']\n                )\n\n                # Calculate azimuth and elevation\n                collid_stats['azimuth'] = np.arctan2(dy, dx)\n                collid_stats['elevation'] = np.arctan2(dz, np.sqrt(dx**2 + dy**2))\n            else:\n                raise ValueError(\"Position columns can only be 2 or 3.\")\n\n            # Loop over first and last frames separately to calculate the spatial extent and convex hull area\n            for frame_name, frame_number in zip(['first_frame', 'last_frame'], [tp_1, tp_2]):\n                # Get data for either the first or last frame\n                frame_data = group_data.query(f'{frame_column} == {frame_number}')\n\n                # Calculate spatial extent\n                spatial_extent = pdist(frame_data[position_columns].values).max() if len(frame_data) &gt; 1 else 0\n                collid_stats[f'{frame_name}_spatial_extent'] = spatial_extent\n\n                # Calculate convex hull area\n                try:\n                    convex_hull_area = (\n                        ConvexHull(frame_data[position_columns].values).volume\n                        if len(frame_data) &gt; len(position_columns)\n                        else 0\n                    )\n                except QhullError:\n                    convex_hull_area = 0\n                collid_stats[f'{frame_name}_convex_hull_area'] = convex_hull_area\n\n        stats_list.append(collid_stats)\n\n    # Create a DataFrame from the list of statistics\n    stats_df = pd.DataFrame(stats_list)\n\n    # Calculate size variability\n    if obj_id_column:\n        # Calculating size for each collid and frame\n        frame_size_stats = data.groupby([clid_column, frame_column])[obj_id_column].nunique().reset_index(name='size')\n        size_variability = frame_size_stats.groupby(clid_column)['size'].std().reset_index(name='size_variability')\n        stats_df = stats_df.merge(size_variability, on=clid_column, how='left')\n\n    return stats_df\n</code></pre>"},{"location":"api/#arcos4py.tools.calculate_statistics_per_frame","title":"<code>calculate_statistics_per_frame(data, frame_column='frame', clid_column='collid', position_columns=None, **kwargs)</code>","text":"<p>Calculate summary statistics for collective events based on the entire duration of each event.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>Input data containing information on the collective events.</p> required <code>frame_column</code> <code>str</code> <p>The column name representing the frame numbers.</p> <code>'frame'</code> <code>clid_column</code> <code>str</code> <p>The column name representing the collective event IDs.</p> <code>'collid'</code> <code>position_columns</code> <code>List[str]</code> <p>List of column names representing the position coordinates. Defaults to None.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments. Includes deprecated parameters. - collid_column (str): Deprecated. Use clid_column instead. - pos_columns (List[str], optional): Deprecated. Use position_columns instead.</p> <code>{}</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: A DataFrame containing the summary statistics of the collective events.</p> Statistics Calculated <ul> <li>collid: The unique ID representing each collective event.</li> <li>frame: The frame number.</li> <li>size: The number of objects in the collective event</li> <li>centroid_x, centroid_y: The x and y coordinates of the centroid of all objects in the collective event     (calculated if pos_columns is provided).</li> <li>spatial_extent: The maximum distance between any pair of objects in the collective event     (calculated if pos_columns is provided).</li> <li>convex_hull_area: The area of the convex hull enclosing all objects in the collective event     (calculated if pos_columns is provided).</li> <li>direction: The direction of motion of the centroid, calculated as the arctangent of the change in y divided     the change in x (calculated if pos_columns is provided).</li> <li>centroid_speed: The speed of the centroid, calculated as the norm of the change     in x and y divided by the duration (calculated if pos_columns is provided).</li> </ul> Source code in <code>arcos4py/tools/_stats.py</code> <pre><code>def calculate_statistics_per_frame(\n    data: pd.DataFrame,\n    frame_column: str = \"frame\",\n    clid_column: str = \"collid\",\n    position_columns: Union[List[str], None] = None,\n    **kwargs,\n) -&gt; pd.DataFrame:\n    \"\"\"Calculate summary statistics for collective events based on the entire duration of each event.\n\n    Arguments:\n        data (pd.DataFrame): Input data containing information on the collective events.\n        frame_column (str): The column name representing the frame numbers.\n        clid_column (str): The column name representing the collective event IDs.\n        position_columns (List[str], optional): List of column names representing the position coordinates. Defaults to None.\n        **kwargs (Any): Additional keyword arguments. Includes deprecated parameters.\n            - collid_column (str): Deprecated. Use clid_column instead.\n            - pos_columns (List[str], optional): Deprecated. Use position_columns instead.\n\n\n    Returns:\n        pd.DataFrame: A DataFrame containing the summary statistics of the collective events.\n\n    Statistics Calculated:\n        - collid: The unique ID representing each collective event.\n        - frame: The frame number.\n        - size: The number of objects in the collective event\n        - centroid_x, centroid_y: The x and y coordinates of the centroid of all objects in the collective event\n            (calculated if pos_columns is provided).\n        - spatial_extent: The maximum distance between any pair of objects in the collective event\n            (calculated if pos_columns is provided).\n        - convex_hull_area: The area of the convex hull enclosing all objects in the collective event\n            (calculated if pos_columns is provided).\n        - direction: The direction of motion of the centroid, calculated as the arctangent of the change in y divided\n            the change in x (calculated if pos_columns is provided).\n        - centroid_speed: The speed of the centroid, calculated as the norm of the change\n            in x and y divided by the duration (calculated if pos_columns is provided).\n    \"\"\"\n    map_deprecated_params = {\n        \"collid_column\": \"clid_column\",\n        \"pos_columns\": \"position_columns\",\n    }\n\n    # check allowed kwargs\n    allowed_kwargs = map_deprecated_params.keys()\n    for key in kwargs:\n        if key not in allowed_kwargs:\n            raise ValueError(f\"Got an unexpected keyword argument '{key}'\")\n\n    updated_kwargs = handle_deprecated_params(map_deprecated_params, **kwargs)\n\n    # Assigning the parameters\n    clid_column = updated_kwargs.get(\"clid_column\", clid_column)\n    position_columns = updated_kwargs.get(\"position_columns\", position_columns)\n\n    necessary_columns = [frame_column, clid_column]\n    if position_columns:\n        necessary_columns.extend(position_columns)\n\n    for col in necessary_columns:\n        if col not in data.columns and col is not None:\n            raise ValueError(f\"The column '{col}' is not present in the input data.\")\n\n    if data.empty:\n        return pd.DataFrame(\n            columns=(\n                [clid_column, frame_column, \"size\"]\n                + [f\"centroid_{col}\" for col in position_columns]\n                + [\"spatial_extent\", \"convex_hull_area\", \"centroid_speed\", \"direction\"]\n                if position_columns\n                else []\n            )\n        )\n\n    collid_groups = data.groupby([frame_column, clid_column])\n    stats_list = []\n\n    for (frame, collid), group_data in collid_groups:\n\n        frame_stats = {clid_column: collid, frame_column: frame}\n\n        frame_stats['size'] = group_data.count()[frame_column]\n\n        # If pos_columns are provided, calculate spatial statistics for this frame\n        if position_columns:\n            # Calculate centroid\n            centroid = group_data[position_columns].mean().to_dict()\n            for pos_col, cent_val in centroid.items():\n                frame_stats[f'centroid_{pos_col}'] = cent_val\n\n            # Calculate spatial extent\n            spatial_extent = pdist(group_data[position_columns].values).max() if len(group_data) &gt; 1 else 0\n            frame_stats['spatial_extent'] = spatial_extent\n\n            # Calculate convex hull area\n            try:\n                convex_hull_area = (\n                    ConvexHull(group_data[position_columns].values).volume\n                    if len(group_data) &gt; len(position_columns)\n                    else 0\n                )\n            except QhullError:\n                convex_hull_area = 0\n            frame_stats['convex_hull_area'] = convex_hull_area\n\n        stats_list.append(frame_stats)\n\n    # Create a DataFrame from the list of statistics\n    stats_df = pd.DataFrame(stats_list)\n\n    # If pos_columns are provided, we can calculate speed and direction by looking at changes between frames\n    if position_columns:\n        stats_df.sort_values(by=[clid_column, frame_column], inplace=True)\n\n        for i, col in enumerate(position_columns):\n            stats_df[f'delta_{col}'] = stats_df.groupby(clid_column)[f'centroid_{col}'].diff()\n\n        # Calculate speed (the norm of the delta vector)\n        stats_df['centroid_speed'] = np.linalg.norm(\n            stats_df[[f'delta_{col}' for col in position_columns]].values, axis=1\n        )\n\n        # Calculate direction (only for 2D)\n        if len(position_columns) == 2:\n            stats_df['direction'] = np.arctan2(\n                stats_df['delta_' + position_columns[1]], stats_df['delta_' + position_columns[0]]\n            )\n\n        # Clean up temporary delta columns\n        stats_df.drop(columns=[f'delta_{col}' for col in position_columns], inplace=True)\n\n    return stats_df\n</code></pre>"},{"location":"api/#arcos4py.tools.estimate_eps","title":"<code>estimate_eps(data=None, image=None, method='kneepoint', position_columns=None, frame_column='t', n_neighbors=5, plot=True, plt_size=(5, 5), max_samples=50000, binarize_threshold=0, **kwargs)</code>","text":"<p>Estimates eps parameter for DBSCAN using the k-distance graph method.</p> <p>Works with either point data in a DataFrame or pixel data from an image/image series.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Optional[DataFrame]</code> <p>DataFrame containing coordinates and frame info.                            Required if 'image' is None.</p> <code>None</code> <code>image</code> <code>Optional[ndarray]</code> <p>Image array (2D) or time series (3D).                         Required if 'data' is None.</p> <code>None</code> <code>method</code> <code>str</code> <p>Method for choosing eps from k-distances: 'kneepoint', 'mean', 'median'.</p> <code>'kneepoint'</code> <code>position_columns</code> <code>Optional[list[str]]</code> <p>Column names for spatial coordinates in 'data'.                                     Defaults to ['y', 'x'] for 2D images or ['y', 'x', 'z'] for 3D.                                     Required if 'data' is provided.</p> <code>None</code> <code>frame_column</code> <code>str</code> <p>Column name for frame/time in 'data'. Defaults to 't'.</p> <code>'t'</code> <code>n_neighbors</code> <code>int</code> <p>The 'k' for k-distance calculation (distance to k-th neighbor).                Typically set to MinPts-1 for DBSCAN. Defaults to 5.</p> <code>5</code> <code>plot</code> <code>bool</code> <p>If True, plots the sorted k-distance graph with the estimated eps.</p> <code>True</code> <code>plt_size</code> <code>tuple[int, int]</code> <p>Figure size for the plot.</p> <code>(5, 5)</code> <code>max_samples</code> <code>int</code> <p>Max number of k-distances to use for estimation (subsampling).</p> <code>50000</code> <code>binarize_threshold</code> <code>float</code> <p>Threshold for converting 'image' pixels to points.</p> <code>0</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments passed to the estimation method.              For 'kneepoint': S, online, curve, interp_method, direction, polynomial_degree.              For 'mean'/'median': mean_multiplier, median_multiplier (defaults to 1.5).</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>float</code> <code>float</code> <p>Estimated eps value.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If input requirements are not met (e.g., both/neither data/image given,         missing columns, no valid distances found).</p> Source code in <code>arcos4py/tools/_detect_events.py</code> <pre><code>def estimate_eps(  # noqa: C901\n    data: Optional[pd.DataFrame] = None,\n    image: Optional[np.ndarray] = None,\n    method: str = \"kneepoint\",\n    position_columns: Optional[list[str]] = None,\n    frame_column: str = \"t\",\n    n_neighbors: int = 5,\n    plot: bool = True,\n    plt_size: tuple[int, int] = (5, 5),\n    max_samples: int = 50_000,\n    binarize_threshold: float = 0,\n    **kwargs: Any,\n) -&gt; float:\n    \"\"\"Estimates eps parameter for DBSCAN using the k-distance graph method.\n\n    Works with either point data in a DataFrame or pixel data from an image/image series.\n\n    Args:\n        data (Optional[pd.DataFrame]): DataFrame containing coordinates and frame info.\n                                       Required if 'image' is None.\n        image (Optional[np.ndarray]): Image array (2D) or time series (3D).\n                                    Required if 'data' is None.\n        method (str): Method for choosing eps from k-distances: 'kneepoint', 'mean', 'median'.\n        position_columns (Optional[list[str]]): Column names for spatial coordinates in 'data'.\n                                                Defaults to ['y', 'x'] for 2D images or ['y', 'x', 'z'] for 3D.\n                                                Required if 'data' is provided.\n        frame_column (str): Column name for frame/time in 'data'. Defaults to 't'.\n        n_neighbors (int): The 'k' for k-distance calculation (distance to k-th neighbor).\n                           Typically set to MinPts-1 for DBSCAN. Defaults to 5.\n        plot (bool): If True, plots the sorted k-distance graph with the estimated eps.\n        plt_size (tuple[int, int]): Figure size for the plot.\n        max_samples (int): Max number of k-distances to use for estimation (subsampling).\n        binarize_threshold (float): Threshold for converting 'image' pixels to points.\n        **kwargs (Any): Additional keyword arguments passed to the estimation method.\n                         For 'kneepoint': S, online, curve, interp_method, direction, polynomial_degree.\n                         For 'mean'/'median': mean_multiplier, median_multiplier (defaults to 1.5).\n\n    Returns:\n        float: Estimated eps value.\n\n    Raises:\n        ValueError: If input requirements are not met (e.g., both/neither data/image given,\n                    missing columns, no valid distances found).\n    \"\"\"\n    method_options = [\"kneepoint\", \"mean\", \"median\"]\n    if method not in method_options:\n        raise ValueError(f\"Method must be one of {method_options}\")\n\n    if (data is None and image is None) or (data is not None and image is not None):\n        raise ValueError(\"Provide either a DataFrame ('data') or an image ('image'), not both.\")\n\n    data_processed: Optional[pd.DataFrame] = None\n\n    # --- Process Image Input ---\n    if image is not None:\n        ndim = image.ndim\n        coords_list = []\n\n        if ndim == 3:  # Time series (T, Y, X) or (T, Z, Y, X)? Assuming (T, Y, X) or (T, Z, Y, X)\n            n_frames = image.shape[0]\n            img_dims = image.shape[1:]  # Spatial dimensions\n            if position_columns is None:\n                # Default names based on spatial dimensions\n                if len(img_dims) == 2:  # (Y, X)\n                    position_columns = ['y', 'x']\n                elif len(img_dims) == 3:  # (Z, Y, X)\n                    position_columns = ['z', 'y', 'x']\n                else:\n                    raise ValueError(f\"Unsupported image spatial dimensions: {len(img_dims)}\")\n\n            elif len(position_columns) != len(img_dims):\n                raise ValueError(\n                    f\"Length of position_columns ({len(position_columns)}) must match image spatial dimensions ({len(img_dims)}).\"\n                )\n\n            print(f\"Processing {n_frames} image frames with threshold {binarize_threshold}...\")\n            for t, img_frame in enumerate(image):\n                frame_coords = _binarize_image_to_coords(img_frame, threshold=binarize_threshold)\n                if frame_coords.size &gt; 0:\n                    # Add frame number as the first column\n                    coords_with_frame = np.column_stack((np.full(frame_coords.shape[0], t), frame_coords))\n                    coords_list.append(coords_with_frame)\n\n        elif ndim == 2:  # Single image (Y, X) or (Z, Y, X)? Assuming (Y, X)\n            img_dims = image.shape\n            if position_columns is None:\n                # Default names based on spatial dimensions\n                if len(img_dims) == 2:  # (Y, X)\n                    position_columns = ['y', 'x']\n                # Add handling for single 3D image if needed\n                # elif len(img_dims) == 3: # (Z, Y, X)\n                #    position_columns = ['z', 'y', 'x']\n                else:\n                    raise ValueError(f\"Unsupported image spatial dimensions: {len(img_dims)}\")\n            elif len(position_columns) != len(img_dims):\n                raise ValueError(\n                    f\"Length of position_columns ({len(position_columns)}) must match image spatial dimensions ({len(img_dims)}).\"\n                )\n\n            print(f\"Processing single image frame with threshold {binarize_threshold}...\")\n            frame_coords = _binarize_image_to_coords(image, threshold=binarize_threshold)\n            if frame_coords.size &gt; 0:\n                # Add a dummy frame number (0) as the first column\n                coords_with_frame = np.column_stack((np.zeros(frame_coords.shape[0]), frame_coords))\n                coords_list.append(coords_with_frame)\n        else:\n            raise ValueError(f\"Unsupported image dimension: {ndim}. Expecting 2 or 3.\")\n\n        if not coords_list:\n            raise ValueError(f\"No coordinates found in image data after applying threshold {binarize_threshold}.\")\n\n        # Combine coordinates from all frames\n        all_coords_np = np.vstack(coords_list)\n        data_processed = pd.DataFrame(all_coords_np, columns=[frame_column] + position_columns)\n\n    # --- Process DataFrame Input ---\n    elif data is not None:\n        if position_columns is None:\n            raise ValueError(\"`position_columns` must be provided when input is a DataFrame.\")\n        # Validate DataFrame structure\n        required_cols = [frame_column] + position_columns\n        missing_cols = [col for col in required_cols if col not in data.columns]\n        if missing_cols:\n            raise ValueError(f\"Missing required columns in DataFrame: {missing_cols}\")\n        data_processed = data[required_cols].copy()  # Work on a copy\n\n    # Should have data_processed DataFrame by now\n    if data_processed is None or data_processed.empty:\n        raise ValueError(\"Could not process input data into a valid format.\")\n\n    # --- Calculate k-Distances ---\n    # Convert relevant columns to numpy for efficiency\n    if position_columns is None:\n        raise ValueError(\"position_columns must be provided.\")\n    data_np = data_processed[[frame_column] + position_columns].to_numpy(dtype=np.float64)\n\n    # Sort by frame first, essential for correct splitting\n    data_np = data_np[data_np[:, 0].argsort()]\n\n    # Get unique frame numbers and the indices where each new frame starts\n    unique_frames, frame_start_indices = np.unique(data_np[:, 0], return_index=True)\n\n    grouped_coords = np.split(data_np[:, 1:], frame_start_indices[1:])\n\n    print(f\"Calculating {n_neighbors}-th neighbor distances for {len(grouped_coords)} frames...\")\n    all_k_distances = []\n    for i, frame_coords in enumerate(grouped_coords):\n        if frame_coords.shape[0] &gt; n_neighbors:\n            k_distances = _get_kth_neighbor_distance(frame_coords, k=n_neighbors)\n            if k_distances.size &gt; 0:\n                all_k_distances.append(k_distances)\n\n    if not all_k_distances:\n        raise ValueError(f\"No frames found with enough points (&gt; {n_neighbors}) to calculate k-th neighbor distances.\")\n\n    # Combine distances from all valid frames\n    distances_array = np.concatenate(all_k_distances)\n\n    # Remove any non-finite values (though less likely with KDTree distances)\n    distances_finite = distances_array[np.isfinite(distances_array)]\n\n    if distances_finite.shape[0] == 0:\n        raise ValueError(\"No valid finite k-th neighbor distances found.\")\n\n    # Subsample if necessary\n    n_total_distances = distances_finite.shape[0]\n    if n_total_distances &gt; max_samples:\n        print(f\"Subsampling {max_samples} distances from {n_total_distances} for estimation.\")\n        distances_sampled = np.random.choice(distances_finite, max_samples, replace=False)\n    else:\n        distances_sampled = distances_finite\n\n    # Sort the distances for analysis and plotting\n    distances_sorted = np.sort(distances_sampled)\n\n    # --- Estimate eps ---\n    eps: float = 0.0  # Initialize eps\n\n    print(f\"Estimating eps using '{method}' method...\")\n    if method == \"kneepoint\":\n        kneedle_kwargs = {\n            'S': kwargs.get(\"S\", 1.0),\n            'online': kwargs.get(\"online\", False),\n            'curve': kwargs.get(\"curve\", \"convex\"),\n            'direction': kwargs.get(\"direction\", \"increasing\"),\n            'interp_method': kwargs.get(\"interp_method\", \"interp1d\"),\n            'polynomial_degree': kwargs.get(\"polynomial_degree\", 7),\n        }\n        # Filter out None values potentially returned by kwargs.get if default was None\n        kneedle_kwargs = {k: v for k, v in kneedle_kwargs.items() if v is not None}\n\n        kneedle = KneeLocator(x=np.arange(distances_sorted.shape[0]), y=distances_sorted, **kneedle_kwargs)\n        if kneedle.knee is None:\n            warnings.warn(\"Kneepoint detection failed. Falling back to median distance as eps.\")\n            # Fallback strategy: use median * 1.0 (no multiplier)\n            eps = np.median(distances_sorted)\n        else:\n            eps = float(distances_sorted[kneedle.knee])  # Ensure float type\n\n    elif method == \"mean\":\n        multiplier = kwargs.get(\"mean_multiplier\", 1.5)\n        eps = np.mean(distances_sorted) * multiplier\n\n    elif method == \"median\":\n        multiplier = kwargs.get(\"median_multiplier\", 1.5)\n        eps = np.median(distances_sorted) * multiplier\n\n    print(f\"Estimated eps: {eps:.4f}\")\n\n    # --- Plotting ---\n    if plot:\n        fig, ax = plt.subplots(figsize=plt_size)\n        ax.plot(distances_sorted, marker='.', linestyle='-', markersize=2, label=f'{n_neighbors}-th Neighbor Distance')\n        ax.axhline(eps, color=\"r\", linestyle=\"--\", label=f'Estimated eps = {eps:.4f}')\n\n        # Annotate kneepoint if found\n        if method == \"kneepoint\" and 'kneedle' in locals() and kneedle.knee is not None:\n            ax.plot(kneedle.knee, distances_sorted[kneedle.knee], 'ro', markersize=6, label='Detected Knee')\n\n        ax.set_xlabel(\"Points Sorted by Distance\")\n        ax.set_ylabel(f\"Distance to {n_neighbors}-th Nearest Neighbor\")\n        ax.set_title(\"k-Distance Graph for eps Estimation\")\n        ax.legend()\n        ax.grid(True, linestyle=':', alpha=0.6)\n        plt.tight_layout()\n        plt.show()\n\n    return eps\n</code></pre>"},{"location":"api/#arcos4py.tools.remove_image_background","title":"<code>remove_image_background(image, filter_type='gaussian', size=(10, 1, 1), dims='TXY', crop_time_axis=False)</code>","text":"<p>Removes background from images. Assumes axis order (t, y, x) for 2d images and (t, z, y, x) for 3d images.</p> <p>Parameters:</p> Name Type Description Default <code>image</code> <code>ndarray</code> <p>Image to remove background from.</p> required <code>filter_type</code> <code>Union[str, function]</code> <p>Filter to use to remove background. Can be one of ['median', 'gaussian'].</p> <code>'gaussian'</code> <code>size</code> <code>(int, Tuple)</code> <p>Size of filter to use. For median filter, this is the size of the window. For gaussian filter, this is the standard deviation. If a single int is passed in, it is assumed to be the same for all dimensions. If a tuple is passed in, it is assumed to correspond to the size of the filter in each dimension. Default is (10, 1, 1).</p> <code>(10, 1, 1)</code> <code>dims</code> <code>str</code> <p>Dimensions to apply filter over. Can be one of ['TXY', 'TZXY']. Default is 'TXY'.</p> <code>'TXY'</code> <code>crop_time_axis</code> <code>bool</code> <p>Whether to crop the time axis. Default is True.</p> <code>False</code> <p>Returns (np.ndarray): Image with background removed.     Along the first axis (t) half of the filter size is removed from the beginning and end respectively.</p> Source code in <code>arcos4py/tools/_cleandata.py</code> <pre><code>def remove_image_background(\n    image: np.ndarray, filter_type: str = 'gaussian', size=(10, 1, 1), dims=\"TXY\", crop_time_axis: bool = False\n) -&gt; np.ndarray:\n    \"\"\"Removes background from images. Assumes axis order (t, y, x) for 2d images and (t, z, y, x) for 3d images.\n\n    Arguments:\n        image (np.ndarray): Image to remove background from.\n        filter_type (Union[str, function]): Filter to use to remove background. Can be one of ['median', 'gaussian'].\n        size (int, Tuple): Size of filter to use. For median filter, this is the size of the window.\n            For gaussian filter, this is the standard deviation.\n            If a single int is passed in, it is assumed to be the same for all dimensions.\n            If a tuple is passed in, it is assumed to correspond to the size of the filter in each dimension.\n            Default is (10, 1, 1).\n        dims (str): Dimensions to apply filter over. Can be one of ['TXY', 'TZXY']. Default is 'TXY'.\n        crop_time_axis (bool): Whether to crop the time axis. Default is True.\n    Returns (np.ndarray): Image with background removed.\n        Along the first axis (t) half of the filter size is removed from the beginning and end respectively.\n    \"\"\"\n    # correct images with a filter applied over time\n    allowed_filters = [\"median\", \"gaussian\"]\n    dims_list = list(dims.upper())\n\n    # check input\n    for i in dims_list:\n        if i not in dims_list:\n            raise ValueError(f\"Invalid dimension {i}. Must be 'T', 'X', 'Y', or 'Z'.\")\n\n    if len(dims_list) &gt; len(set(dims_list)):\n        raise ValueError(\"Duplicate dimensions in dims.\")\n\n    if len(dims_list) != image.ndim:\n        raise ValueError(\n            f\"Length of dims must be equal to number of dimensions in image. Image has {image.ndim} dimensions.\"\n        )\n    # make sure axis dont occur twice and that they are valid\n    if len(dims) != len(set(dims)):\n        raise ValueError('Dimensions must not occur twice.')\n\n    if filter_type not in allowed_filters:\n        raise ValueError(f'Filter type must be one of {allowed_filters}.')\n\n    # get index of time axis\n    t_idx = dims_list.index(\"T\")\n\n    orig_image = image.copy()\n\n    if isinstance(size, int):\n        size = (size,) * image.ndim\n    elif isinstance(size, tuple):\n        if len(size) != image.ndim:\n            raise ValueError(f'Filter size must have {image.ndim} dimensions.')\n        # check size of dimensions are compatible with image\n        for idx, s in enumerate(size):\n            if s &gt; image.shape[idx]:\n                raise ValueError(f'Filter size in dimension {idx} is larger than image size in that dimension.')\n    else:\n        raise ValueError('Filter size must be an int or tuple.')\n\n    if filter_type == 'median':\n        filtered = median_filter(orig_image, size=size)\n    elif filter_type == 'gaussian':\n        filtered = gaussian_filter(orig_image, sigma=size)\n\n    # crop time axis if necessary\n    shift = size[t_idx] // 2\n    corr = np.subtract(orig_image, filtered, dtype=np.float32)\n    if crop_time_axis:\n        corr = corr[shift:-shift]\n\n    return corr\n</code></pre>"},{"location":"api/#arcos4py.tools.track_events_dataframe","title":"<code>track_events_dataframe(X, position_columns, frame_column, id_column=None, binarized_measurement_column=None, clid_column='collid', eps=1.0, eps_prev=None, min_clustersize=3, min_samples=None, clustering_method='dbscan', linking_method='nearest', allow_merges=False, allow_splits=False, stability_threshold=10, remove_small_clusters=False, min_size_for_split=1, reg=1, reg_m=10, cost_threshold=0, n_prev=1, predictor=False, n_jobs=1, show_progress=True, **kwargs)</code>","text":"<p>Function to track collective events in a dataframe.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>DataFrame</code> <p>The input dataframe containing the data to track.</p> required <code>position_columns</code> <code>List[str]</code> <p>The names of the columns representing coordinates.</p> required <code>frame_column</code> <code>str</code> <p>The name of the column containing frame ids.</p> required <code>id_column</code> <code>str | None</code> <p>The name of the column representing IDs. None if no such column.</p> <code>None</code> <code>binarized_measurement_column</code> <code>str | None</code> <p>The name of the column representing binarized measurements, if None all measurements are used.</p> <code>None</code> <code>clid_column</code> <code>str</code> <p>The name of the output column representing collective events, will be generated.</p> <code>'collid'</code> <code>eps</code> <code>float</code> <p>Maximum distance for clustering, default is 1.</p> <code>1.0</code> <code>eps_prev</code> <code>float | None</code> <p>Maximum distance for linking previous clusters, if None, eps is used. Default is None.</p> <code>None</code> <code>min_clustersize</code> <code>int</code> <p>Minimum cluster size. Default is 3.</p> <code>3</code> <code>min_samples</code> <code>int</code> <p>The number of samples (or total weight) in a neighbourhood for a point to be considered as a core point. This includes the point itself. Only used if clusteringMethod is 'hdbscan'. If None, minSamples =  minClsz.</p> <code>None</code> <code>clustering_method</code> <code>str</code> <p>The method used for clustering, one of [dbscan, hdbscan]. Default is \"dbscan\".</p> <code>'dbscan'</code> <code>linking_method</code> <code>str</code> <p>The method used for linking, one of ['nearest', 'transportsolver']. Default is 'nearest'.</p> <code>'nearest'</code> <code>allow_merges</code> <code>bool</code> <p>Whether or not to allow merges. Default is False.</p> <code>False</code> <code>allow_splits</code> <code>bool</code> <p>Whether or not to allow splits. Default is False.</p> <code>False</code> <code>stability_threshold</code> <code>int</code> <p>Number of frames to consider for stability. Default is 10.</p> <code>10</code> <code>remove_small_clusters</code> <code>bool</code> <p>Whether or not to remove small clusters. Default is False.</p> <code>False</code> <code>min_size_for_split</code> <code>int</code> <p>Minimum size for a split. Default is 1.</p> <code>1</code> <code>reg</code> <code>float</code> <p>Regularization parameter for transportation solver. Default is 1.</p> <code>1</code> <code>reg_m</code> <code>float</code> <p>Regularization parameter for transportation solver. Default is 10.</p> <code>10</code> <code>cost_threshold</code> <code>float</code> <p>Cost threshold for transportation solver. Default is 0.</p> <code>0</code> <code>n_prev</code> <code>int</code> <p>Number of previous frames to consider. Default is 1.</p> <code>1</code> <code>predictor</code> <code>bool | Callable</code> <p>Whether or not to use a predictor. Default is False. True uses the default predictor. A callable can be passed to use a custom predictor. See default predictor method for details.</p> <code>False</code> <code>n_jobs</code> <code>int</code> <p>Number of jobs to run in parallel. Default is 1.</p> <code>1</code> <code>show_progress</code> <code>bool</code> <p>Whether or not to show progress bar. Default is True.</p> <code>True</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments. Includes deprecated parameters for backwards compatibility. - epsPrev: Deprecated parameter for eps_prev. Use eps_prev instead. - minClSz: Deprecated parameter for min_clustersize. Use min_clustersize instead. - minSamples: Deprecated parameter for min_samples. Use min_samples instead. - clusteringMethod: Deprecated parameter for clustering_method. Use clustering_method instead. - linkingMethod: Deprecated parameter for linking_method. Use linking_method instead. - nPrev: Deprecated parameter for n_prev. Use n_prev instead. - nJobs: Deprecated parameter for n_jobs. Use n_jobs instead. - showProgress: Deprecated parameter for show_progress. Use show_progress instead.</p> <code>{}</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: Dataframe with tracked events.</p> Source code in <code>arcos4py/tools/_detect_events.py</code> <pre><code>def track_events_dataframe(\n    X: pd.DataFrame,\n    position_columns: List[str],\n    frame_column: str,\n    id_column: str | None = None,\n    binarized_measurement_column: str | None = None,\n    clid_column: str = \"collid\",\n    eps: float = 1.0,\n    eps_prev: float | None = None,\n    min_clustersize: int = 3,\n    min_samples: int | None = None,\n    clustering_method: str = \"dbscan\",\n    linking_method: str = 'nearest',\n    allow_merges: bool = False,\n    allow_splits: bool = False,\n    stability_threshold: int = 10,\n    remove_small_clusters: bool = False,\n    min_size_for_split: int = 1,\n    reg: float = 1,\n    reg_m: float = 10,\n    cost_threshold: float = 0,\n    n_prev: int = 1,\n    predictor: bool | Callable = False,\n    n_jobs: int = 1,\n    show_progress: bool = True,\n    **kwargs,\n) -&gt; pd.DataFrame:\n    \"\"\"Function to track collective events in a dataframe.\n\n    Arguments:\n        X (pd.DataFrame): The input dataframe containing the data to track.\n        position_columns (List[str]): The names of the columns representing coordinates.\n        frame_column (str): The name of the column containing frame ids.\n        id_column (str | None): The name of the column representing IDs. None if no such column.\n        binarized_measurement_column (str | None): The name of the column representing binarized measurements,\n            if None all measurements are used.\n        clid_column (str): The name of the output column representing collective events, will be generated.\n        eps (float): Maximum distance for clustering, default is 1.\n        eps_prev (float | None): Maximum distance for linking previous clusters, if None, eps is used. Default is None.\n        min_clustersize (int): Minimum cluster size. Default is 3.\n        min_samples (int): The number of samples (or total weight) in a neighbourhood for a\n            point to be considered as a core point. This includes the point itself.\n            Only used if clusteringMethod is 'hdbscan'. If None, minSamples =  minClsz.\n        clustering_method (str): The method used for clustering, one of [dbscan, hdbscan]. Default is \"dbscan\".\n        linking_method (str): The method used for linking, one of ['nearest', 'transportsolver']. Default is 'nearest'.\n        allow_merges (bool): Whether or not to allow merges. Default is False.\n        allow_splits (bool): Whether or not to allow splits. Default is False.\n        stability_threshold (int): Number of frames to consider for stability. Default is 10.\n        remove_small_clusters (bool): Whether or not to remove small clusters. Default is False.\n        min_size_for_split (int): Minimum size for a split. Default is 1.\n        reg (float): Regularization parameter for transportation solver. Default is 1.\n        reg_m (float): Regularization parameter for transportation solver. Default is 10.\n        cost_threshold (float): Cost threshold for transportation solver. Default is 0.\n        n_prev (int): Number of previous frames to consider. Default is 1.\n        predictor (bool | Callable): Whether or not to use a predictor. Default is False.\n            True uses the default predictor. A callable can be passed to use a custom predictor.\n            See default predictor method for details.\n        n_jobs (int): Number of jobs to run in parallel. Default is 1.\n        show_progress (bool): Whether or not to show progress bar. Default is True.\n        **kwargs (Any): Additional keyword arguments. Includes deprecated parameters for backwards compatibility.\n            - epsPrev: Deprecated parameter for eps_prev. Use eps_prev instead.\n            - minClSz: Deprecated parameter for min_clustersize. Use min_clustersize instead.\n            - minSamples: Deprecated parameter for min_samples. Use min_samples instead.\n            - clusteringMethod: Deprecated parameter for clustering_method. Use clustering_method instead.\n            - linkingMethod: Deprecated parameter for linking_method. Use linking_method instead.\n            - nPrev: Deprecated parameter for n_prev. Use n_prev instead.\n            - nJobs: Deprecated parameter for n_jobs. Use n_jobs instead.\n            - showProgress: Deprecated parameter for show_progress. Use show_progress instead.\n\n    Returns:\n        pd.DataFrame: Dataframe with tracked events.\n    \"\"\"\n    map_params = {\n        \"coordinates_column\": \"position_columns\",\n        \"bin_meas_column\": \"binarized_measurement_column\",\n        \"collid_column\": \"clid_column\",\n        'epsPrev': 'eps_prev',\n        'minClSz': 'min_clustersize',\n        'minSamples': 'min_samples',\n        'clusteringMethod': 'clustering_method',\n        'linkingMethod': 'linking_method',\n        'nPrev': 'n_prev',\n        'nJobs': 'n_jobs',\n        'showProgress': 'show_progress',\n    }\n\n    # check for allowed kwargs\n    for key in kwargs:\n        if key not in map_params.keys():\n            raise ValueError(f'Invalid keyword argument {key}')\n\n    # Handle deprecated parameters\n    kwargs = handle_deprecated_params(map_params, **kwargs)\n\n    # Assign parameters\n    eps_prev = kwargs.get('eps_prev', eps_prev)\n    min_clustersize = kwargs.get('min_clustersize', min_clustersize)\n    min_samples = kwargs.get('min_samples', min_samples)\n    clustering_method = kwargs.get('clustering_method', clustering_method)\n    linking_method = kwargs.get('linking_method', linking_method)\n    n_prev = kwargs.get('n_prev', n_prev)\n    n_jobs = kwargs.get('n_jobs', n_jobs)\n\n    linker = Linker(\n        eps=eps,\n        eps_prev=eps_prev,\n        min_clustersize=min_clustersize,\n        min_samples=min_samples,\n        clustering_method=clustering_method,\n        linking_method=linking_method,\n        n_prev=n_prev,\n        predictor=predictor,\n        n_jobs=n_jobs,\n        allow_merges=allow_merges,\n        allow_splits=allow_splits,\n        stability_threshold=stability_threshold,\n        remove_small_clusters=remove_small_clusters,\n        min_size_for_split=min_size_for_split,\n        reg=reg,\n        reg_m=reg_m,\n        cost_threshold=cost_threshold,\n    )\n\n    tracker = DataFrameTracker(\n        linker=linker,\n        position_columns=position_columns,\n        frame_column=frame_column,\n        obj_id_column=id_column,\n        binarized_measurement_column=binarized_measurement_column,\n        clid_column=clid_column,\n    )\n    df_out = pd.concat(\n        [timepoint for timepoint in tqdm(tracker.track(X), total=X[frame_column].nunique(), disable=not show_progress)]\n    ).reset_index(drop=True)\n\n    if any([allow_merges, allow_splits]):\n        return df_out.query(f\"{clid_column} != -1\").reset_index(drop=True), linker.lineage_tracker\n    return df_out.query(f\"{clid_column} != -1\").reset_index(drop=True)\n</code></pre>"},{"location":"api/#arcos4py.tools.track_events_image","title":"<code>track_events_image(X, eps=1, eps_prev=None, min_clustersize=1, min_samples=None, clustering_method='dbscan', n_prev=1, predictor=False, linking_method='nearest', allow_merges=False, allow_splits=False, stability_threshold=10, remove_small_clusters=False, min_size_for_split=1, reg=1, reg_m=10, cost_threshold=0, dims='TXY', downsample=1, n_jobs=1, show_progress=True, **kwargs)</code>","text":"<p>Function to track events in an image using specified linking and clustering methods.</p> <p>Parameters:</p> Name Type Description Default <code>X</code> <code>ndarray</code> <p>The input array containing the images to track.</p> required <code>eps</code> <code>float</code> <p>Distance for clustering. Default is 1.</p> <code>1</code> <code>eps_prev</code> <code>float | None</code> <p>Maximum distance for linking previous clusters, if None, eps is used. Default is None.</p> <code>None</code> <code>min_clustersize</code> <code>int</code> <p>Minimum cluster size. Default is 1.</p> <code>1</code> <code>min_samples</code> <code>int | None</code> <p>The number of samples (or total weight) in a neighbourhood for a point to be considered as a core point. This includes the point itself. Only used if clusteringMethod is 'hdbscan'. If None, minSamples =  minClsz.</p> <code>None</code> <code>clustering_method</code> <code>str</code> <p>The method used for clustering, one of [dbscan, hdbscan]. Default is \"dbscan\".</p> <code>'dbscan'</code> <code>n_prev</code> <code>int</code> <p>Number of previous frames to consider. Default is 1.</p> <code>1</code> <code>predictor</code> <code>bool | Callable</code> <p>Whether or not to use a predictor. Default is False. True uses the default predictor. A callable can be passed to use a custom predictor. See default predictor method for details.</p> <code>False</code> <code>linking_method</code> <code>str</code> <p>The method used for linking. Default is 'nearest'.</p> <code>'nearest'</code> <code>allow_merges</code> <code>bool</code> <p>Whether or not to allow merges. Default is False.</p> <code>False</code> <code>allow_splits</code> <code>bool</code> <p>Whether or not to allow splits. Default is False.</p> <code>False</code> <code>stability_threshold</code> <code>int</code> <p>The number of frames required for a stable merge or split. Default is 10.</p> <code>10</code> <code>remove_small_clusters</code> <code>bool</code> <p>Whether or not to remove small clusters. Default is False.</p> <code>False</code> <code>min_size_for_split</code> <code>int</code> <p>Minimum size for a split. Default is 1.</p> <code>1</code> <code>reg</code> <code>float</code> <p>Entropy regularization parameter for unbalanced OT algorithm (only for transportation linking).</p> <code>1</code> <code>reg_m</code> <code>float</code> <p>Marginal relaxation parameter for unbalanced OT (only for transportation linking).</p> <code>10</code> <code>cost_threshold</code> <code>float</code> <p>Threshold for filtering low-probability matches (only for transportation linking).</p> <code>0</code> <code>dims</code> <code>str</code> <p>String of dimensions in order, such as. Default is \"TXY\". Possible values are \"T\", \"X\", \"Y\", \"Z\".</p> <code>'TXY'</code> <code>downsample</code> <code>int</code> <p>Factor by which to downsample the image. Default is 1.</p> <code>1</code> <code>n_jobs</code> <code>int</code> <p>Number of jobs to run in parallel. Default is 1.</p> <code>1</code> <code>show_progress</code> <code>bool</code> <p>Whether or not to show progress bar. Default is True.</p> <code>True</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments. Includes deprecated parameters for backwards compatibility. - epsPrev: Deprecated parameter for eps_prev. Use eps_prev instead. - minClSz: Deprecated parameter for min_clustersize. Use min_clustersize instead. - minSamples: Deprecated parameter for min_samples. Use min_samples instead. - clusteringMethod: Deprecated parameter for clustering_method. Use clustering_method instead. - linkingMethod: Deprecated parameter for linking_method. Use linking_method instead. - nPrev: Deprecated parameter for n_prev. Use n_prev instead. - nJobs: Deprecated parameter for n_jobs. Use n_jobs instead. - showProgress: Deprecated parameter for show_progress. Use show_progress instead.</p> <code>{}</code> <p>Returns:</p> Type Description <code>ndarray | tuple[ndarray, LineageTracker]</code> <p>np.ndarray: Array of images with tracked events.</p> Source code in <code>arcos4py/tools/_detect_events.py</code> <pre><code>def track_events_image(\n    X: np.ndarray,\n    eps: float = 1,\n    eps_prev: float | None = None,\n    min_clustersize: int = 1,\n    min_samples: int | None = None,\n    clustering_method: str = \"dbscan\",\n    n_prev: int = 1,\n    predictor: bool | Callable = False,\n    linking_method: str = 'nearest',\n    allow_merges: bool = False,\n    allow_splits: bool = False,\n    stability_threshold: int = 10,\n    remove_small_clusters: bool = False,\n    min_size_for_split: int = 1,\n    reg: float = 1,\n    reg_m: float = 10,\n    cost_threshold: float = 0,\n    dims: str = \"TXY\",\n    downsample: int = 1,\n    n_jobs: int = 1,\n    show_progress: bool = True,\n    **kwargs,\n) -&gt; np.ndarray | tuple[np.ndarray, LineageTracker]:\n    \"\"\"Function to track events in an image using specified linking and clustering methods.\n\n    Arguments:\n        X (np.ndarray): The input array containing the images to track.\n        eps (float): Distance for clustering. Default is 1.\n        eps_prev (float | None): Maximum distance for linking previous clusters, if None, eps is used. Default is None.\n        min_clustersize (int): Minimum cluster size. Default is 1.\n        min_samples (int | None): The number of samples (or total weight) in a neighbourhood for a\n            point to be considered as a core point. This includes the point itself.\n            Only used if clusteringMethod is 'hdbscan'. If None, minSamples =  minClsz.\n        clustering_method (str): The method used for clustering, one of [dbscan, hdbscan]. Default is \"dbscan\".\n        n_prev (int): Number of previous frames to consider. Default is 1.\n        predictor (bool | Callable): Whether or not to use a predictor. Default is False.\n            True uses the default predictor. A callable can be passed to use a custom predictor.\n            See default predictor method for details.\n        linking_method (str): The method used for linking. Default is 'nearest'.\n        allow_merges (bool): Whether or not to allow merges. Default is False.\n        allow_splits (bool): Whether or not to allow splits. Default is False.\n        stability_threshold (int): The number of frames required for a stable merge or split. Default is 10.\n        remove_small_clusters (bool): Whether or not to remove small clusters. Default is False.\n        min_size_for_split (int): Minimum size for a split. Default is 1.\n        reg (float): Entropy regularization parameter for unbalanced OT algorithm (only for transportation linking).\n        reg_m (float): Marginal relaxation parameter for unbalanced OT (only for transportation linking).\n        cost_threshold (float): Threshold for filtering low-probability matches (only for transportation linking).\n        dims (str): String of dimensions in order, such as. Default is \"TXY\". Possible values are \"T\", \"X\", \"Y\", \"Z\".\n        downsample (int): Factor by which to downsample the image. Default is 1.\n        n_jobs (int): Number of jobs to run in parallel. Default is 1.\n        show_progress (bool): Whether or not to show progress bar. Default is True.\n        **kwargs (Any): Additional keyword arguments. Includes deprecated parameters for backwards compatibility.\n            - epsPrev: Deprecated parameter for eps_prev. Use eps_prev instead.\n            - minClSz: Deprecated parameter for min_clustersize. Use min_clustersize instead.\n            - minSamples: Deprecated parameter for min_samples. Use min_samples instead.\n            - clusteringMethod: Deprecated parameter for clustering_method. Use clustering_method instead.\n            - linkingMethod: Deprecated parameter for linking_method. Use linking_method instead.\n            - nPrev: Deprecated parameter for n_prev. Use n_prev instead.\n            - nJobs: Deprecated parameter for n_jobs. Use n_jobs instead.\n            - showProgress: Deprecated parameter for show_progress. Use show_progress instead.\n\n    Returns:\n        np.ndarray: Array of images with tracked events.\n    \"\"\"\n    map_params = {\n        'epsPrev': 'eps_prev',\n        'minClSz': 'min_clustersize',\n        'minSamples': 'min_samples',\n        'clusteringMethod': 'clustering_method',\n        'linkingMethod': 'linking_method',\n        'nPrev': 'n_prev',\n        'nJobs': 'n_jobs',\n        'showProgress': 'show_progress',\n    }\n\n    # check for allowed kwargs\n    for key in kwargs:\n        if key not in map_params.keys():\n            raise ValueError(f'Invalid keyword argument {key}')\n\n    # Handle deprecated parameters\n    kwargs = handle_deprecated_params(map_params, **kwargs)\n\n    # Assign parameters\n    eps_prev = kwargs.get('eps_prev', eps_prev)\n    min_clustersize = kwargs.get('min_clustersize', min_clustersize)\n    min_samples = kwargs.get('min_samples', min_samples)\n    clustering_method = kwargs.get('clustering_method', clustering_method)\n    linking_method = kwargs.get('linking_method', linking_method)\n    n_prev = kwargs.get('n_prev', n_prev)\n    n_jobs = kwargs.get('n_jobs', n_jobs)\n\n    # Determine the dimensionality\n    spatial_dims = set(\"XYZ\")\n    D = len([d for d in dims if d in spatial_dims])\n\n    # Adjust parameters based on dimensionality\n    adjusted_epsPrev = eps_prev / downsample if eps_prev is not None else None\n    adjusted_minClSz = int(min_clustersize / (downsample**D))\n    adjusted_minSamples = int(min_samples / (downsample**D)) if min_samples is not None else None\n\n    linker = Linker(\n        eps=eps / downsample,\n        eps_prev=adjusted_epsPrev,\n        min_clustersize=adjusted_minClSz,\n        min_samples=adjusted_minSamples,\n        clustering_method=clustering_method,\n        linking_method=linking_method,\n        n_prev=n_prev,\n        predictor=predictor,\n        reg=reg,\n        reg_m=reg_m,\n        cost_threshold=cost_threshold,\n        n_jobs=n_jobs,\n        allow_merges=allow_merges,\n        allow_splits=allow_splits,\n        stability_threshold=stability_threshold,\n        remove_small_clusters=remove_small_clusters,\n        min_size_for_split=min_size_for_split,\n    )\n    tracker = ImageTracker(linker, downsample=downsample)\n    # find indices of T in dims\n    T_index = dims.upper().index(\"T\")\n    out = np.zeros_like(X, dtype=np.uint16)\n\n    for i in tqdm(range(X.shape[T_index]), disable=not show_progress):\n        out[i] = tracker.track_iteration(X[i])\n\n    if any([allow_merges, allow_splits]):\n        return out, linker.lineage_tracker\n\n    return out\n</code></pre>"},{"location":"api/#arcos4py.validation","title":"<code>validation</code>","text":"<p>Tools for validating detected collective events.</p>"},{"location":"api/#arcos4py.validation.bootstrap_arcos","title":"<code>bootstrap_arcos(df, position_columns=['x'], frame_column='frame', obj_id_column='obj_id', measurement_column='m', method='shuffle_tracks', smooth_k=3, bias_k=51, peak_threshold=0.2, binarization_threshold=0.1, polynomial_degree=1, bias_method='runmed', eps=2, eps_prev=None, min_clustersize=1, n_prev=1, min_duration=1, min_total_size=1, stats_metric=['total_size', 'duration'], pval_alternative='greater', finite_correction=True, n=100, seed=42, allow_duplicates=False, max_tries=100, show_progress=True, verbose=False, parallel_processing=True, plot=True, **kwargs)</code>","text":"<p>Bootstrap data using the ARCOS algorithm.</p> <p>Parameters:</p> Name Type Description Default <code>df</code> <code>DataFrame</code> <p>DataFrame containing the data to be bootstrapped.</p> required <code>position_columns</code> <code>list</code> <p>List of column names containing the x and y coordinates.</p> <code>['x']</code> <code>frame_column</code> <code>str</code> <p>Name of the column containing the frame number.</p> <code>'frame'</code> <code>obj_id_column</code> <code>str</code> <p>Name of the column containing the track id.</p> <code>'obj_id'</code> <code>measurement_column</code> <code>str</code> <p>Name of the column containing the measurement.</p> <code>'m'</code> <code>method</code> <code>str | list[str]</code> <p>Method used for bootstrapping. Can be \"shuffle_tracks\", 'shuffle_timepoints', 'shift_timepoints', 'shuffle_binary_blocks', 'shuffle_coordinates_timepoint or a list of methods, which will be applied in order of index.</p> <code>'shuffle_tracks'</code> <code>smooth_k</code> <code>int</code> <p>Smoothing kernel size.</p> <code>3</code> <code>bias_k</code> <code>int</code> <p>Bias kernel size.</p> <code>51</code> <code>peak_threshold</code> <code>float</code> <p>Threshold for peak detection.</p> <code>0.2</code> <code>binarization_threshold</code> <code>float</code> <p>Threshold for binarization.</p> <code>0.1</code> <code>polynomial_degree</code> <code>int</code> <p>Degree of the polynomial used for bias correction.</p> <code>1</code> <code>bias_method</code> <code>str</code> <p>Bias correction method. Can be 'none', 'runmed', 'lm'</p> <code>'runmed'</code> <code>eps</code> <code>float</code> <p>Epsilon parameter for DBSCAN.</p> <code>2</code> <code>eps_prev</code> <code>int | None</code> <p>Parameter for linking tracks. If None, eps is used.</p> <code>None</code> <code>min_clustersize</code> <code>int</code> <p>Minimum cluster size.</p> <code>1</code> <code>n_prev</code> <code>int</code> <p>Number of previous frames to consider for linking.</p> <code>1</code> <code>min_duration</code> <code>int</code> <p>Minimum duration of a track.</p> <code>1</code> <code>min_total_size</code> <code>int</code> <p>Minimum size of a track.</p> <code>1</code> <code>stats_metric</code> <code>str | list[str]</code> <p>Metric to calculate. Can be \"duration\", \"total_size\", \"min_size\", \"max_size\" or a list of metrics. Default is [\"duration\", \"total_size\"].</p> <code>['total_size', 'duration']</code> <code>pval_alternative</code> <code>str</code> <p>Alternative hypothesis for the p-value calculation. Can be \"less\" or \"greater\".</p> <code>'greater'</code> <code>finite_correction</code> <code>bool</code> <p>Correct p-values for finite sampling. Default is True.</p> <code>True</code> <code>n</code> <code>int</code> <p>Number of bootstraps.</p> <code>100</code> <code>seed</code> <code>int</code> <p>Seed for the random number generator.</p> <code>42</code> <code>allow_duplicates</code> <code>bool</code> <p>If False, resampling will check if the resampled data contains duplicates. If True, duplicates will be allowed.</p> <code>False</code> <code>max_tries</code> <code>int</code> <p>Maximum number of tries to resample data without duplicates.</p> <code>100</code> <code>show_progress</code> <code>bool</code> <p>Show a progress bar.</p> <code>True</code> <code>verbose</code> <code>bool</code> <p>Print additional information.</p> <code>False</code> <code>parallel_processing</code> <code>bool</code> <p>Use parallel processing.</p> <code>True</code> <code>plot</code> <code>bool</code> <p>Plot the distribution of the bootstrapped data.</p> <code>True</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments. Includes deprecated parameters. - id_column: Deprecated. Use obj_id_column instead. - meas_column: Deprecated. Use measurement_column instead. - smoothK: Deprecated. Use smooth_k instead. - biasK: Deprecated. Use bias_k instead. - peakThr: Deprecated. Use peak_threshold instead. - binThr: Deprecated. Use binarization_threshold instead. - polyDeg: Deprecated. Use polynomial_degree instead. - biasMet: Deprecated. Use bias_method instead. - epsPrev: Deprecated. Use eps_prev instead. - minClsz: Deprecated. Use min_clustersize instead. - min_size: Deprecated. Use min_total_size instead. - paralell_processing: Deprecated. Use parallel_processing instead.</p> <code>{}</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>DataFrame containing the bootstrapped data.</p> Source code in <code>arcos4py/validation/_bootstrapping.py</code> <pre><code>def bootstrap_arcos(\n    df: pd.DataFrame,\n    position_columns: list = ['x'],\n    frame_column: str = 'frame',\n    obj_id_column: str = 'obj_id',\n    measurement_column: str = 'm',\n    method: str | list[str] = 'shuffle_tracks',\n    smooth_k: int = 3,\n    bias_k: int = 51,\n    peak_threshold: float = 0.2,\n    binarization_threshold: float = 0.1,\n    polynomial_degree: int = 1,\n    bias_method: str = \"runmed\",\n    eps: float = 2,\n    eps_prev: int | None = None,\n    min_clustersize: int = 1,\n    n_prev: int = 1,\n    min_duration: int = 1,\n    min_total_size: int = 1,\n    stats_metric: str | list[str] = [\"total_size\", \"duration\"],\n    pval_alternative: str = \"greater\",\n    finite_correction: bool = True,\n    n: int = 100,\n    seed: int = 42,\n    allow_duplicates: bool = False,\n    max_tries: int = 100,\n    show_progress: bool = True,\n    verbose: bool = False,\n    parallel_processing: bool = True,\n    plot: bool = True,\n    **kwargs,\n) -&gt; tuple[pd.DataFrame, pd.DataFrame]:\n    \"\"\"Bootstrap data using the ARCOS algorithm.\n\n    Arguments:\n        df: DataFrame containing the data to be bootstrapped.\n        position_columns: List of column names containing the x and y coordinates.\n        frame_column: Name of the column containing the frame number.\n        obj_id_column: Name of the column containing the track id.\n        measurement_column: Name of the column containing the measurement.\n        method: Method used for bootstrapping. Can be \"shuffle_tracks\", 'shuffle_timepoints', 'shift_timepoints',\n            'shuffle_binary_blocks', 'shuffle_coordinates_timepoint or a list of methods,\n            which will be applied in order of index.\n        smooth_k: Smoothing kernel size.\n        bias_k: Bias kernel size.\n        peak_threshold: Threshold for peak detection.\n        binarization_threshold: Threshold for binarization.\n        polynomial_degree: Degree of the polynomial used for bias correction.\n        bias_method: Bias correction method. Can be 'none', 'runmed', 'lm'\n        eps: Epsilon parameter for DBSCAN.\n        eps_prev: Parameter for linking tracks. If None, eps is used.\n        min_clustersize: Minimum cluster size.\n        n_prev: Number of previous frames to consider for linking.\n        min_duration: Minimum duration of a track.\n        min_total_size: Minimum size of a track.\n        stats_metric: Metric to calculate. Can be \"duration\", \"total_size\", \"min_size\", \"max_size\" or a list of metrics.\n            Default is [\"duration\", \"total_size\"].\n        pval_alternative: Alternative hypothesis for the p-value calculation. Can be \"less\" or \"greater\".\n        finite_correction: Correct p-values for finite sampling. Default is True.\n        n: Number of bootstraps.\n        seed: Seed for the random number generator.\n        allow_duplicates: If False, resampling will check if the resampled data contains duplicates.\n            If True, duplicates will be allowed.\n        max_tries: Maximum number of tries to resample data without duplicates.\n        show_progress: Show a progress bar.\n        verbose: Print additional information.\n        parallel_processing: Use parallel processing.\n        plot: Plot the distribution of the bootstrapped data.\n        **kwargs (Any): Additional keyword arguments. Includes deprecated parameters.\n            - id_column: Deprecated. Use obj_id_column instead.\n            - meas_column: Deprecated. Use measurement_column instead.\n            - smoothK: Deprecated. Use smooth_k instead.\n            - biasK: Deprecated. Use bias_k instead.\n            - peakThr: Deprecated. Use peak_threshold instead.\n            - binThr: Deprecated. Use binarization_threshold instead.\n            - polyDeg: Deprecated. Use polynomial_degree instead.\n            - biasMet: Deprecated. Use bias_method instead.\n            - epsPrev: Deprecated. Use eps_prev instead.\n            - minClsz: Deprecated. Use min_clustersize instead.\n            - min_size: Deprecated. Use min_total_size instead.\n            - paralell_processing: Deprecated. Use parallel_processing instead.\n\n    Returns:\n        DataFrame containing the bootstrapped data.\n    \"\"\"\n    map_deprecated_params = {\n        \"id_column\": \"obj_id_column\",\n        \"meas_column\": \"measurement_column\",\n        \"smoothK\": \"smooth_k\",\n        \"biasK\": \"bias_k\",\n        \"peakThr\": \"peak_threshold\",\n        \"binThr\": \"binarization_threshold\",\n        \"polyDeg\": \"polynomial_degree\",\n        \"biasMet\": \"bias_method\",\n        \"epsPrev\": \"eps_prev\",\n        \"minClsz\": \"min_clustersize\",\n        \"min_size\": \"min_total_size\",\n        \"paralell_processing\": \"parallel_processing\",\n    }\n\n    # check allowed kwargs\n    allowed_kwargs = map_deprecated_params.keys()\n    for key in kwargs:\n        if key not in allowed_kwargs:\n            raise ValueError(f\"Got an unexpected keyword argument '{key}'\")\n\n    updated_kwargs = handle_deprecated_params(map_deprecated_params, **kwargs)\n\n    # Assigning the parameters\n    obj_id_column = updated_kwargs.get(\"obj_id_column\", obj_id_column)\n    measurement_column = updated_kwargs.get(\"measurement_column\", measurement_column)\n    smooth_k = updated_kwargs.get(\"smooth_k\", smooth_k)\n    bias_k = updated_kwargs.get(\"bias_k\", bias_k)\n    peak_threshold = updated_kwargs.get(\"peak_threshold\", peak_threshold)\n    binarization_threshold = updated_kwargs.get(\"binarization_threshold\", binarization_threshold)\n    polynomial_degree = updated_kwargs.get(\"polynomial_degree\", polynomial_degree)\n    bias_method = updated_kwargs.get(\"bias_method\", bias_method)\n    eps_prev = updated_kwargs.get(\"eps_prev\", eps_prev)\n    min_clustersize = updated_kwargs.get(\"min_clustersize\", min_clustersize)\n    min_total_size = updated_kwargs.get(\"min_total_size\", min_total_size)\n    parallel_processing = updated_kwargs.get(\"parallel_processing\", parallel_processing)\n\n    if not isinstance(stats_metric, list):\n        stats_metric = [stats_metric]\n\n    for stats_m in stats_metric:\n        if stats_m not in [\n            \"duration\",\n            \"total_size\",\n            \"min_size\",\n            \"max_size\",\n        ]:\n            raise ValueError(f\"Invalid metric: {stats_metric}\")\n\n    if pval_alternative not in [\"less\", \"greater\"]:\n        raise ValueError(f\"Invalid alternative hypothesis: {pval_alternative}\")\n\n    clid_name = 'clid'\n\n    if isinstance(method, str):\n        print(f'Resampling data using method \"{method}\"...')\n    elif isinstance(method, list):\n        print(f'Resampling data using methods \"{method}\"...')\n\n    df_resampled = resample_data(\n        data=df,\n        position_columns=position_columns,\n        frame_column=frame_column,\n        obj_id_column=obj_id_column,\n        measurement_column=measurement_column,\n        method=method,\n        n=n,\n        seed=seed,\n        allow_duplicates=allow_duplicates,\n        max_tries=max_tries,\n        show_progress=show_progress,\n        verbose=verbose,\n        parallel_processing=parallel_processing,\n    )\n\n    iterations = df_resampled['iteration'].unique()\n\n    print(f'Running ARCOS and calculating \"{stats_metric}\"...')\n\n    stats_df, stats_df_mean = calculate_arcos_stats(\n        df_resampled=df_resampled,\n        position_columns=position_columns,\n        frame_column=frame_column,\n        obj_id_column=obj_id_column,\n        measurement_column=measurement_column,\n        smooth_k=smooth_k,\n        bias_k=bias_k,\n        peak_threshold=peak_threshold,\n        binarization_threshold=binarization_threshold,\n        polynomial_degree=polynomial_degree,\n        bias_method=bias_method,\n        eps=eps,\n        eps_prev=eps_prev,\n        min_clustersize=min_clustersize,\n        n_prev=n_prev,\n        min_duration=min_duration,\n        min_total_size=min_total_size,\n        stats_metric=stats_metric,\n        show_progress=show_progress,\n        parallel_processing=parallel_processing,\n        clid_column=clid_name,\n        iterations=iterations,\n    )\n    df_p = calculate_pvalue(stats_df_mean, stats_metric, pval_alternative, finite_correction, plot)\n    return stats_df, df_p\n</code></pre>"},{"location":"api/#arcos4py.validation.calculate_arcos_stats","title":"<code>calculate_arcos_stats(df_resampled, iterations, position_columns=['x'], frame_column='frame', obj_id_column='obj_id', measurement_column='m', smooth_k=3, bias_k=51, peak_threshold=0.2, binarization_threshold=0.1, polynomial_degree=1, bias_method='runmed', eps=2, eps_prev=None, min_clustersize=1, n_prev=1, min_duration=1, min_total_size=1, stats_metric=['duration', 'total_size'], show_progress=True, parallel_processing=True, clid_column='clid', **kwargs)</code>","text":"<p>Calculate the bootstrapped statistics.</p> <p>Parameters:</p> Name Type Description Default <code>df_resampled</code> <code>DataFrame</code> <p>Dataframe with resampled data.</p> required <code>iterations</code> <code>list[int]</code> <p>List of iteration names, or range.</p> required <code>position_columns</code> <code>list</code> <p>List of position columns..</p> <code>['x']</code> <code>frame_column</code> <code>str</code> <p>Name of the frame column.</p> <code>'frame'</code> <code>obj_id_column</code> <code>str</code> <p>Name of the id column.</p> <code>'obj_id'</code> <code>measurement_column</code> <code>str</code> <p>Name of the measurement column.</p> <code>'m'</code> <code>smooth_k</code> <code>int</code> <p>Smoothing kernel size for local detrending. Defaults to 3.</p> <code>3</code> <code>bias_k</code> <code>int</code> <p>Bias kernel size for large scale detrending (used with biasMet='runmed'). Defaults to 51.</p> <code>51</code> <code>peak_threshold</code> <code>float</code> <p>Peak threshold used for rescaling (used with biasMet='runmed'). Defaults to 0.2.</p> <code>0.2</code> <code>binarization_threshold</code> <code>float</code> <p>Threshold for binarizing measurements after detrending. Defaults to 0.1.</p> <code>0.1</code> <code>polynomial_degree</code> <code>int</code> <p>Polynomial degree used for detrending (used with biasMet='lm'). Defaults to 1.</p> <code>1</code> <code>bias_method</code> <code>str</code> <p>Bias method, can be 'none', 'runmed', 'lm'. Defaults to \"runmed\".</p> <code>'runmed'</code> <code>eps</code> <code>float</code> <p>Epsilon used for culstering active entities. Defaults to 2.</p> <code>2</code> <code>eps_prev</code> <code>int</code> <p>Epsilon used for linking together culsters across time. Defaults to None.</p> <code>None</code> <code>min_clustersize</code> <code>int</code> <p>Minimum cluster size. Defaults to 1.</p> <code>1</code> <code>n_prev</code> <code>int</code> <p>Number of previous frames to consider when tracking clusters. Defaults to 1.</p> <code>1</code> <code>min_duration</code> <code>int</code> <p>Minimum duration of detected event. Defaults to 1.</p> <code>1</code> <code>min_total_size</code> <code>int</code> <p>Minimum size, minimum size of detected event. Defaults to 1.</p> <code>1</code> <code>stats_metric</code> <code>list[str]</code> <p>List of metrics to calculate. Defaults to ['duration', 'total_size'].</p> <code>['duration', 'total_size']</code> <code>show_progress</code> <code>bool</code> <p>Show progress bar. Defaults to True.</p> <code>True</code> <code>parallel_processing</code> <code>bool</code> <p>Use paralell processing, uses the joblib package. Defaults to True.</p> <code>True</code> <code>clid_column</code> <code>str</code> <p>Name of the cluster id column. Defaults to 'clid'.</p> <code>'clid'</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments. Includes deprecated parameters. - posCols: Deprecated. Use position_columns instead. - id_column: Deprecated. Use obj_id_column instead. - meas_column: Deprecated. Use measurement_column instead. - smoothK: Deprecated. Use smooth_k instead. - biasK: Deprecated. Use bias_k instead. - peakThr: Deprecated. Use peak_threshold instead. - binThr: Deprecated. Use binarization_threshold instead. - polyDeg: Deprecated. Use polynomial_degree instead. - biasMet: Deprecated. Use bias_method instead. - epsPrev: Deprecated. Use eps_prev instead. - minClsz: Deprecated. Use min_clustersize instead. - min_size: Deprecated. Use min_total_size instead. - nPrev: Deprecated. Use n_prev instead. - paralell_processing: Deprecated. Use parallel_processing instead.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>Dataframe with the bootstrapped statistics.</p> <code>DataFrame</code> <code>DataFrame</code> <p>Dataframe with mean statistics.</p> Source code in <code>arcos4py/validation/_bootstrapping.py</code> <pre><code>def calculate_arcos_stats(\n    df_resampled: pd.DataFrame,\n    iterations: list[int],\n    position_columns: list = ['x'],\n    frame_column: str = 'frame',\n    obj_id_column: str = 'obj_id',\n    measurement_column: str = 'm',\n    smooth_k: int = 3,\n    bias_k: int = 51,\n    peak_threshold: float = 0.2,\n    binarization_threshold: float = 0.1,\n    polynomial_degree: int = 1,\n    bias_method: str = \"runmed\",\n    eps: float = 2,\n    eps_prev: int | None = None,\n    min_clustersize: int = 1,\n    n_prev: int = 1,\n    min_duration: int = 1,\n    min_total_size: int = 1,\n    stats_metric: list[str] = ['duration', 'total_size'],\n    show_progress: bool = True,\n    parallel_processing: bool = True,\n    clid_column: str = 'clid',\n    **kwargs,\n):\n    \"\"\"Calculate the bootstrapped statistics.\n\n    Arguments:\n        df_resampled (DataFrame): Dataframe with resampled data.\n        iterations (list[int]): List of iteration names, or range.\n        position_columns (list): List of position columns..\n        frame_column (str): Name of the frame column.\n        obj_id_column (str): Name of the id column.\n        measurement_column (str): Name of the measurement column.\n        smooth_k (int, optional): Smoothing kernel size for local detrending. Defaults to 3.\n        bias_k (int, optional): Bias kernel size for large scale detrending (used with biasMet='runmed'). Defaults to 51.\n        peak_threshold (float, optional): Peak threshold used for rescaling (used with biasMet='runmed'). Defaults to 0.2.\n        binarization_threshold (float, optional): Threshold for binarizing measurements after detrending. Defaults to 0.1.\n        polynomial_degree (int, optional): Polynomial degree used for detrending (used with biasMet='lm'). Defaults to 1.\n        bias_method (str, optional): Bias method, can be 'none', 'runmed', 'lm'. Defaults to \"runmed\".\n        eps (float, optional): Epsilon used for culstering active entities. Defaults to 2.\n        eps_prev (int, optional): Epsilon used for linking together culsters across time. Defaults to None.\n        min_clustersize (int, optional): Minimum cluster size. Defaults to 1.\n        n_prev (int, optional): Number of previous frames to consider when tracking clusters. Defaults to 1.\n        min_duration (int, optional): Minimum duration of detected event. Defaults to 1.\n        min_total_size (int, optional): Minimum size, minimum size of detected event. Defaults to 1.\n        stats_metric (list[str], optional): List of metrics to calculate. Defaults to ['duration', 'total_size'].\n        show_progress (bool, optional): Show progress bar. Defaults to True.\n        parallel_processing (bool, optional): Use paralell processing, uses the joblib package. Defaults to True.\n        clid_column (str, optional): Name of the cluster id column. Defaults to 'clid'.\n        **kwargs (Any): Additional keyword arguments. Includes deprecated parameters.\n            - posCols: Deprecated. Use position_columns instead.\n            - id_column: Deprecated. Use obj_id_column instead.\n            - meas_column: Deprecated. Use measurement_column instead.\n            - smoothK: Deprecated. Use smooth_k instead.\n            - biasK: Deprecated. Use bias_k instead.\n            - peakThr: Deprecated. Use peak_threshold instead.\n            - binThr: Deprecated. Use binarization_threshold instead.\n            - polyDeg: Deprecated. Use polynomial_degree instead.\n            - biasMet: Deprecated. Use bias_method instead.\n            - epsPrev: Deprecated. Use eps_prev instead.\n            - minClsz: Deprecated. Use min_clustersize instead.\n            - min_size: Deprecated. Use min_total_size instead.\n            - nPrev: Deprecated. Use n_prev instead.\n            - paralell_processing: Deprecated. Use parallel_processing instead.\n\n    Returns:\n        DataFrame (pd.DataFrame): Dataframe with the bootstrapped statistics.\n        DataFrame (pd.DataFrame): Dataframe with mean statistics.\n    \"\"\"\n    map_deprecated_params = {\n        \"posCols\": \"position_columns\",\n        \"id_column\": \"obj_id_column\",\n        \"meas_column\": \"measurement_column\",\n        \"smoothK\": \"smooth_k\",\n        \"biasK\": \"bias_k\",\n        \"peakThr\": \"peak_threshold\",\n        \"binThr\": \"binarization_threshold\",\n        \"polyDeg\": \"polynomial_degree\",\n        \"biasMet\": \"bias_method\",\n        \"epsPrev\": \"eps_prev\",\n        \"minClsz\": \"min_clustersize\",\n        \"nPrev\": \"n_prev\",\n        \"min_size\": \"min_total_size\",\n        \"paralell_processing\": \"parallel_processing\",\n        \"clid_name\": \"clid_column\",\n    }\n\n    # check allowed kwargs\n    allowed_kwargs = map_deprecated_params.keys()\n    for key in kwargs:\n        if key not in allowed_kwargs:\n            raise ValueError(f\"Got an unexpected keyword argument '{key}'\")\n\n    updated_kwargs = handle_deprecated_params(map_deprecated_params, **kwargs)\n\n    # Assigning the parameters\n    position_columns = updated_kwargs.get(\"position_columns\", position_columns)\n    obj_id_column = updated_kwargs.get(\"obj_id_column\", obj_id_column)\n    measurement_column = updated_kwargs.get(\"measurement_column\", measurement_column)\n    smooth_k = updated_kwargs.get(\"smooth_k\", smooth_k)\n    bias_k = updated_kwargs.get(\"bias_k\", bias_k)\n    peak_threshold = updated_kwargs.get(\"peak_threshold\", peak_threshold)\n    binarization_threshold = updated_kwargs.get(\"binarization_threshold\", binarization_threshold)\n    polynomial_degree = updated_kwargs.get(\"polynomial_degree\", polynomial_degree)\n    bias_method = updated_kwargs.get(\"bias_method\", bias_method)\n    min_total_size = updated_kwargs.get(\"min_total_size\", min_total_size)\n    parallel_processing = updated_kwargs.get(\"parallel_processing\", parallel_processing)\n    clid_column = updated_kwargs.get(\"clid_column\", clid_column)\n    min_clustersize = updated_kwargs.get(\"min_clustersize\", min_clustersize)\n    eps_prev = updated_kwargs.get(\"eps_prev\", eps_prev)\n    n_prev = updated_kwargs.get(\"n_prev\", n_prev)\n\n    if parallel_processing:\n        from joblib import Parallel, delayed\n\n        stats_df_list = Parallel(n_jobs=-1)(\n            delayed(_apply_arcos)(\n                i_iter=i_iter,\n                df_resampled=df_resampled,\n                position_columns=position_columns,\n                frame_column=frame_column,\n                obj_id_column=obj_id_column,\n                measurement_column=measurement_column,\n                smooth_k=smooth_k,\n                bias_k=bias_k,\n                peak_threshold=peak_threshold,\n                binarization_threshold=binarization_threshold,\n                polynomial_degree=polynomial_degree,\n                bias_method=bias_method,\n                eps=eps,\n                eps_prev=eps_prev,\n                min_clustersize=min_clustersize,\n                n_prev=n_prev,\n                min_duration=min_duration,\n                min_total_size=min_total_size,\n                clid_column=clid_column,\n            )\n            for i_iter in tqdm(iterations, disable=not show_progress)\n        )\n    else:\n        stats_df_list = []\n        for i_iter in tqdm(iterations, disable=not show_progress):\n            stats_df = _apply_arcos(\n                i_iter=i_iter,\n                df_resampled=df_resampled,\n                position_columns=position_columns,\n                frame_column=frame_column,\n                obj_id_column=obj_id_column,\n                measurement_column=measurement_column,\n                smooth_k=smooth_k,\n                bias_k=bias_k,\n                peak_threshold=peak_threshold,\n                binarization_threshold=binarization_threshold,\n                polynomial_degree=polynomial_degree,\n                bias_method=bias_method,\n                eps=eps,\n                eps_prev=eps_prev,\n                min_clustersize=min_clustersize,\n                n_prev=n_prev,\n                min_duration=min_duration,\n                min_total_size=min_total_size,\n                clid_column=clid_column,\n            )\n            stats_df_list.append(stats_df)\n\n    stats_df = pd.concat(stats_df_list, ignore_index=True)\n\n    stats_df_indexer = ['bootstrap_iteration'] + stats_metric\n    stats_df_mean: pd.DataFrame = (\n        stats_df[stats_df_indexer].groupby(['bootstrap_iteration']).agg(['mean']).reset_index()\n    )\n    stats_df_mean = stats_df_mean.droplevel(level=1, axis=1)\n    # for bootstrap iteratoins that did not detect any events, set the metric to 0\n    stats_df_mean[stats_metric] = stats_df_mean[stats_metric].fillna(0)\n    return stats_df, stats_df_mean\n</code></pre>"},{"location":"api/#arcos4py.validation.calculate_pvalue","title":"<code>calculate_pvalue(stats_df_mean, stats_metric, pval_alternative, finite_correction, plot, **plot_kwargs)</code>","text":"<p>Calculates the p-value with the given alternative hypothesis.</p> <p>Parameters:</p> Name Type Description Default <code>stats_df_mean</code> <code>DataFrame</code> <p>DataFrame containing the bootstrapped data.</p> required <code>stats_metric</code> <code>str | list[str]</code> <p>Metric to calculate. Can be \"duration\", \"total_size\", \"min_size\", \"max_size\" or a list of metrics. Default is [\"duration\", \"total_size\"].</p> required <code>pval_alternative</code> <code>str</code> <p>Alternative hypothesis for the p-value calculation. Can be \"less\", \"greater\" or both which will return p values for both alternatives.</p> required <code>finite_correction</code> <code>bool</code> <p>Correct p-values for finite sampling. Default is True.</p> required <code>plot</code> <code>bool</code> <p>Plot the distribution of the bootstrapped data.</p> required <p>Returns:</p> Name Type Description <code>DataFrame</code> <code>DataFrame</code> <p>containing the p-values.</p> Source code in <code>arcos4py/validation/_bootstrapping.py</code> <pre><code>def calculate_pvalue(\n    stats_df_mean: pd.DataFrame,\n    stats_metric: str | list[str],\n    pval_alternative: str,\n    finite_correction: bool,\n    plot: bool,\n    **plot_kwargs,\n):\n    \"\"\"Calculates the p-value with the given alternative hypothesis.\n\n    Arguments:\n        stats_df_mean (DataFrame): DataFrame containing the bootstrapped data.\n        stats_metric (str | list[str]): Metric to calculate.\n            Can be \"duration\", \"total_size\", \"min_size\", \"max_size\" or a list of metrics.\n            Default is [\"duration\", \"total_size\"].\n        pval_alternative (str): Alternative hypothesis for the p-value calculation.\n            Can be \"less\", \"greater\" or both which will return p values for both alternatives.\n        finite_correction (bool): Correct p-values for finite sampling. Default is True.\n        plot (bool): Plot the distribution of the bootstrapped data.\n\n    Returns:\n        DataFrame (pd.DataFrame): containing the p-values.\n    \"\"\"\n    if finite_correction:\n        pval = stats_df_mean[stats_metric].agg(lambda x: _p_val_finite_sampling(x, pval_alternative))\n    else:\n        pval = stats_df_mean[stats_metric].agg(lambda x: _p_val_infinite_sampling(x, pval_alternative))\n    pval.name = 'p_value'\n\n    if isinstance(stats_metric, list):\n        _stats_metric = stats_metric\n    else:\n        _stats_metric = [stats_metric]\n\n    mean_control = stats_df_mean[stats_metric].iloc[0]\n    stats_df_mean = stats_df_mean[stats_df_mean['bootstrap_iteration'] != 0].reset_index(drop=True)\n\n    if plot:\n        fig, axis = plt.subplots(1, len(_stats_metric))\n        try:\n            iter(axis)\n        except TypeError:\n            axis = [axis]\n        for idx, (ax, stats_col) in enumerate(zip(axis, _stats_metric)):\n            # sns.kdeplot(stats_df_mean[stats_col], ax=ax, shade=True, sharey=True)\n            sns.histplot(stats_df_mean[stats_col], ax=ax, kde=True, stat='density', common_norm=False, **plot_kwargs)\n            # ax.hist(stats_df_mean[stats_col], alpha=0.5)\n            ax.set_title(stats_col)\n            ax.vlines(mean_control[stats_col], ymin=0, ymax=ax.get_ylim()[1], color='red', ls='--')\n            ax.set_xlabel('Value')\n            if len(axis) &gt; 1 and idx == 0:\n                ax.set_ylabel('Density')\n            else:\n                ax.set_ylabel('')\n            x_pos = ax.get_xlim()[0] + ((ax.get_xlim()[1] - ax.get_xlim()[0]) * 0.7)\n            y_pos = ax.get_ylim()[0] + ((ax.get_ylim()[1] - ax.get_ylim()[0]) * 0.7)\n            ax.text(\n                x_pos,\n                y_pos,\n                f'p-value\\n{pval[stats_col].values[0]:.3f}',\n                ha='center',\n                va='center',\n                color='red',\n            )\n        fig.suptitle(f'Bootstrapped metrics: pval_alternative {pval.index[0]}')\n        return pval, fig, axis\n    return pval\n</code></pre>"},{"location":"api/#arcos4py.validation.resample_data","title":"<code>resample_data(data, position_columns=['x'], frame_column='frame', obj_id_column='obj_id', measurement_column=None, method='shuffle_tracks', n=100, seed=42, allow_duplicates=False, max_tries=100, show_progress=True, verbose=False, parallel_processing=True, **kwargs)</code>","text":"<p>Resamples data in order to perform bootstrapping analysis.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Dataframe</code> <p>The data to resample.</p> required <code>position_columns</code> <code>list</code> <p>The columns to use for the position.</p> <code>['x']</code> <code>frame_column</code> <code>str</code> <p>The column to use for the frame.</p> <code>'frame'</code> <code>obj_id_column</code> <code>str</code> <p>The column to use for the object ID.</p> <code>'obj_id'</code> <code>measurement_column</code> <code>str</code> <p>The column to use for the measurement. Only needed for 'activity_blocks_shuffle'. Defaults to None.</p> <code>None</code> <code>method</code> <code>str</code> <p>The method to use for resampling. Defaults to 'shuffle_tracks'. Available methods are: \"shuffle_tracks\", 'shuffle_timepoints', 'shift_timepoints', 'shuffle_binary_blocks', 'shuffle_coordinates_timepoint'</p> <code>'shuffle_tracks'</code> <code>n</code> <code>int</code> <p>The number of resample iterations. Defaults to 100.</p> <code>100</code> <code>seed</code> <code>int</code> <p>The random seed. Defaults to 42.</p> <code>42</code> <code>allow_duplicates</code> <code>bool</code> <p>Whether to allow resampling to randomly generate the same data twice. Defaults to False.</p> <code>False</code> <code>max_tries</code> <code>int</code> <p>The maximum number of tries to try ot generate unique data when allow_duplicates is set to True. Defaults to 100.</p> <code>100</code> <code>verbose</code> <code>bool</code> <p>Whether to print progress. Defaults to False.</p> <code>False</code> <code>parallel_processing</code> <code>bool</code> <p>Whether to use parallel processing. Defaults to True.</p> <code>True</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments. Includes deprecated parameters. - posCols (list): Deprecated. Use position_columns instead. - id_column (str): Deprecated. Use obj_id_column instead. - meas_column (str): Deprecated. Use measurement_column instead. - paralell_processing (bool): Deprecated. Use parallel_processing instead.</p> <code>{}</code> <p>Returns:</p> Type Description <code>DataFrame</code> <p>pd.DataFrame: The resampled data.</p> Source code in <code>arcos4py/validation/_resampling.py</code> <pre><code>def resample_data(  # noqa: C901\n    data: pd.DataFrame,\n    position_columns: list = ['x'],\n    frame_column: str = 'frame',\n    obj_id_column: str = 'obj_id',\n    measurement_column: Union[str, None] = None,\n    method: Union[str, list[str]] = 'shuffle_tracks',\n    n=100,\n    seed=42,\n    allow_duplicates=False,\n    max_tries=100,\n    show_progress=True,\n    verbose=False,\n    parallel_processing=True,\n    **kwargs,\n) -&gt; pd.DataFrame:\n    \"\"\"Resamples data in order to perform bootstrapping analysis.\n\n    Arguments:\n        data (pd.Dataframe): The data to resample.\n        position_columns (list): The columns to use for the position.\n        frame_column (str): The column to use for the frame.\n        obj_id_column (str): The column to use for the object ID.\n        measurement_column (str, optional): The column to use for the measurement.\n            Only needed for 'activity_blocks_shuffle'. Defaults to None.\n        method (str, optional): The method to use for resampling. Defaults to 'shuffle_tracks'.\n            Available methods are: \"shuffle_tracks\", 'shuffle_timepoints',\n            'shift_timepoints', 'shuffle_binary_blocks', 'shuffle_coordinates_timepoint'\n        n (int, optional): The number of resample iterations. Defaults to 100.\n        seed (int, optional): The random seed. Defaults to 42.\n        allow_duplicates (bool, optional): Whether to allow resampling to randomly generate the same data twice.\n            Defaults to False.\n        max_tries (int, optional): The maximum number of tries to try ot generate unique data\n            when allow_duplicates is set to True. Defaults to 100.\n        verbose (bool, optional): Whether to print progress. Defaults to False.\n        parallel_processing (bool, optional): Whether to use parallel processing. Defaults to True.\n        **kwargs (Any): Additional keyword arguments. Includes deprecated parameters.\n            - posCols (list): Deprecated. Use position_columns instead.\n            - id_column (str): Deprecated. Use obj_id_column instead.\n            - meas_column (str): Deprecated. Use measurement_column instead.\n            - paralell_processing (bool): Deprecated. Use parallel_processing instead.\n\n    Returns:\n        pd.DataFrame: The resampled data.\n    \"\"\"\n    map_deprecated_params = {\n        \"posCols\": \"position_columns\",\n        \"id_column\": \"obj_id_column\",\n        \"meas_column\": \"measurement_column\",\n        \"paralell_processing\": \"parallel_processing\",\n    }\n\n    # check allowed kwargs\n    allowed_kwargs = map_deprecated_params.keys()\n    for key in kwargs:\n        if key not in allowed_kwargs:\n            raise ValueError(f\"Got an unexpected keyword argument '{key}'\")\n    updated_kwargs = handle_deprecated_params(map_deprecated_params, **kwargs)\n\n    position_columns = updated_kwargs.get(\"position_columns\", position_columns)\n    obj_id_column = updated_kwargs.get(\"obj_id_column\", obj_id_column)\n    measurement_column = updated_kwargs.get(\"measurement_column\", measurement_column)\n    parallel_processing = updated_kwargs.get(\"parallel_processing\", parallel_processing)\n\n    # validate the input\n    if not isinstance(data, pd.DataFrame):\n        raise TypeError('data must be a pandas.DataFrame')\n    if not isinstance(position_columns, list):\n        raise TypeError('posCols must be a list')\n    if not isinstance(frame_column, str):\n        raise TypeError('frame_column must be a string')\n    if not isinstance(obj_id_column, str):\n        raise TypeError('id_column must be a string')\n    if not isinstance(measurement_column, str) and measurement_column is not None:\n        raise TypeError('meas_column must be a string or None')\n    if not isinstance(method, str) and not isinstance(method, list):\n        raise TypeError('method must be a string or list')\n    if not isinstance(n, int):\n        raise TypeError('n must be a positive integer')\n    if not isinstance(seed, int):\n        raise TypeError('seed must be an integer')\n    if not isinstance(verbose, bool):\n        raise TypeError('verbose must be a boolean')\n    if not isinstance(parallel_processing, bool):\n        raise TypeError('paralell_processing must be a boolean')\n\n    if len(position_columns) &lt; 1:\n        raise ValueError('posCols must contain at least one column')\n    if n &lt; 1:\n        raise ValueError('n must be a positive integer')\n    if seed &lt; 0:\n        raise ValueError('seed must be a positive integer')\n\n    method_dict: dict[str, Callable] = {\n        'shuffle_tracks': shuffle_tracks,\n        'shuffle_timepoints': shuffle_timepoints,\n        'shift_timepoints': shift_timepoints_per_trajectory,\n        'shuffle_binary_blocks': shuffle_activity_bocks_per_trajectory,\n        'shuffle_coordinates_timepoint': shuffle_coordinates_per_timepoint,\n    }\n\n    function_args: dict[str, tuple] = {\n        'shuffle_tracks': (obj_id_column, position_columns, frame_column),\n        'shuffle_timepoints': (obj_id_column, frame_column),\n        'shift_timepoints': (obj_id_column, frame_column),\n        'shuffle_binary_blocks': (obj_id_column, frame_column, measurement_column),\n        'shuffle_coordinates_timepoint': (position_columns, frame_column),\n    }\n\n    resampling_func_list = []\n\n    # convert method to list if necessary\n    if isinstance(method, str):\n        methods = [method]\n    else:\n        methods = method\n\n    # Check if the method is valid\n    for method in methods:\n        if method not in method_dict.keys():\n            raise ValueError(f'method must be one of {method_dict.keys()}')\n        if method == 'shuffle_binary_blocks' and measurement_column is None:\n            raise ValueError('meas_column must be set for binary_blocks_shuffle')\n\n    # Check if the columns are in the data\n    if 'shuffle_binary_blocks' in methods:\n        relevant_columns = position_columns + [frame_column, obj_id_column, measurement_column]\n    else:\n        relevant_columns = position_columns + [frame_column, obj_id_column]\n\n    for i in relevant_columns:\n        if i not in data.columns:\n            raise ValueError(f'{i} not in df.columns')\n\n    # check if there are any Nan in the columns selected\n    na_cols = []\n    for i in relevant_columns:\n        if data[position_columns].isnull().values.any():\n            na_cols.append(i)\n    if na_cols:\n        warnings.warn(f'NaN values in {na_cols}, default behaviour is to drop these rows')\n        data.dropna(subset=na_cols, inplace=True)\n\n    # Sort the data\n    data.sort_values([obj_id_column, frame_column], inplace=True)\n\n    rng = np.random.default_rng(seed)\n    # create a list of random numbers between 0 and 1000000\n    seed_list = rng.integers(1_000_000_000, size=n)\n    df_out: list[pd.DataFrame] = []\n    # shuffle xy position for each object\n    if verbose:\n        print(f'Resampling for each object {n} times')\n\n    # create a list of functions to call\n    for method in methods:\n        resampling_func_list.append(method_dict[method])\n    iter_range = range(1, n + 1)\n    if parallel_processing:\n        from joblib import Parallel, delayed\n\n        # iterate over the number of resamples\n        df_out = Parallel(n_jobs=-1)(\n            delayed(_apply_resampling)(\n                iter_number=i,\n                data=data,\n                methods=methods,\n                resampling_func_list=resampling_func_list,\n                seed_list=seed_list,\n                function_args=function_args,\n            )\n            for i in tqdm(iter_range, disable=not show_progress)\n        )\n\n    else:\n        # iterate over the number of resamples\n        for i in tqdm(iter_range, disable=not show_progress):\n            data_new = _apply_resampling(\n                iter_number=i,\n                data=data,\n                methods=methods,\n                resampling_func_list=resampling_func_list,\n                seed_list=seed_list,\n                function_args=function_args,\n            )\n            if not allow_duplicates:\n                current_try = 0\n                # make sure that data_new is not already in df_out,\n                # but they are both dataframes, else redo the resampling\n                while any(\n                    data_new.loc[:, data_new.columns != 'iteration'].equals(i.loc[:, i.columns != 'iteration'])\n                    for i in df_out\n                ):\n                    current_try += 1\n                    data_new = _apply_resampling(\n                        iter_number=i,\n                        data=data,\n                        methods=methods,\n                        resampling_func_list=resampling_func_list,\n                        seed_list=seed_list,\n                        function_args=function_args,\n                    )\n                    if current_try &gt; max_tries:\n                        raise ValueError(\n                            'Could not find a unique resampling after 100 tries, try increasing n or allow_duplicates'\n                        )\n\n            df_out.append(data_new)\n\n    data_it0 = data.copy()\n    data_it0['iteration'] = np.repeat(0, len(data_it0))\n    df_out.insert(0, data_it0)\n    return pd.concat(df_out)[data.columns.tolist() + ['iteration']]\n</code></pre>"},{"location":"changelog/","title":"Changelog","text":""},{"location":"changelog/#changelog","title":"Changelog","text":""},{"location":"changelog/#031---2025-04-01","title":"[0.3.1] - 2025-04-01","text":""},{"location":"changelog/#added","title":"Added","text":"<ul> <li>Added function to export timeseries images to a folder</li> </ul>"},{"location":"changelog/#changed","title":"Changed","text":"<ul> <li>Stats functions now return empty dataframes if input is empty instead of raising an error</li> </ul>"},{"location":"changelog/#030---2025-03-26","title":"[0.3.0] - 2025-03-26","text":""},{"location":"changelog/#added_1","title":"Added","text":"<ul> <li>Added support for split/merge detection</li> </ul>"},{"location":"changelog/#025---2024-09-10","title":"[0.2.5] - 2024-09-10","text":""},{"location":"changelog/#fixed","title":"Fixed","text":"<ul> <li>scikit-image dependency version</li> </ul>"},{"location":"changelog/#changed_1","title":"Changed","text":"<ul> <li>Updated transportation linking to use the POT library</li> </ul>"},{"location":"changelog/#024---2024-03-19","title":"[0.2.4] - 2024-03-19","text":""},{"location":"changelog/#fixed_1","title":"Fixed","text":"<ul> <li>Hard-coded column names in stats functions</li> <li>False example for a plot in the documentation</li> </ul>"},{"location":"changelog/#changed_2","title":"Changed","text":"<ul> <li>Updated plotOriginalDetrended to include separate methods for plotting detrended and original data</li> <li>Updated plotOriginalDetrended to include markers for binarized regions</li> <li>changes to parameter names to unify naming conventions across packages old Parameter names are still supported but will be deprecated in the future.</li> <li>updated documentation to reflect changes in parameter names</li> <li>Noodleplot supports kwargs for plot customization</li> <li>Changes to the way p value is represented on the validation plots</li> <li>Validation plots now dont include original metrics</li> </ul>"},{"location":"changelog/#added_2","title":"Added","text":"<ul> <li>Support for python 3.12</li> <li>Drop support for python 3.8</li> </ul>"},{"location":"changelog/#023---2023-10-10","title":"[0.2.3] - 2023-10-10","text":""},{"location":"changelog/#fixed_2","title":"Fixed","text":"<ul> <li>Noodle plot would produce an axis error if no collective events were detected</li> <li>Noodle plot would produce false results if object id was not an integer</li> </ul>"},{"location":"changelog/#changed_3","title":"Changed","text":"<ul> <li>More input data validation for stats functions</li> </ul>"},{"location":"changelog/#022---2023-09-22","title":"[0.2.2] - 2023-09-22","text":""},{"location":"changelog/#fixed_3","title":"Fixed","text":"<ul> <li>Bug in eps estimation for DBSCAN clustering</li> </ul>"},{"location":"changelog/#changed_4","title":"Changed","text":"<ul> <li>Updated input data validation for remove_background function</li> <li>ImageTracker, DataFrameTracker, remove_background can be imported from arcos4py.tools</li> <li>Updated api documentation</li> </ul>"},{"location":"changelog/#added_3","title":"Added","text":"<ul> <li>Added new function to calculate more statistics of collective events</li> <li>Added new function to calculate statistics per frame of collective events</li> <li>Accont for downsampling in in track_events_image fuction for parameters</li> </ul>"},{"location":"changelog/#021---2023-08-09","title":"[0.2.1] - 2023-08-09","text":""},{"location":"changelog/#fixed_4","title":"Fixed","text":"<ul> <li>patch for dependencies in pyproject.toml file</li> </ul>"},{"location":"changelog/#020---2023-08-09","title":"[0.2.0] - 2023-08-09","text":""},{"location":"changelog/#added_4","title":"Added","text":"<ul> <li>Funcionallity to directly apply ARCOS to images</li> <li>Simple movement predictor to improve tracking</li> <li>HDBSCAN as an alternative clustering method</li> <li>Transportation linking as an alternative linking method</li> <li>Preprocessing function for detrending of images</li> <li>Unit tests for image tracking</li> <li>Added optional Progressbar</li> </ul>"},{"location":"changelog/#changed_5","title":"Changed","text":"<ul> <li>Refactorization of event detection to improve memory usage and simplify algorithm</li> <li>ARCOS main class now also supports event detection without specifying a tracking column</li> <li>Package is now tested on python 3.8 to 3.11 (dropped 3.7 and added 3.11)</li> <li>Event detection can now ingest data lazily</li> <li>Old detectCollev class is now deprecated in favor of track_events_image and track_events_dataframe</li> </ul>"},{"location":"changelog/#016---2022-05-02","title":"[0.1.6] - 2022-05-02","text":""},{"location":"changelog/#fixed_5","title":"Fixed","text":"<ul> <li>Bug where trackCollev would overwrite the inputdata in the ARCOS object, preventing repeat tracking of collective events.</li> <li>Spelling mistake in __init__.py</li> </ul>"},{"location":"changelog/#changed_6","title":"Changed","text":"<ul> <li>None detrending now rescales measurements to 0,1 range on a global scale and not on a track-by-track basis.</li> <li>Added the parameter epsPrev by separating it from eps. epsPrev is the maximum distance cells within collective events can be separated from each other when linking them from one frame to the next. If set to 'None', as default, the same value as for eps is used.</li> </ul>"},{"location":"changelog/#added_5","title":"Added","text":"<ul> <li>New function estimate_eps (import from tools) to estimate the eps paramter used for DBSCAN clustering based on the nearest neighbour distribution. Three methods are supported, either mean of NN, median of NN or kneepoint of the sorted NN distribution.</li> <li>Functions to perform resampling and bootstrapping to perform validation of arcos output.</li> <li>Unittests for added functionallity.</li> </ul>"},{"location":"changelog/#015---2022-08-23","title":"[0.1.5] - 2022-08-23","text":""},{"location":"changelog/#changed_7","title":"Changed","text":"<ul> <li>Changed running median for global smoothing of trajectories from scipy running_median to pandas running.median, since this allows a different endrule mode</li> <li>Changed running median endrule for local smoothing of trajectories from constant to nearest</li> </ul>"},{"location":"changelog/#014---2022-06-24","title":"[0.1.4] - 2022-06-24","text":""},{"location":"changelog/#fixed_6","title":"Fixed","text":"<ul> <li>Fix for lm detrending</li> <li>Fix for none detrending</li> <li>Fix grouping failure when object id was a string in rescale data method in binarization class</li> </ul>"},{"location":"changelog/#changed_8","title":"Changed","text":"<ul> <li>Binarization Thresholding value now sets everything to be active for &gt;=, as opposed to &gt; like it was before</li> </ul>"},{"location":"changelog/#013---2022-05-23","title":"[0.1.3] - 2022-05-23","text":""},{"location":"changelog/#fixed_7","title":"Fixed","text":"<ul> <li>Bug where if object id was a string, splitting arrays into groups would fail.</li> <li>Hardcoded collective id name in stats module</li> <li>Wrong example in main module</li> </ul>"},{"location":"changelog/#added_6","title":"Added","text":"<ul> <li>More examples for plots in plotting module</li> <li>mkdocstrings-python-legacy extension (required for building docs)</li> </ul>"},{"location":"changelog/#012---2022-05-03","title":"[0.1.2] - 2022-05-03","text":""},{"location":"changelog/#added_7","title":"Added","text":"<ul> <li>NoodlePlot for collective events</li> </ul>"},{"location":"changelog/#changed_9","title":"Changed","text":"<ul> <li>binarize_detrend: converted pandas operations to numpy for performance improvements</li> <li>detect_events: converted pandas operations to numpy for performance imporovements</li> <li>stats: converted pandas operations to numpy for performance improvements</li> <li>various small changes</li> <li>updated docstrings to match changes</li> </ul>"},{"location":"changelog/#fixed_8","title":"Fixed","text":"<ul> <li>numpy warning caused by stats module</li> </ul>"},{"location":"changelog/#011---2022-04-04","title":"[0.1.1] - 2022-04-04","text":""},{"location":"changelog/#added_8","title":"Added","text":"<ul> <li>More plotting functionallity to the plotting module.<ul> <li>Measurment density plot</li> <li>Tracklength histogram</li> <li>Position/T plot</li> <li>Collective event statistcs plot</li> </ul> </li> </ul>"},{"location":"changelog/#changed_10","title":"Changed","text":"<ul> <li>Interpolation class in tools now uses pandas.interpolate to interpolate missing values.</li> <li>Interpolation now interpolates all values in all columns of the dataframe.</li> <li>Improved usage section in the documentation.</li> </ul>"},{"location":"changelog/#fixed_9","title":"Fixed","text":"<ul> <li>Bug in trackCollev class that would lead to an error message in some cases.</li> <li>Spelling in docstrings.</li> </ul>"},{"location":"changelog/#010---2022-03-26","title":"[0.1.0] - 2022-03-26","text":""},{"location":"changelog/#added_9","title":"Added","text":"<ul> <li>First release on PyPI.</li> </ul>"},{"location":"contributing/","title":"Contributing","text":""},{"location":"contributing/#contributing","title":"Contributing","text":"<p>Contributions are welcome, and they are greatly appreciated! Every little bit helps, and credit will always be given.</p> <p>You can contribute in many ways:</p>"},{"location":"contributing/#types-of-contributions","title":"Types of Contributions","text":""},{"location":"contributing/#report-bugs","title":"Report Bugs","text":"<p>Report bugs at https://github.com/bgraedel/arcos4py/issues.</p> <p>If you are reporting a bug, please include:</p> <ul> <li>Your operating system name and version.</li> <li>Any details about your local setup that might be helpful in troubleshooting.</li> <li>Detailed steps to reproduce the bug.</li> </ul>"},{"location":"contributing/#fix-bugs","title":"Fix Bugs","text":"<p>Look through the GitHub issues for bugs. Anything tagged with \"bug\" and \"help wanted\" is open to whoever wants to implement it.</p>"},{"location":"contributing/#implement-features","title":"Implement Features","text":"<p>Look through the GitHub issues for features. Anything tagged with \"enhancement\" and \"help wanted\" is open to whoever wants to implement it.</p>"},{"location":"contributing/#write-documentation","title":"Write Documentation","text":"<p>arcos4py could always use more documentation, whether as part of the official arcos4py docs, in docstrings, or even on the web in blog posts, articles, and such.</p>"},{"location":"contributing/#submit-feedback","title":"Submit Feedback","text":"<p>The best way to send feedback is to file an issue at https://github.com/bgraedel/arcos4py/issues.</p> <p>If you are proposing a feature:</p> <ul> <li>Explain in detail how it would work.</li> <li>Keep the scope as narrow as possible to make it easier to implement.</li> <li>Remember that this is a volunteer-driven project and that contributions are welcome :)</li> </ul>"},{"location":"contributing/#get-started","title":"Get Started!","text":"<p>Ready to contribute? Here's how to set up <code>arcos4py</code> for local development.</p> <ol> <li>Fork the <code>arcos4py</code> repo on GitHub.</li> <li> <p>Clone your fork locally</p> <pre><code>git clone git@github.com:your_name_here/arcos4py.git\n</code></pre> </li> <li> <p>Ensure poetry is installed.</p> </li> <li> <p>Install dependencies and start your virtualenv:</p> <pre><code>poetry install -E test -E doc -E dev\n</code></pre> </li> <li> <p>Create a branch for local development:</p> <pre><code>git checkout -b name-of-your-bugfix-or-feature\n</code></pre> <p>Now you can make your changes locally.</p> </li> <li> <p>When you're done making changes, check that your changes pass the    tests, including testing other Python versions, with tox:</p> <pre><code>poetry run tox\n</code></pre> </li> <li> <p>Commit your changes and push your branch to GitHub:</p> <pre><code>git add .\ngit commit -m \"Your detailed description of your changes.\"\ngit push origin name-of-your-bugfix-or-feature\n</code></pre> </li> <li> <p>Submit a pull request through the GitHub website.</p> </li> </ol>"},{"location":"contributing/#pull-request-guidelines","title":"Pull Request Guidelines","text":"<p>Before you submit a pull request, check that it meets these guidelines:</p> <ol> <li>The pull request should include tests.</li> <li>If the pull request adds functionality, the docs should be updated. Put    your new functionality into a function with a docstring, and add the    feature to the list in README.md.</li> <li>The pull request should work for Python 3.8, 3.9, 3.10 and 3.11. Check    https://github.com/bgraedel/arcos4py/actions    and make sure that the tests pass for all supported Python versions.</li> </ol>"},{"location":"contributing/#tips","title":"Tips","text":"<pre><code>$ poetry run pytest tests/test_arcos4py.py\n</code></pre> <p>To run a subset of tests.</p>"},{"location":"contributing/#deploying","title":"Deploying","text":"<p>A reminder for the maintainers on how to deploy: Make sure all your changes are committed (including an entry in CHANGELOG.md). Then run:</p> <pre><code>poetry run bump2version patch # possible: major / minor / patch\ngit push\ngit push --tags\n</code></pre> <p>GitHub Actions will then deploy to PyPI if tests pass.</p>"},{"location":"installation/","title":"Installation","text":""},{"location":"installation/#installation","title":"Installation","text":""},{"location":"installation/#stable-release","title":"Stable release","text":"<p>To install arcos4py, run this command in your terminal:</p> <pre><code>pip install arcos4py\n</code></pre> <p>This is the preferred method to install arcos4py, as it will always install the most recent stable release.</p> <p>If you don't have pip installed, this Python installation guide can guide you through the process.</p>"},{"location":"installation/#from-source","title":"From source","text":"<p>The source for arcos4py can be downloaded from the Github repo.</p> <p>You can either clone the public repository:</p> <pre><code>git clone https://github.com/bgraedel/arcos4py\n</code></pre> <p>Or download the tarball:</p> <pre><code>curl -OJL https://github.com/bgraedel/arcos4py/tarball/master\n</code></pre> <p>Once you have a copy of the source, you can install it with:</p> <pre><code>pip install *tar.gz\n</code></pre>"},{"location":"usage/","title":"Usage","text":""},{"location":"usage/#usage","title":"Usage","text":"<p>To use arcos4py in a project</p> <pre><code>import arcos4py\n</code></pre>"},{"location":"usage/#basic-usage-example-to-track-collective-events-in-time-series-data","title":"Basic usage example to track collective events in time-series data","text":"<p>To use the main class, generate a new class instance of ARCOS</p> <p><pre><code>from arcos4py import ARCOS\nts = ARCOS(data,[\"x, y\"], 't', 'id', 'm', 'clTrackID')\n</code></pre> Data has to be a time-series provided as a pandas DataFrame in the long format, containing at least a measurement column, a frame/index column, and an id column.</p> t x y m id Position 0 1 0.228724716134052 -0.158939933368972 0 1 0 1 1 0.880322831777765 -0.117711550077457 0 2 0 2 1 1.93057074895645 0.0786037381335957 0 3 0 3 1 2.95877070488632 0.189801493820322 0 4 0 4 1 3.90293266588805 -0.0413798066471996 0 5 0 .. . ............... ................. . .. ."},{"location":"usage/#prepare-the-input-data","title":"Prepare the input data.","text":""},{"location":"usage/#interpolate-measurments","title":"interpolate Measurments","text":"<p>If the measurement column contains missing values, run interpolate_measurements() first.</p> <pre><code>ts.interpolate_measurements()\n</code></pre>"},{"location":"usage/#clip-measurement-to-provided-quantile-range","title":"Clip measurement to provided quantile range","text":"<p>Clipping can be performed to remove extreme outliers from the dataset, but it is not necessary.</p> <pre><code>ts.clip_meas(clip_low: = 0.001, clip_high=0.999)\n</code></pre>"},{"location":"usage/#rescale-and-binarize-the-measurement","title":"Rescale and Binarize the measurement","text":"<p>Rescaling and detrending are optional for the algorithm to work but recommended. There are three options available: ['none', 'lm', 'runmed']. Rumned is the default.</p> <p>However, ARCOS requires binarized data to detect and track collective event clusters. Binarization is done by setting a threshold (binThr) and defining measurements below this threshold as 0 and above as 1.</p> <pre><code>ts.bin_measurements(smoothK: int = 1, biasK = 1, peakThr = 1,binThr = 1, polyDeg = 1, biasMet = \"runmed\",)\n</code></pre>"},{"location":"usage/#detect-collective-events","title":"Detect collective events","text":"<pre><code>events_df = ts.trackCollev(eps = 1, minClsz = 1, nPrev = 1)\nprint(events_df)\n</code></pre> t id x y clTrackID m Position 0 2 41 4.15698907764003 3.91461390425413 1 1 0 1 3 32 3.89042167730585 2.98886585399189 1 1 0 2 3 40 3.08624924975602 4.193936843095 1 1 0 3 3 41 3.99750905085216 3.9553900675078 1 1 0 4 3 42 5.06006349489829 4.0631364410516 1 1 0 .. . .. ... .. . . . <p>TrackCollev returns a pandas DataFrame object containing a column with the collecive event id.</p>"},{"location":"usage/#basic-usage-example-to-track-collective-events-in-images","title":"Basic usage example to track collective events in images","text":"<p>Currently there is no object oriented approach to track events in images. However, the track_events_image function can be used to track events in images.</p> <pre><code>from arcos4py.tools import track_events_image\nimg_tracked = track_events_image(np.where(img &gt; 0, 0, 1), eps=1.5)\n</code></pre> <p>For more advanced usage, please checkout the scripts and notebooks here: ARCOSpx-publication</p>"},{"location":"usage/#perform-calculations-without-main-class","title":"Perform calculations without main class","text":"<p>All functions from the ARCOS class are also accessible individually through the tools module, such as:</p> <p><pre><code>from arcos4py.tools import trackCollev\n</code></pre> or to directly track events in a dataframe use</p> <pre><code>from arcos4py import track_events_dataframe\n</code></pre>"},{"location":"usage/#additional-modules","title":"Additional modules","text":"<p>In addition to the ARCOS algorithm and its helper classes, plots are generated with the plotting module, collective event statistics using the stats module. Please see the Modules Page for further details.</p>"}]}